{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b198a92-1ef2-4e8f-af1b-a72528ef6a85",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "第五部分：无监督学习\n",
    "第10课：聚类分析\n",
    "10.1 聚类分析概述\n",
    "聚类是无监督学习的重要方法，目标是将相似的数据点分组：\n",
    "\n",
    "* **无标签学习**：不需要预先定义的类别标签\n",
    "* **相似性度量**：基于距离或密度等指标\n",
    "* **探索性分析**：发现数据中的自然分组\n",
    "* **应用场景**：客户细分、图像分割、异常检测等\n",
    "\n",
    "主要聚类方法：\n",
    "\n",
    "* **划分聚类**：K-Means, K-Medoids\n",
    "* **层次聚类**：凝聚式、分裂式\n",
    "* **密度聚类**：DBSCAN, OPTICS\n",
    "* **基于模型**：高斯混合模型\n",
    "\n",
    "---\n",
    "10.2 K-Means聚类\n",
    "10.2.1 K-Means原理和实现\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs, load_iris, load_digits\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "\n",
    "# 创建示例数据\n",
    "X, y_true = make_blobs(n_samples=300, centers=4, n_features=2,\n",
    "                       cluster_std=0.6, random_state=42)\n",
    "\n",
    "# 可视化原始数据\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X[:, 0], X[:, 1], s=50, alpha=0.8)\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Original Data')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# 应用K-Means\n",
    "kmeans = KMeans(n_clusters=4, random_state=42, n_init='auto') # Added n_init for future versions\n",
    "y_kmeans = kmeans.fit_predict(X)\n",
    "\n",
    "# 可视化聚类结果\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='viridis', alpha=0.8)\n",
    "centers = kmeans.cluster_centers_\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='red', s=200, alpha=0.8,\n",
    "            marker='*', edgecolor='black', linewidth=2)\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('K-Means Clustering Results')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Cluster centers:\\n{centers}\")\n",
    "print(f\"Inertia (within-cluster sum of squares): {kmeans.inertia_:.2f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "10.2.2 K-Means算法步骤可视化\n",
    "```python\n",
    "def plot_kmeans_steps(X, n_clusters=3, max_iters=6):\n",
    "    \"\"\"可视化K-Means的迭代过程\"\"\"\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # 初始化随机中心\n",
    "    indices = np.random.choice(X.shape[0], n_clusters, replace=False)\n",
    "    centers = X[indices]\n",
    "\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    old_centers = centers.copy() # Initialize old_centers before the loop\n",
    "\n",
    "    for i in range(max_iters):\n",
    "        # 分配到最近的中心\n",
    "        distances = np.sqrt(((X - centers[:, np.newaxis])**2).sum(axis=2))\n",
    "        labels = np.argmin(distances, axis=0)\n",
    "\n",
    "        # 绘图\n",
    "        ax = axes[i]\n",
    "        ax.scatter(X[:, 0], X[:, 1], c=labels, s=50, cmap='viridis', alpha=0.6)\n",
    "        ax.scatter(centers[:, 0], centers[:, 1], c='red', s=200,\n",
    "                  marker='*', edgecolor='black', linewidth=2)\n",
    "        ax.set_title(f'Iteration {i+1}')\n",
    "        ax.set_xlabel('Feature 1')\n",
    "        ax.set_ylabel('Feature 2')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "        # 更新中心\n",
    "        new_centers = np.array([X[labels == k].mean(axis=0) for k in range(n_clusters)])\n",
    "\n",
    "        # 画出移动路径\n",
    "        # Check if old_centers is defined and centers are different\n",
    "        if i > 0: # old_centers was defined in the previous iteration\n",
    "            for j in range(n_clusters):\n",
    "                ax.plot([old_centers[j, 0], centers[j, 0]],\n",
    "                       [old_centers[j, 1], centers[j, 1]],\n",
    "                       'k--', alpha=0.5, linewidth=1)\n",
    "\n",
    "        old_centers = centers.copy() # Update old_centers for the next iteration's path drawing\n",
    "        centers = new_centers\n",
    "\n",
    "        # 检查收敛\n",
    "        if i > 0 and np.allclose(old_centers, centers):\n",
    "            ax.text(0.5, 0.95, 'Converged!', transform=ax.transAxes,\n",
    "                   ha='center', va='top', fontsize=12, color='green',\n",
    "                   bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "            # 隐藏剩余的子图\n",
    "            for j_ax in range(i + 1, len(axes)): # Iterate up to len(axes)\n",
    "                axes[j_ax].axis('off')\n",
    "            break\n",
    "    # Ensure all remaining axes are turned off if max_iters is reached before convergence\n",
    "    # or if convergence happens early and fewer than 6 plots are made.\n",
    "    for j_ax in range(i + 1, len(axes)):\n",
    "        axes[j_ax].axis('off')\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 演示K-Means步骤\n",
    "# Re-create X if it's not in the current scope from 10.2.1\n",
    "X_steps, _ = make_blobs(n_samples=300, centers=4, n_features=2, # Renamed to X_steps to avoid conflict\n",
    "                       cluster_std=0.6, random_state=42)\n",
    "plot_kmeans_steps(X_steps)\n",
    "```\n",
    "\n",
    "---\n",
    "10.2.3 选择最佳K值 - 肘部法则\n",
    "```python\n",
    "# 肘部法则\n",
    "K_range = range(1, 11)\n",
    "inertias = []\n",
    "silhouette_scores = []\n",
    "\n",
    "# Using X from 10.2.1\n",
    "for k in K_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init='auto') # Added n_init\n",
    "    kmeans.fit(X)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "\n",
    "    if k > 1:  # silhouette score需要至少2个簇\n",
    "        score = silhouette_score(X, kmeans.labels_)\n",
    "        silhouette_scores.append(score)\n",
    "    else:\n",
    "        silhouette_scores.append(np.nan) # Use NaN for k=1 for silhouette\n",
    "\n",
    "# 可视化\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# 肘部图\n",
    "ax1.plot(K_range, inertias, 'bo-')\n",
    "ax1.set_xlabel('Number of clusters (K)')\n",
    "ax1.set_ylabel('Inertia')\n",
    "ax1.set_title('Elbow Method')\n",
    "# The original code hardcoded x=4. It's better to find it or not draw it if not explicitly calculated.\n",
    "# For demonstration, we'll keep it as it was in the original.\n",
    "ax1.axvline(x=4, color='r', linestyle='--', alpha=0.5, label='Elbow at K=4')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 轮廓系数图\n",
    "# Plotting only for k > 1 where silhouette_scores are actual numbers\n",
    "ax2.plot(K_range[1:], silhouette_scores[1:], 'go-') # Adjusted K_range and scores\n",
    "ax2.set_xlabel('Number of clusters (K)')\n",
    "ax2.set_ylabel('Silhouette Score')\n",
    "ax2.set_title('Silhouette Analysis')\n",
    "ax2.axvline(x=4, color='r', linestyle='--', alpha=0.5, label='Best at K=4') # As per original\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Elbow method suggests K=4 (visually)\")\n",
    "# Find best silhouette score programmatically\n",
    "valid_silhouette_scores = [s for s in silhouette_scores if not np.isnan(s)]\n",
    "if valid_silhouette_scores:\n",
    "    best_score = max(valid_silhouette_scores)\n",
    "    best_k_silhouette = K_range[silhouette_scores.index(best_score)]\n",
    "    print(f\"Best silhouette score: {best_score:.3f} at K={best_k_silhouette}\")\n",
    "else:\n",
    "    print(\"Not enough clusters to calculate a meaningful silhouette score.\")\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "10.2.4 轮廓分析详细可视化\n",
    "```python\n",
    "from matplotlib import cm\n",
    "\n",
    "def plot_silhouette_analysis(X_data, n_clusters_to_plot): # Renamed X to X_data\n",
    "    \"\"\"详细的轮廓分析\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    fig.set_facecolor('white') # Setting background for better visibility if saved\n",
    "\n",
    "    # 聚类\n",
    "    clusterer = KMeans(n_clusters=n_clusters_to_plot, random_state=42, n_init='auto') # Added n_init\n",
    "    cluster_labels = clusterer.fit_predict(X_data)\n",
    "\n",
    "    # 计算轮廓系数\n",
    "    silhouette_avg = silhouette_score(X_data, cluster_labels)\n",
    "    sample_silhouette_values = silhouette_samples(X_data, cluster_labels)\n",
    "\n",
    "    # 绘制轮廓图\n",
    "    y_lower = 10\n",
    "    for i in range(n_clusters_to_plot):\n",
    "        # 获取簇i的轮廓系数并排序\n",
    "        ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = cm.nipy_spectral(float(i) / n_clusters_to_plot)\n",
    "        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                          0, ith_cluster_silhouette_values,\n",
    "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "        # 标记簇编号\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "        y_lower = y_upper + 10  # 10 for the next plot\n",
    "\n",
    "    ax1.set_title(f'Silhouette Plot (n_clusters = {n_clusters_to_plot})')\n",
    "    ax1.set_xlabel('Silhouette Coefficient')\n",
    "    ax1.set_ylabel('Cluster Label')\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\",\n",
    "                label=f'Average: {silhouette_avg:.3f}')\n",
    "    ax1.legend(loc='best') # Changed to 'best'\n",
    "    ax1.set_yticks([]) # Clear y-axis ticks\n",
    "    ax1.set_xticks(np.arange(-0.1, 1.1, 0.2)) # Corrected set_xlim to set_xticks for range like display\n",
    "    ax1.set_xlim([-0.1, 1])\n",
    "\n",
    "\n",
    "    # 绘制聚类结果\n",
    "    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters_to_plot)\n",
    "    ax2.scatter(X_data[:, 0], X_data[:, 1], marker='.', s=100, lw=0, alpha=0.7,\n",
    "                c=colors, edgecolor='k')\n",
    "\n",
    "    # 绘制聚类中心\n",
    "    centers = clusterer.cluster_centers_\n",
    "    ax2.scatter(centers[:, 0], centers[:, 1], marker='o',\n",
    "                c=\"white\", alpha=1, s=200, edgecolor='k')\n",
    "\n",
    "    for i, c in enumerate(centers):\n",
    "        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,\n",
    "                    s=100, edgecolor='k') # Changed s to be consistent\n",
    "\n",
    "    ax2.set_title(f'Clustered Data (n_clusters = {n_clusters_to_plot})')\n",
    "    ax2.set_xlabel('Feature 1')\n",
    "    ax2.set_ylabel('Feature 2')\n",
    "    ax2.grid(True, alpha=0.3) # Added grid for consistency\n",
    "\n",
    "    plt.suptitle((f\"Silhouette analysis for KMeans clustering on sample data \"\n",
    "                  f\"with n_clusters = {n_clusters_to_plot}\"),\n",
    "                 fontsize=14, fontweight='bold') # Added suptitle\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96]) # Adjust layout for suptitle\n",
    "    plt.show()\n",
    "\n",
    "# 对不同K值进行轮廓分析\n",
    "# Using X from 10.2.1\n",
    "for n_clusters_iter in [2, 3, 4, 5]: # Renamed n_clusters to n_clusters_iter\n",
    "    plot_silhouette_analysis(X, n_clusters_iter)\n",
    "```\n",
    "\n",
    "---\n",
    "10.2.5 K-Means++初始化\n",
    "```python\n",
    "# 比较随机初始化和K-Means++\n",
    "n_init_methods = 2 # This variable is not directly used later, but defined in original\n",
    "init_methods = ['random', 'k-means++']\n",
    "results_init = [] # Renamed to avoid conflict\n",
    "\n",
    "# 多次运行以显示初始化的影响\n",
    "# Using X from 10.2.1\n",
    "for method in init_methods:\n",
    "    inertias_method = [] # Renamed\n",
    "    for _ in range(20):\n",
    "        # For 'random' init, n_init=1 is fine.\n",
    "        # For 'k-means++', n_init default is 10 in scikit-learn >= 0.23.\n",
    "        # The original code used n_init=1 for k-means++ too, which relies on a single run of k-means++.\n",
    "        # To truly compare, one might want to let k-means++ run multiple times (n_init > 1)\n",
    "        # or stick to n_init=1 for both to compare single initialization procedures.\n",
    "        # Sticking to original's n_init=1 for this reproduction.\n",
    "        kmeans = KMeans(n_clusters=4, init=method, n_init=1, random_state=None) # random_state=None for variability\n",
    "        kmeans.fit(X)\n",
    "        inertias_method.append(kmeans.inertia_)\n",
    "    results_init.append(inertias_method)\n",
    "\n",
    "# 可视化比较\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.boxplot(results_init, labels=init_methods)\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('K-Means Initialization Methods Comparison (Inertia)') # Added Inertia to title\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.show()\n",
    "\n",
    "print(\"Statistics for Inertia:\")\n",
    "for method, inertias_vals in zip(init_methods, results_init): # Renamed inertias\n",
    "    print(f\"{method}: mean={np.mean(inertias_vals):.2f}, std={np.std(inertias_vals):.2f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "10.3 层次聚类\n",
    "10.3.1 凝聚式层次聚类\n",
    "```python\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage # linkage was already imported by name\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "# 创建不同形状的数据集\n",
    "datasets_hierarchical = [] # Renamed\n",
    "\n",
    "# 数据集1: blobs\n",
    "X1_h, y1_h = make_blobs(n_samples=150, centers=3, n_features=2, # Renamed variables\n",
    "                    cluster_std=0.5, random_state=42)\n",
    "datasets_hierarchical.append((X1_h, \"Blobs\"))\n",
    "\n",
    "# 数据集2: moons\n",
    "X2_h, y2_h = make_moons(n_samples=150, noise=0.1, random_state=42) # Renamed variables\n",
    "datasets_hierarchical.append((X2_h, \"Moons\"))\n",
    "\n",
    "# 数据集3: 不同密度\n",
    "X3_1_h, _ = make_blobs(n_samples=100, centers=1, n_features=2, # Renamed variables\n",
    "                     cluster_std=0.3, center_box=(-2, -2), random_state=42)\n",
    "X3_2_h, _ = make_blobs(n_samples=100, centers=1, n_features=2, # Renamed variables\n",
    "                     cluster_std=0.8, center_box=(2, 2), random_state=42)\n",
    "X3_h = np.vstack([X3_1_h, X3_2_h]) # Renamed variables\n",
    "datasets_hierarchical.append((X3_h, \"Different Densities\"))\n",
    "\n",
    "# 对每个数据集应用层次聚类\n",
    "linkage_methods = ['ward', 'complete', 'average', 'single']\n",
    "\n",
    "# Determine n_clusters for each dataset (original code uses 2 for all)\n",
    "# For blobs, we know it's 3. For moons, 2. For different densities, 2.\n",
    "# The original used n_clusters=2, so we'll stick to that.\n",
    "n_clusters_agg = 2\n",
    "\n",
    "fig, axes = plt.subplots(len(datasets_hierarchical), len(linkage_methods),\n",
    "                        figsize=(16, 12), facecolor='white')\n",
    "\n",
    "for i, (X_data_h, name_h) in enumerate(datasets_hierarchical): # Renamed variables\n",
    "    for j, linkage_method in enumerate(linkage_methods):\n",
    "        # 聚类\n",
    "        # 'ward' linkage must be used with a distance metric that supports it (e.g., Euclidean)\n",
    "        # and typically n_clusters is specified.\n",
    "        if linkage_method == 'ward':\n",
    "            # Ward is defined only if compute_full_tree is True or n_clusters is not None.\n",
    "             clustering = AgglomerativeClustering(n_clusters=n_clusters_agg if name_h != \"Blobs\" else 3, # Adjust for blobs\n",
    "                                               linkage=linkage_method)\n",
    "        else:\n",
    "             clustering = AgglomerativeClustering(n_clusters=n_clusters_agg if name_h != \"Blobs\" else 3, # Adjust for blobs\n",
    "                                               linkage=linkage_method)\n",
    "\n",
    "        labels = clustering.fit_predict(X_data_h)\n",
    "\n",
    "        # 绘图\n",
    "        ax = axes[i, j]\n",
    "        scatter = ax.scatter(X_data_h[:, 0], X_data_h[:, 1], c=labels, cmap='viridis',\n",
    "                           s=30, alpha=0.8) # Reduced s for clarity\n",
    "\n",
    "        if i == 0:\n",
    "            ax.set_title(f'{linkage_method.capitalize()} Linkage')\n",
    "        if j == 0:\n",
    "            ax.set_ylabel(name_h, fontsize=12) # Increased fontsize\n",
    "\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_aspect('equal', adjustable='box') # Maintain aspect ratio\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "10.3.2 树状图（Dendrogram）\n",
    "```python\n",
    "# 使用鸢尾花数据集\n",
    "iris = load_iris()\n",
    "X_iris = iris.data\n",
    "y_iris = iris.target # y_iris contains class labels, not sample indices for dendrogram labels\n",
    "\n",
    "# 标准化\n",
    "scaler = StandardScaler()\n",
    "X_iris_scaled = scaler.fit_transform(X_iris)\n",
    "\n",
    "# 计算链接矩阵\n",
    "linkage_matrix = linkage(X_iris_scaled, method='ward')\n",
    "\n",
    "# 绘制树状图\n",
    "plt.figure(figsize=(15, 8), facecolor='white')\n",
    "dendrogram(linkage_matrix,\n",
    "           orientation='top',\n",
    "           # labels=y_iris, # labels expects individual sample labels, not class labels.\n",
    "           # If you want to color by class, you'd need a more complex setup or use libraries that support it directly.\n",
    "           # For simplicity, removing labels or using sample indices.\n",
    "           # labels=[str(i) for i in range(X_iris_scaled.shape[0])], # Example: using sample indices\n",
    "           truncate_mode='lastp', # Show only the last p merged clusters\n",
    "           p=12, # Example: show last 12 merges\n",
    "           show_leaf_counts=True,\n",
    "           leaf_rotation=90.,\n",
    "           leaf_font_size=8.,\n",
    "           show_contracted=True, # To visualize hierarchies better\n",
    "           distance_sort='descending') # Already default but good to specify\n",
    "\n",
    "plt.title('Hierarchical Clustering Dendrogram (Iris Dataset - Ward Linkage)')\n",
    "plt.xlabel('Sample Index or (Cluster Size)')\n",
    "plt.ylabel('Distance (Ward)')\n",
    "\n",
    "# 添加切割线\n",
    "plt.axhline(y=7.5, c='r', linestyle='--', label='Cut at distance=7.5 (example)') # Adjusted y for Ward on Iris\n",
    "plt.legend()\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7) # Add y-grid\n",
    "plt.show()\n",
    "\n",
    "# 根据树状图确定簇数\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "max_d = 7.5  # 切割距离 (example, chosen based on visual inspection of the dendrogram)\n",
    "clusters_dendro = fcluster(linkage_matrix, max_d, criterion='distance')\n",
    "print(f\"Number of clusters by cutting at distance {max_d}: {len(np.unique(clusters_dendro))}\")\n",
    "\n",
    "# If we aim for 3 clusters (since Iris has 3 classes)\n",
    "clusters_k3 = fcluster(linkage_matrix, 3, criterion='maxclust')\n",
    "print(f\"Number of clusters by specifying k=3: {len(np.unique(clusters_k3))}\")\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "10.3.3 不同链接方法的比较\n",
    "```python\n",
    "# 创建一个有明显层次结构的数据集\n",
    "np.random.seed(42)\n",
    "# 创建三个主簇，每个主簇包含两个子簇\n",
    "n_points_hier = 50 # Renamed\n",
    "clusters_data_h = [] # Renamed\n",
    "\n",
    "# 主簇1\n",
    "clusters_data_h.append(np.random.randn(n_points_hier, 2) * 0.5 + [-5, 0])\n",
    "clusters_data_h.append(np.random.randn(n_points_hier, 2) * 0.5 + [-3, 0])\n",
    "\n",
    "# 主簇2\n",
    "clusters_data_h.append(np.random.randn(n_points_hier, 2) * 0.5 + [0, 5])\n",
    "clusters_data_h.append(np.random.randn(n_points_hier, 2) * 0.5 + [0, 3])\n",
    "\n",
    "# 主簇3\n",
    "clusters_data_h.append(np.random.randn(n_points_hier, 2) * 0.5 + [5, 0])\n",
    "clusters_data_h.append(np.random.randn(n_points_hier, 2) * 0.5 + [3, 0])\n",
    "\n",
    "X_hierarchical_comp = np.vstack(clusters_data_h) # Renamed\n",
    "\n",
    "# 比较不同链接方法\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 10), facecolor='white')\n",
    "\n",
    "linkage_methods_comp = ['single', 'complete', 'average', 'ward'] # Renamed\n",
    "\n",
    "for i, method in enumerate(linkage_methods_comp):\n",
    "    # 计算链接\n",
    "    Z = linkage(X_hierarchical_comp, method=method)\n",
    "\n",
    "    # 绘制树状图\n",
    "    ax = axes[0, i]\n",
    "    dendrogram(Z, ax=ax, color_threshold=0, above_threshold_color='k',\n",
    "               truncate_mode='lastp', p=30) # Added truncate for clarity\n",
    "    ax.set_title(f'{method.capitalize()} Linkage Dendrogram')\n",
    "    ax.set_xlabel('Sample Index')\n",
    "    ax.set_ylabel('Distance')\n",
    "    ax.set_xticks([]) # Hide x-ticks for dendrogram clarity\n",
    "\n",
    "    # 绘制聚类结果（3个簇）\n",
    "    ax = axes[1, i]\n",
    "    # AgglomerativeClustering requires n_clusters if not computing full tree.\n",
    "    # For 'ward', it's common to specify n_clusters.\n",
    "    # For others, you might cut the dendrogram (fcluster) or specify n_clusters.\n",
    "    clustering_comp = AgglomerativeClustering(n_clusters=3, linkage=method) # Original uses 3 clusters\n",
    "    labels_comp = clustering_comp.fit_predict(X_hierarchical_comp)\n",
    "\n",
    "    ax.scatter(X_hierarchical_comp[:, 0], X_hierarchical_comp[:, 1],\n",
    "              c=labels_comp, cmap='viridis', s=30, alpha=0.7) # Reduced s\n",
    "    ax.set_title(f'{method.capitalize()} - 3 Clusters')\n",
    "    ax.set_xlabel('Feature 1')\n",
    "    ax.set_ylabel('Feature 2')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_aspect('equal', adjustable='box')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "10.4 DBSCAN（基于密度的聚类）\n",
    "10.4.1 DBSCAN原理和实现\n",
    "```python\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.datasets import make_moons, make_circles # make_circles already imported\n",
    "\n",
    "# 创建复杂形状的数据集\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10), facecolor='white') # Increased width slightly\n",
    "\n",
    "# 数据集1: Moons\n",
    "X_moons_db, _ = make_moons(n_samples=200, noise=0.05, random_state=42) # Reduced noise for better DBSCAN demo\n",
    "\n",
    "# 数据集2: Circles\n",
    "X_circles_db, _ = make_circles(n_samples=200, noise=0.05, factor=0.5, random_state=42) # Reduced noise\n",
    "\n",
    "# 数据集3: 混合数据\n",
    "X_mixed_db = np.vstack([ # Renamed\n",
    "    np.random.randn(100, 2) * 0.5 + [2, 2],\n",
    "    np.random.randn(50, 2) * 0.5 + [-2, -2],\n",
    "    np.random.randn(50, 2) * 0.5 + [2, -2],\n",
    "    np.random.uniform(-4, 4, (20, 2))  # 噪声点\n",
    "])\n",
    "\n",
    "datasets_db = [(X_moons_db, \"Moons\", {'eps': 0.3, 'min_samples': 5}, 2), # Added params for DBSCAN and k for K-Means\n",
    "               (X_circles_db, \"Circles\", {'eps': 0.3, 'min_samples': 5}, 2),\n",
    "               (X_mixed_db, \"Mixed with Noise\", {'eps': 0.4, 'min_samples': 5}, 3)] # Adjusted params for mixed\n",
    "\n",
    "dbscan_outputs = [] # To store dbscan labels for later printing\n",
    "\n",
    "# 对每个数据集应用K-Means和DBSCAN\n",
    "for i, (X_data_db, name_db, db_params, k_kmeans) in enumerate(datasets_db): # Renamed variables\n",
    "    # K-Means\n",
    "    kmeans = KMeans(n_clusters=k_kmeans, random_state=42, n_init='auto') # Added n_init\n",
    "    kmeans_labels = kmeans.fit_predict(X_data_db)\n",
    "\n",
    "    # DBSCAN\n",
    "    dbscan = DBSCAN(eps=db_params['eps'], min_samples=db_params['min_samples'])\n",
    "    dbscan_labels = dbscan.fit_predict(X_data_db)\n",
    "    dbscan_outputs.append((name_db, dbscan_labels))\n",
    "\n",
    "\n",
    "    # 绘制K-Means结果\n",
    "    ax = axes[0, i]\n",
    "    ax.scatter(X_data_db[:, 0], X_data_db[:, 1], c=kmeans_labels,\n",
    "                        cmap='viridis', s=40, alpha=0.8) # Adjusted s\n",
    "    ax.set_title(f'K-Means on {name_db}')\n",
    "    ax.set_xlabel('Feature 1')\n",
    "    ax.set_ylabel('Feature 2')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_aspect('equal', adjustable='box')\n",
    "\n",
    "\n",
    "    # 绘制DBSCAN结果\n",
    "    ax = axes[1, i]\n",
    "    unique_labels = set(dbscan_labels)\n",
    "    # Ensure consistent color mapping, noise is always red\n",
    "    colors = plt.cm.get_cmap('viridis', len(unique_labels) - (1 if -1 in unique_labels else 0) )\n",
    "\n",
    "    color_map = {}\n",
    "    cluster_idx = 0\n",
    "    for k_label in sorted(list(unique_labels)): # Sort labels for consistent color assignment\n",
    "        if k_label == -1:\n",
    "            color_map[k_label] = 'red'\n",
    "        else:\n",
    "            color_map[k_label] = colors(cluster_idx)\n",
    "            cluster_idx += 1\n",
    "\n",
    "    for k_label in unique_labels:\n",
    "        class_member_mask = (dbscan_labels == k_label)\n",
    "        xy = X_data_db[class_member_mask]\n",
    "        marker = 'x' if k_label == -1 else 'o'\n",
    "        ax.scatter(xy[:, 0], xy[:, 1], c=[color_map[k_label]], marker=marker,\n",
    "                  s=40, alpha=0.8, edgecolor='black' if k_label == -1 else 'none',\n",
    "                  label=f'Cluster {k_label}' if k_label !=-1 else 'Noise')\n",
    "\n",
    "\n",
    "    ax.set_title(f'DBSCAN on {name_db} (eps={db_params[\"eps\"]}, min_s={db_params[\"min_samples\"]})') # Show params\n",
    "    ax.set_xlabel('Feature 1')\n",
    "    ax.set_ylabel('Feature 2')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_aspect('equal', adjustable='box')\n",
    "    if i == 0: # Legend for the first DBSCAN plot\n",
    "        handles, labels_legend = ax.get_legend_handles_labels()\n",
    "        # Filter out duplicate noise labels if any, and sort\n",
    "        by_label = dict(zip(labels_legend, handles))\n",
    "        ax.legend(by_label.values(), by_label.keys(), loc='best', fontsize='small')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 打印DBSCAN结果统计\n",
    "print(\"\\nDBSCAN Results Statistics:\")\n",
    "for name_db, labels_db in dbscan_outputs: # Using stored labels\n",
    "    n_clusters_db = len(set(labels_db)) - (1 if -1 in labels_db else 0)\n",
    "    n_noise_db = list(labels_db).count(-1)\n",
    "    print(f\"\\n{name_db}:\")\n",
    "    print(f\"  Number of clusters found: {n_clusters_db}\")\n",
    "    print(f\"  Number of noise points: {n_noise_db}\")\n",
    "```\n",
    "\n",
    "---\n",
    "10.4.2 DBSCAN参数调优\n",
    "```python\n",
    "# 参数网格搜索\n",
    "def plot_dbscan_params(X_db_param, eps_range_param, min_samples_range_param): # Renamed vars\n",
    "    \"\"\"可视化不同参数组合的DBSCAN结果\"\"\"\n",
    "    num_eps = len(eps_range_param)\n",
    "    num_min_samples = len(min_samples_range_param)\n",
    "    fig, axes = plt.subplots(num_min_samples, num_eps,\n",
    "                            figsize=(num_eps * 3.5, num_min_samples * 3), # Adjusted figsize\n",
    "                            facecolor='white', sharex=True, sharey=True) # Added sharex/y\n",
    "\n",
    "    for i, min_samples_val in enumerate(min_samples_range_param): # Renamed var\n",
    "        for j, eps_val in enumerate(eps_range_param): # Renamed var\n",
    "            # 应用DBSCAN\n",
    "            dbscan = DBSCAN(eps=eps_val, min_samples=min_samples_val)\n",
    "            labels = dbscan.fit_predict(X_db_param)\n",
    "\n",
    "            # 绘图\n",
    "            ax = axes[i, j] if num_min_samples > 1 and num_eps > 1 else (axes[j] if num_eps > 1 else axes[i] if num_min_samples > 1 else axes)\n",
    "\n",
    "\n",
    "            # 统计信息\n",
    "            n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "            n_noise = list(labels).count(-1)\n",
    "\n",
    "            # 绘制结果\n",
    "            unique_labels = set(labels)\n",
    "            # Consistent color mapping\n",
    "            colors_param = plt.cm.get_cmap('viridis', len(unique_labels) - (1 if -1 in unique_labels else 0) )\n",
    "            color_map_param = {}\n",
    "            cluster_idx_param = 0\n",
    "            for k_label in sorted(list(unique_labels)):\n",
    "                if k_label == -1:\n",
    "                    color_map_param[k_label] = 'red'\n",
    "                else:\n",
    "                    color_map_param[k_label] = colors_param(cluster_idx_param)\n",
    "                    cluster_idx_param +=1\n",
    "\n",
    "            for k_label in unique_labels:\n",
    "                class_member_mask = (labels == k_label)\n",
    "                xy = X_db_param[class_member_mask]\n",
    "                marker = 'x' if k_label == -1 else '.' # Use '.' for denser plots\n",
    "                ax.scatter(xy[:, 0], xy[:, 1], c=[color_map_param[k_label]], s=15, alpha=0.7, # Reduced s\n",
    "                          marker=marker)\n",
    "\n",
    "            ax.set_title(f'eps={eps_val}, min_s={min_samples_val}\\nCl: {n_clusters}, Noi: {n_noise}',\n",
    "                         fontsize=9) # Reduced fontsize\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            ax.set_aspect('equal', adjustable='box')\n",
    "\n",
    "\n",
    "    fig.suptitle('DBSCAN Parameter Tuning (Moons Dataset)', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95]) # Adjust for suptitle\n",
    "    plt.show()\n",
    "\n",
    "# 使用Moons数据集测试参数 (X_moons_db from 10.4.1)\n",
    "eps_range_test = [0.1, 0.2, 0.3, 0.5] # Renamed\n",
    "min_samples_range_test = [3, 5, 7, 10] # Renamed, adjusted values\n",
    "plot_dbscan_params(X_moons_db, eps_range_test, min_samples_range_test)\n",
    "```\n",
    "\n",
    "---\n",
    "10.4.3 寻找最佳eps值 - k距离图\n",
    "```python\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "def plot_k_distance(X_k_dist, k_val, dataset_name=\"Dataset\"): # Renamed X, k and added dataset_name\n",
    "    \"\"\"绘制k距离图来帮助选择eps\"\"\"\n",
    "    # 计算每个点到第k个最近邻的距离\n",
    "    # k_val for NearestNeighbors should be min_samples (or min_samples - 1 by some conventions)\n",
    "    # If min_samples is chosen for DBSCAN, k for k-distance plot is often min_samples.\n",
    "    # The original code used k=5, implying min_samples=5.\n",
    "    nbrs = NearestNeighbors(n_neighbors=k_val).fit(X_k_dist)\n",
    "    distances, indices = nbrs.kneighbors(X_k_dist)\n",
    "\n",
    "    # 获取第k个最近邻的距离（索引k-1 because kneighbors returns the point itself as 1st neighbor if include_self=True (default))\n",
    "    # distances are already sorted for each point's neighbors if algorithm='auto'\n",
    "    # We need the distance to the k-th neighbor, which is at column k-1\n",
    "    k_distances = np.sort(distances[:, k_val-1], axis=0) # Sort these k-th distances\n",
    "\n",
    "    # 绘图\n",
    "    plt.figure(figsize=(10, 6), facecolor='white')\n",
    "    plt.plot(range(len(k_distances)), k_distances) # Plot against index\n",
    "    plt.xlabel(f'Points sorted by distance to {k_val}-th nearest neighbor')\n",
    "    plt.ylabel(f'{k_val}-th Nearest Neighbor Distance (eps candidate)')\n",
    "    plt.title(f'{k_val}-Distance Graph for {dataset_name} (eps selection)')\n",
    "    plt.grid(True, alpha=0.5, linestyle='--')\n",
    "\n",
    "    # 尝试找到\"肘部\" - This is a heuristic and can be tricky.\n",
    "    # A more robust way might involve libraries like 'kneed'\n",
    "    # Original's second derivative approach is sensitive.\n",
    "    # For simplicity, we'll show the plot and user can infer.\n",
    "    # Or, if we want to suggest a point:\n",
    "    if len(k_distances) > k_val +1 : # Ensure enough points for diff\n",
    "        try:\n",
    "            from kneed import KneeLocator\n",
    "            kn = KneeLocator(range(len(k_distances)), k_distances, curve='convex', direction='increasing')\n",
    "            if kn.elbow_y:\n",
    "                 plt.axhline(y=kn.elbow_y, color='r', linestyle='--',\n",
    "                           label=f'Suggested eps (knee) ≈ {kn.elbow_y:.3f}')\n",
    "                 plt.plot(kn.elbow, kn.elbow_y, 'ro', markersize=8) # Mark the elbow point\n",
    "                 plt.legend()\n",
    "        except ImportError:\n",
    "            print(\"Consider installing 'kneed' library for automatic elbow detection.\")\n",
    "            # Fallback to a simpler visual cue if kneed is not available\n",
    "            # This simple diff method is often not robust\n",
    "            if len(k_distances) > 100: # Only if enough points for a smooth diff\n",
    "                 first_diff = np.diff(k_distances,1)\n",
    "                 second_diff = np.diff(first_diff,1)\n",
    "                 if len(second_diff) > 0 :\n",
    "                    elbow_idx = np.argmax(second_diff) + 2 # +1 for first diff, +1 for second diff index\n",
    "                    if elbow_idx < len(k_distances):\n",
    "                        elbow_value = k_distances[elbow_idx]\n",
    "                        plt.axhline(y=elbow_value, color='g', linestyle=':',\n",
    "                                   label=f'Approx. eps (max curvature) ≈ {elbow_value:.3f}')\n",
    "                        plt.legend()\n",
    "\n",
    "\n",
    "    plt.show()\n",
    "    return k_distances\n",
    "\n",
    "# 对不同数据集绘制k距离图 (Using X_moons_db, X_circles_db from 10.4.1)\n",
    "# The k value for k-distance plot is typically related to min_samples for DBSCAN.\n",
    "# Let's assume min_samples will be around 5-10 for these datasets.\n",
    "min_samples_for_k_dist = 5\n",
    "\n",
    "print(f\"\\nK-Distance plot for k={min_samples_for_k_dist}\")\n",
    "for X_data_k_dist, name_k_dist in [(X_moons_db, \"Moons\"), (X_circles_db, \"Circles\")]: # Renamed vars\n",
    "    print(f\"\\nAnalyzing {name_k_dist} dataset:\")\n",
    "    k_distances_plot = plot_k_distance(X_data_k_dist, k=min_samples_for_k_dist, dataset_name=name_k_dist)\n",
    "    # Heuristic: an eps around the \"elbow\" of this plot.\n",
    "    # For X_moons_db with min_samples=5, eps might be around 0.15-0.3 from visual.\n",
    "    # For X_circles_db with min_samples=5, eps might be around 0.15-0.3 from visual.\n",
    "```\n",
    "\n",
    "---\n",
    "10.5 聚类评估指标\n",
    "```python\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score, homogeneity_score, completeness_score, v_measure_score\n",
    "\n",
    "# 创建一个函数来评估不同聚类算法\n",
    "def evaluate_clustering(X_eval_data, algorithms_dict, true_labels_eval=None): # Renamed variables\n",
    "    \"\"\"评估多个聚类算法\"\"\"\n",
    "    results_list = [] # Renamed\n",
    "\n",
    "    for name, algorithm_instance in algorithms_dict.items(): # Renamed variables\n",
    "        # 聚类\n",
    "        # Some algorithms might be pre-fitted if passed with labels_\n",
    "        if hasattr(algorithm_instance, 'labels_') and algorithm_instance.labels_ is not None:\n",
    "            labels = algorithm_instance.labels_\n",
    "        else:\n",
    "            try:\n",
    "                labels = algorithm_instance.fit_predict(X_eval_data)\n",
    "            except TypeError: # For algorithms like LDA that need y for fit\n",
    "                if true_labels_eval is not None:\n",
    "                    # This case is tricky for clustering evaluation as it implies supervised\n",
    "                    print(f\"Algorithm {name} seems to require labels for fitting, skipping unsupervised eval or assuming prefit.\")\n",
    "                    continue\n",
    "                else:\n",
    "                    print(f\"Algorithm {name} requires y for fit, and true_labels not provided. Skipping.\")\n",
    "                    continue\n",
    "\n",
    "\n",
    "        n_clusters_found = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "        n_noise_points = list(labels).count(-1)\n",
    "\n",
    "        current_result = {\n",
    "            'Algorithm': name,\n",
    "            'Clusters': n_clusters_found,\n",
    "            'Noise': n_noise_points\n",
    "        }\n",
    "\n",
    "        # 计算内部评估指标\n",
    "        # Ensure there's more than 1 cluster and not all points are noise for these metrics\n",
    "        if n_clusters_found > 1 and (len(labels) - n_noise_points) > n_clusters_found :\n",
    "            # Filter out noise points for silhouette, CH, DB scores if present\n",
    "            valid_mask = labels != -1\n",
    "            if np.sum(valid_mask) > 1: # Ensure some non-noise points exist\n",
    "                X_eval_filtered = X_eval_data[valid_mask]\n",
    "                labels_filtered = labels[valid_mask]\n",
    "                if len(set(labels_filtered)) > 1: # Check again after filtering noise\n",
    "                    current_result['Silhouette'] = silhouette_score(X_eval_filtered, labels_filtered)\n",
    "                    current_result['Calinski-Harabasz'] = calinski_harabasz_score(X_eval_filtered, labels_filtered)\n",
    "                    current_result['Davies-Bouldin'] = davies_bouldin_score(X_eval_filtered, labels_filtered)\n",
    "                else:\n",
    "                    current_result['Silhouette'] = current_result['Calinski-Harabasz'] = current_result['Davies-Bouldin'] = np.nan\n",
    "            else: # All points are noise or only one cluster of non-noise\n",
    "                current_result['Silhouette'] = current_result['Calinski-Harabasz'] = current_result['Davies-Bouldin'] = np.nan\n",
    "        else: # Not enough clusters or data points for these scores\n",
    "            current_result['Silhouette'] = np.nan\n",
    "            current_result['Calinski-Harabasz'] = np.nan\n",
    "            current_result['Davies-Bouldin'] = np.nan\n",
    "\n",
    "        # 如果有真实标签，计算外部评估指标\n",
    "        if true_labels_eval is not None:\n",
    "            # Filter out noise points from both true_labels and predicted labels\n",
    "            mask = labels != -1\n",
    "            if mask.sum() > 0: # If there are any non-noise predictions\n",
    "                true_labels_masked = true_labels_eval[mask]\n",
    "                predicted_labels_masked = labels[mask]\n",
    "                # Ensure there's something to compare\n",
    "                if len(true_labels_masked) > 0 and len(set(predicted_labels_masked)) > 0 :\n",
    "                    current_result['ARI'] = adjusted_rand_score(true_labels_masked, predicted_labels_masked)\n",
    "                    current_result['NMI'] = normalized_mutual_info_score(true_labels_masked, predicted_labels_masked, average_method='arithmetic')\n",
    "                    current_result['Homogeneity'] = homogeneity_score(true_labels_masked, predicted_labels_masked)\n",
    "                    current_result['Completeness'] = completeness_score(true_labels_masked, predicted_labels_masked)\n",
    "                    current_result['V-measure'] = v_measure_score(true_labels_masked, predicted_labels_masked)\n",
    "                else:\n",
    "                    current_result['ARI'] = current_result['NMI'] = current_result['Homogeneity'] = current_result['Completeness'] = current_result['V-measure'] = np.nan\n",
    "            else: # All predicted labels are noise\n",
    "                 current_result['ARI'] = current_result['NMI'] = current_result['Homogeneity'] = current_result['Completeness'] = current_result['V-measure'] = np.nan\n",
    "\n",
    "\n",
    "        results_list.append(current_result)\n",
    "\n",
    "    return pd.DataFrame(results_list)\n",
    "\n",
    "# 使用有真实标签的数据集\n",
    "X_eval, y_true_eval = make_blobs(n_samples=300, centers=4, n_features=2, # Renamed y_true\n",
    "                           cluster_std=0.7, random_state=42) # Slightly increased std\n",
    "X_eval_scaled = StandardScaler().fit_transform(X_eval) # Scale data for distance-based algos\n",
    "\n",
    "# 定义算法\n",
    "algorithms_eval = { # Renamed\n",
    "    'K-Means (k=4)': KMeans(n_clusters=4, random_state=42, n_init='auto'),\n",
    "    'K-Means (k=3)': KMeans(n_clusters=3, random_state=42, n_init='auto'),\n",
    "    'K-Means (k=5)': KMeans(n_clusters=5, random_state=42, n_init='auto'),\n",
    "    'Hierarchical (Ward, k=4)': AgglomerativeClustering(n_clusters=4, linkage='ward'),\n",
    "    'Hierarchical (Complete, k=4)': AgglomerativeClustering(n_clusters=4, linkage='complete'),\n",
    "    'Hierarchical (Average, k=4)': AgglomerativeClustering(n_clusters=4, linkage='average'),\n",
    "    'DBSCAN (eps=0.8, ms=5)': DBSCAN(eps=0.8, min_samples=5), # Adjusted eps for scaled data\n",
    "    'DBSCAN (eps=1.2, ms=5)': DBSCAN(eps=1.2, min_samples=5)  # Adjusted eps\n",
    "}\n",
    "\n",
    "# 评估\n",
    "eval_results_df = evaluate_clustering(X_eval_scaled, algorithms_eval, y_true_eval) # Renamed\n",
    "print(\"\\nClustering Evaluation Results (on Scaled Data):\")\n",
    "# Define order of columns for better readability\n",
    "column_order = ['Algorithm', 'Clusters', 'Noise', 'Silhouette', 'Calinski-Harabasz', 'Davies-Bouldin', 'ARI', 'NMI', 'Homogeneity', 'Completeness', 'V-measure']\n",
    "# Filter out columns that might not exist if true_labels_eval was None (though it's provided here)\n",
    "final_columns = [col for col in column_order if col in eval_results_df.columns]\n",
    "print(eval_results_df[final_columns].round(3))\n",
    "\n",
    "\n",
    "# 可视化评估结果 (only metrics that are present)\n",
    "metrics_to_plot = {\n",
    "    'Silhouette': 'Higher is Better',\n",
    "    'Davies-Bouldin': 'Lower is Better',\n",
    "    'Calinski-Harabasz': 'Higher is Better',\n",
    "    'ARI': 'Higher is Better (vs True Labels)',\n",
    "    'NMI': 'Higher is Better (vs True Labels)',\n",
    "    'V-measure': 'Higher is Better (vs True Labels)'\n",
    "}\n",
    "plot_cols = [m for m in metrics_to_plot.keys() if m in eval_results_df.columns and eval_results_df[m].notna().any()]\n",
    "\n",
    "if plot_cols:\n",
    "    num_metrics = len(plot_cols)\n",
    "    fig_rows = (num_metrics + 1) // 2 # Arrange in 2 columns\n",
    "    fig, axes = plt.subplots(fig_rows, 2, figsize=(15, fig_rows * 5), facecolor='white')\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, metric in enumerate(plot_cols):\n",
    "        ax = axes[i]\n",
    "        # Sort by metric for better visualization in bar chart\n",
    "        sorted_df = eval_results_df.sort_values(by=metric, ascending=(metrics_to_plot[metric] == 'Lower is Better'))\n",
    "        bars = ax.barh(sorted_df['Algorithm'], sorted_df[metric], color=plt.cm.viridis(np.linspace(0,1,len(sorted_df))))\n",
    "        ax.set_title(f'{metric} ({metrics_to_plot[metric]})')\n",
    "        ax.set_xlabel('Score')\n",
    "        ax.grid(axis='x', linestyle='--', alpha=0.6)\n",
    "        # Add labels to bars\n",
    "        for bar in bars:\n",
    "            width = bar.get_width()\n",
    "            label_x_pos = width + (max(sorted_df[metric])*0.01) if metrics_to_plot[metric] == 'Higher is Better' else width - (max(sorted_df[metric])*0.1) # Adjust label pos\n",
    "            if not np.isnan(width):\n",
    "                ax.text(label_x_pos, bar.get_y() + bar.get_height()/2., f'{width:.3f}', va='center', ha='left' if metrics_to_plot[metric] == 'Higher is Better' else 'right')\n",
    "\n",
    "\n",
    "    # Hide any unused subplots\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No metrics available to plot.\")\n",
    "```\n",
    "\n",
    "---\n",
    "第11课：降维技术\n",
    "11.1 降维概述\n",
    "降维的目标是减少数据的特征数量，同时保留最重要的信息：\n",
    "\n",
    "* **维度诅咒**：高维空间中的数据稀疏性问题\n",
    "* **可视化**：将高维数据投影到2D/3D空间\n",
    "* **去噪**：去除冗余特征和噪声\n",
    "* **计算效率**：减少计算复杂度\n",
    "\n",
    "主要降维方法：\n",
    "\n",
    "* **线性方法**：PCA、LDA\n",
    "* **非线性方法**：t-SNE、UMAP\n",
    "* **基于自编码器**：深度学习方法\n",
    "\n",
    "---\n",
    "11.2 主成分分析（PCA）\n",
    "11.2.1 PCA原理和实现\n",
    "```python\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import load_digits, fetch_olivetti_faces # fetch_olivetti_faces already imported\n",
    "\n",
    "# 使用手写数字数据集\n",
    "digits = load_digits()\n",
    "X_digits = digits.data # Renamed\n",
    "y_digits = digits.target # Renamed\n",
    "\n",
    "print(f\"Original data shape: {X_digits.shape}\")\n",
    "print(f\"Number of features: {X_digits.shape[1]}\")\n",
    "\n",
    "# Scale data before PCA for better results, though PCA works on unscaled data by centering it.\n",
    "# Scaling is important if features have very different variances. For digits, pixels are somewhat comparable.\n",
    "scaler_digits = StandardScaler()\n",
    "X_digits_scaled = scaler_digits.fit_transform(X_digits)\n",
    "\n",
    "\n",
    "# 应用PCA\n",
    "pca_2c = PCA(n_components=2) # Renamed\n",
    "X_pca_2c = pca_2c.fit_transform(X_digits_scaled) # Use scaled data\n",
    "\n",
    "# 可视化降维结果\n",
    "plt.figure(figsize=(10, 8), facecolor='white')\n",
    "scatter = plt.scatter(X_pca_2c[:, 0], X_pca_2c[:, 1], c=y_digits, cmap='tab10',\n",
    "                     alpha=0.7, edgecolor='none', s=40) # Adjusted alpha, edgecolor, s\n",
    "plt.xlabel('First Principal Component')\n",
    "plt.ylabel('Second Principal Component')\n",
    "plt.title('PCA of Digits Dataset (2 Components from Scaled Data)')\n",
    "plt.colorbar(scatter, label='Digit Class') # Changed label\n",
    "plt.grid(True, alpha=0.3, linestyle='--')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nExplained variance ratio by 2 components: {pca_2c.explained_variance_ratio_}\")\n",
    "print(f\"Total variance explained by 2 components: {sum(pca_2c.explained_variance_ratio_):.2%}\")\n",
    "```\n",
    "\n",
    "---\n",
    "11.2.2 选择主成分数量\n",
    "```python\n",
    "# 计算所有主成分 (using X_digits_scaled from 11.2.1)\n",
    "pca_full = PCA() # n_components=None by default, meaning all components\n",
    "pca_full.fit(X_digits_scaled)\n",
    "\n",
    "# 累积方差解释率\n",
    "explained_variance_ratio_all = pca_full.explained_variance_ratio_\n",
    "cumsum_ratio = np.cumsum(explained_variance_ratio_all)\n",
    "\n",
    "# 可视化\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6), facecolor='white') # Increased width\n",
    "\n",
    "# 方差解释率 for first 20 components\n",
    "num_components_to_show = 20\n",
    "ax1.bar(range(1, num_components_to_show + 1), explained_variance_ratio_all[:num_components_to_show],\n",
    "        alpha=0.7, color='steelblue')\n",
    "ax1.set_xlabel('Principal Component Index')\n",
    "ax1.set_ylabel('Explained Variance Ratio')\n",
    "ax1.set_title(f'Variance Explained by First {num_components_to_show} Components')\n",
    "ax1.grid(True, alpha=0.3, axis='y', linestyle='--')\n",
    "ax1.set_xticks(range(1, num_components_to_show + 1, 2)) # Adjust x-ticks\n",
    "\n",
    "# 累积方差解释率\n",
    "ax2.plot(range(1, len(cumsum_ratio) + 1), cumsum_ratio, 'bo-', markersize=5, alpha=0.7)\n",
    "ax2.axhline(y=0.99, color='indigo', linestyle='--', label='99% variance')\n",
    "ax2.axhline(y=0.95, color='red', linestyle='--', label='95% variance')\n",
    "ax2.axhline(y=0.90, color='green', linestyle='--', label='90% variance')\n",
    "# ax2.axhline(y=0.80, color='orange', linestyle='--', label='80% variance') # Original\n",
    "ax2.set_xlabel('Number of Principal Components')\n",
    "ax2.set_ylabel('Cumulative Explained Variance Ratio')\n",
    "ax2.set_title('Cumulative Variance Explained vs. Num Components')\n",
    "ax2.legend(loc='center right')\n",
    "ax2.grid(True, alpha=0.3, linestyle='--')\n",
    "ax2.set_ylim(0, 1.05) # Set y-limit for better view\n",
    "ax2.set_xlim(0, len(cumsum_ratio) + 1) # Set x-limit\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 找到解释95%方差所需的组件数\n",
    "n_components_95 = np.argmax(cumsum_ratio >= 0.95) + 1 # +1 because argmax returns 0-based index\n",
    "print(f\"\\nComponents needed to explain at least 95% of variance: {n_components_95}\")\n",
    "print(f\"Dimensionality reduction from {X_digits_scaled.shape[1]} to {n_components_95} features.\")\n",
    "# Original calculation was: ({n_components_95/X_digits_scaled.shape[1]:.1%}), this is percentage of original dimensions\n",
    "print(f\"This is {n_components_95/X_digits_scaled.shape[1]:.1%} of the original number of features.\")\n",
    "```\n",
    "\n",
    "---\n",
    "11.2.3 PCA用于图像压缩\n",
    "```python\n",
    "# 加载人脸数据集\n",
    "# fetch_olivetti_faces might require internet and has specific usage terms.\n",
    "# For robustness, let's check if data was fetched.\n",
    "try:\n",
    "    faces = fetch_olivetti_faces(shuffle=True, random_state=42)\n",
    "    X_faces = faces.data\n",
    "    # Data is typically float64 in range [0,1]. No scaling needed if already normalized.\n",
    "    # If not, scaling to [0,1] or StandardScaler can be applied.\n",
    "    # X_faces = X_faces / X_faces.max() # Simple normalization if needed\n",
    "    n_samples_faces, n_features_faces = X_faces.shape # Renamed variables\n",
    "    image_shape = (64, 64) # Olivetti faces are 64x64\n",
    "\n",
    "    # 不同数量的主成分\n",
    "    n_components_list_img = [10, 25, 50, 100, 200, 300] # Adjusted list slightly, max is 400 faces * features\n",
    "\n",
    "    num_images_to_show = 3\n",
    "    fig, axes = plt.subplots(num_images_to_show, len(n_components_list_img) + 1,\n",
    "                             figsize=(18, num_images_to_show * 2.5), # Adjusted figsize\n",
    "                             facecolor='white',\n",
    "                             gridspec_kw={'wspace': 0.1, 'hspace': 0.1}) # Adjust spacing\n",
    "\n",
    "    # 选择几张人脸 (e.g., first 3 after shuffling)\n",
    "    selected_face_indices = [0, 1, 2]\n",
    "\n",
    "    for i, face_idx in enumerate(selected_face_indices):\n",
    "        # 原始图像\n",
    "        axes[i, 0].imshow(X_faces[face_idx].reshape(image_shape), cmap='gray')\n",
    "        if i == 0:\n",
    "            axes[i, 0].set_title('Original', fontsize=10)\n",
    "        axes[i, 0].axis('off')\n",
    "\n",
    "        # 不同压缩率的重建\n",
    "        for j, n_comp in enumerate(n_components_list_img): # Renamed n_components\n",
    "            if n_comp > X_faces.shape[0] or n_comp > X_faces.shape[1]: # Cannot have more components than samples or features\n",
    "                n_comp = min(X_faces.shape[0], X_faces.shape[1])\n",
    "\n",
    "            pca_img = PCA(n_components=n_comp, random_state=42) # Added random_state for reproducibility\n",
    "            X_pca_faces = pca_img.fit_transform(X_faces) # Fit on all faces\n",
    "            X_reconstructed_face = pca_img.inverse_transform(X_pca_faces[face_idx:face_idx+1]) # Reconstruct one face\n",
    "\n",
    "            # 显示重建图像\n",
    "            axes[i, j + 1].imshow(X_reconstructed_face.reshape(image_shape), cmap='gray')\n",
    "            if i == 0:\n",
    "                # compression_ratio = n_comp / n_features_faces # This is ratio of components to original features\n",
    "                # More accurately for storage, it's (n_samples * n_comp + n_comp * n_features) vs (n_samples * n_features)\n",
    "                # For simplicity, let's use the original definition.\n",
    "                axes[i, j + 1].set_title(f'{n_comp} PCs\\n({n_comp/n_features_faces:.1%})', fontsize=10)\n",
    "            axes[i, j + 1].axis('off')\n",
    "\n",
    "    plt.suptitle('PCA for Image Compression (Olivetti Faces)', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # Adjust layout for suptitle\n",
    "    plt.show()\n",
    "\n",
    "    # 计算重建误差 vs. number of components\n",
    "    reconstruction_errors_mse = [] # Renamed\n",
    "    components_range = range(1, min(n_features_faces, n_samples_faces), 10) # Iterate up to min(n_features, n_samples)\n",
    "    if not components_range: components_range = [10, 20, 50, 100] # Default if range is empty\n",
    "\n",
    "    for n_comp_err in components_range: # Renamed\n",
    "        pca_err = PCA(n_components=n_comp_err, random_state=42)\n",
    "        X_pca_err = pca_err.fit_transform(X_faces)\n",
    "        X_reconstructed_err = pca_err.inverse_transform(X_pca_err)\n",
    "\n",
    "        # 计算MSE (Mean Squared Error)\n",
    "        mse = np.mean((X_faces - X_reconstructed_err) ** 2)\n",
    "        reconstruction_errors_mse.append(mse)\n",
    "\n",
    "    plt.figure(figsize=(10, 6), facecolor='white')\n",
    "    plt.plot(list(components_range), reconstruction_errors_mse, 'b-', marker='o', markersize=5)\n",
    "    plt.xlabel('Number of Principal Components')\n",
    "    plt.ylabel('Reconstruction Error (MSE)')\n",
    "    plt.title('PCA Compression: Reconstruction Error vs. Number of Components')\n",
    "    plt.grid(True, alpha=0.5, linestyle='--')\n",
    "    plt.show()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Could not load or process Olivetti faces dataset: {e}\")\n",
    "    print(\"Skipping PCA for image compression example.\")\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "11.2.4 PCA主成分可视化\n",
    "```python\n",
    "# 可视化主成分（使用手写数字数据 - X_digits_scaled from 11.2.1)\n",
    "# Fit PCA with a certain number of components, e.g., 10 as in original\n",
    "n_components_viz = 10\n",
    "pca_viz = PCA(n_components=n_components_viz)\n",
    "pca_viz.fit(X_digits_scaled) # Use scaled data\n",
    "\n",
    "# 显示前n_components_viz个主成分\n",
    "# Each principal component is a vector in the original feature space (64 dimensions for digits)\n",
    "# It can be reshaped into an 8x8 image.\n",
    "fig, axes = plt.subplots(2, (n_components_viz + 1) // 2, figsize=(15, 6), facecolor='white') # Adjusted for 2 rows\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(n_components_viz):\n",
    "    ax = axes[i]\n",
    "    # pca_viz.components_[i] is the i-th principal component vector\n",
    "    pc_image = pca_viz.components_[i].reshape(8, 8)\n",
    "    # Use a diverging colormap to show positive and negative contributions\n",
    "    cmap_pc = 'RdBu_r' # Reversed Red-Blue\n",
    "    vmax_abs = np.max(np.abs(pc_image)) # For symmetrical color scale\n",
    "    img = ax.imshow(pc_image, cmap=cmap_pc, interpolation='nearest', vmin=-vmax_abs, vmax=vmax_abs)\n",
    "    ax.set_title(f'PC {i+1}\\n({pca_viz.explained_variance_ratio_[i]:.2%} var)', fontsize=10)\n",
    "    ax.axis('off')\n",
    "    # Add a colorbar to one of the subplots for reference\n",
    "    if i == 0:\n",
    "        cbar = fig.colorbar(img, ax=ax, orientation='vertical', fraction=0.046, pad=0.04)\n",
    "        cbar.set_label('Component Weight')\n",
    "\n",
    "\n",
    "# Hide any unused subplots\n",
    "for j in range(i + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.suptitle(f'First {n_components_viz} Principal Components of Digits Dataset (as Images)', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "11.3 t-SNE（t-分布随机邻域嵌入）\n",
    "11.3.1 t-SNE基础应用\n",
    "```python\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# t-SNE是计算密集型的，使用部分数据 (using X_digits and y_digits from 11.2.1)\n",
    "n_samples_tsne = 1000\n",
    "if X_digits.shape[0] > n_samples_tsne:\n",
    "    indices_tsne = np.random.choice(X_digits.shape[0], n_samples_tsne, replace=False)\n",
    "    X_subset_tsne = X_digits[indices_tsne]\n",
    "    y_subset_tsne = y_digits[indices_tsne]\n",
    "else:\n",
    "    X_subset_tsne = X_digits\n",
    "    y_subset_tsne = y_digits\n",
    "\n",
    "\n",
    "# 先用PCA降维以加速t-SNE and sometimes improve results (reduce noise)\n",
    "# Target dimensionality for PCA pre-processing is often around 30-50\n",
    "n_pca_for_tsne = min(50, X_subset_tsne.shape[0]-1, X_subset_tsne.shape[1]) # Ensure n_components is valid\n",
    "if n_pca_for_tsne > 1: # Only apply PCA if it makes sense\n",
    "    print(f\"Applying PCA to reduce to {n_pca_for_tsne} components before t-SNE...\")\n",
    "    pca_tsne_pre = PCA(n_components=n_pca_for_tsne, random_state=42)\n",
    "    X_pca_subset_tsne = pca_tsne_pre.fit_transform(X_subset_tsne) # No need to scale again if X_digits was already pixels\n",
    "else:\n",
    "    print(\"Skipping PCA pre-reduction for t-SNE due to low dimensionality/sample size.\")\n",
    "    X_pca_subset_tsne = X_subset_tsne # Use original subset if PCA is not beneficial\n",
    "\n",
    "# 应用t-SNE\n",
    "# Common perplexity values are between 5 and 50.\n",
    "# learning_rate can also be tuned.\n",
    "print(f\"Running t-SNE on data with shape {X_pca_subset_tsne.shape}...\")\n",
    "tsne_2d = TSNE(n_components=2, perplexity=30, learning_rate='auto', # 'auto' for LR (new in 1.0)\n",
    "             init='pca', # PCA initialization is often good\n",
    "             random_state=42, n_jobs=-1) # Use all processors\n",
    "X_tsne_2d = tsne_2d.fit_transform(X_pca_subset_tsne)\n",
    "\n",
    "# 可视化\n",
    "plt.figure(figsize=(10, 8), facecolor='white')\n",
    "scatter = plt.scatter(X_tsne_2d[:, 0], X_tsne_2d[:, 1], c=y_subset_tsne,\n",
    "                     cmap='tab10', alpha=0.7, edgecolor='none', s=40)\n",
    "plt.xlabel('t-SNE Component 1')\n",
    "plt.ylabel('t-SNE Component 2')\n",
    "plt.title(f't-SNE Visualization of Digits Subset ({n_samples_tsne} samples)')\n",
    "plt.colorbar(scatter, label='Digit Class')\n",
    "plt.grid(True, alpha=0.3, linestyle='--')\n",
    "plt.show()\n",
    "\n",
    "print(f\"t-SNE KL divergence: {tsne_2d.kl_divergence_:.3f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "11.3.2 perplexity参数的影响\n",
    "```python\n",
    "# 测试不同的perplexity值 (using X_pca_subset_tsne and y_subset_tsne from 11.3.1)\n",
    "perplexities_test = [5, 15, 30, 50, 80] # Added 80\n",
    "# Check if data is available\n",
    "if 'X_pca_subset_tsne' not in locals() or X_pca_subset_tsne is None:\n",
    "    print(\"Skipping perplexity test as input data is not available.\")\n",
    "else:\n",
    "    num_perplexities = len(perplexities_test)\n",
    "    fig_cols_perp = min(num_perplexities, 3) # Max 3 columns\n",
    "    fig_rows_perp = (num_perplexities + fig_cols_perp -1) // fig_cols_perp\n",
    "    fig, axes = plt.subplots(fig_rows_perp, fig_cols_perp, figsize=(fig_cols_perp * 5, fig_rows_perp * 5),\n",
    "                             facecolor='white', sharex=True, sharey=True)\n",
    "    axes = axes.flatten() if num_perplexities > 1 else [axes]\n",
    "\n",
    "\n",
    "    for i, perplexity_val in enumerate(perplexities_test): # Renamed\n",
    "        print(f\"Running t-SNE with perplexity={perplexity_val}...\")\n",
    "        tsne_perp = TSNE(n_components=2, perplexity=perplexity_val, learning_rate='auto',\n",
    "                       init='pca', random_state=42, n_jobs=-1)\n",
    "        X_tsne_perp = tsne_perp.fit_transform(X_pca_subset_tsne)\n",
    "\n",
    "        ax = axes[i]\n",
    "        scatter = ax.scatter(X_tsne_perp[:, 0], X_tsne_perp[:, 1], c=y_subset_tsne,\n",
    "                            cmap='tab10', alpha=0.6, s=20) # s=20 from original\n",
    "        ax.set_title(f'Perplexity = {perplexity_val}\\nKL div: {tsne_perp.kl_divergence_:.2f}', fontsize=10)\n",
    "        ax.set_xlabel('t-SNE 1')\n",
    "        ax.set_ylabel('t-SNE 2')\n",
    "        ax.set_xticks([]) # Hide ticks for cleaner look\n",
    "        ax.set_yticks([])\n",
    "\n",
    "    # Hide any unused subplots\n",
    "    for j_ax in range(i + 1, len(axes)):\n",
    "        fig.delaxes(axes[j_ax])\n",
    "\n",
    "    plt.suptitle('Effect of Perplexity on t-SNE (Digits Subset)', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\nPerplexity interpretation:\")\n",
    "    print(\"- Perplexity is related to the number of nearest neighbors considered for each point.\")\n",
    "    print(\"- Low perplexity (e.g., 2-5): Focuses on very local structure, can lead to fragmented clusters.\")\n",
    "    print(\"- High perplexity (e.g., >50 for this sample size): Considers more global structure, can smooth out details.\")\n",
    "    print(\"- Typical range: 5 to 50. Should be less than n_samples - 1.\")\n",
    "    print(\"- Results can vary with perplexity, it's a key parameter to tune.\")\n",
    "```\n",
    "\n",
    "---\n",
    "11.3.3 PCA vs t-SNE比较\n",
    "```python\n",
    "# 使用三个不同的数据集比较PCA和t-SNE\n",
    "from sklearn.datasets import make_swiss_roll\n",
    "\n",
    "# 数据集1: 线性可分 Blobs\n",
    "X1_comp, y1_comp = make_blobs(n_samples=500, centers=4, n_features=30, # Increased samples and features\n",
    "                            cluster_std=1.5, random_state=42) # Increased std\n",
    "\n",
    "# 数据集2: 非线性（瑞士卷）\n",
    "X2_comp, color2_comp = make_swiss_roll(n_samples=1000, noise=0.2, random_state=42) # Added noise\n",
    "y2_comp = color2_comp  # Use color for labeling swiss roll\n",
    "\n",
    "# 数据集3: 手写数字（采样 from 11.2.1, X_digits, y_digits)\n",
    "n_samples_digits_comp = 500\n",
    "indices_digits_comp = np.random.choice(X_digits.shape[0], n_samples_digits_comp, replace=False)\n",
    "X3_comp = X_digits[indices_digits_comp]\n",
    "y3_comp = y_digits[indices_digits_comp]\n",
    "\n",
    "\n",
    "datasets_comp_dimred = [ # Renamed\n",
    "    (X1_comp, y1_comp, \"Blobs (Linear-ish)\"),\n",
    "    (X2_comp, y2_comp, \"Swiss Roll (Nonlinear)\"),\n",
    "    (X3_comp, y3_comp, \"Digits Subset (Complex)\")\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(len(datasets_comp_dimred), 2, figsize=(11, 14), facecolor='white') # Adjusted fig size\n",
    "plt.subplots_adjust(hspace=0.3) # Add space between rows\n",
    "\n",
    "for i, (X_data_cr, y_data_cr, name_cr) in enumerate(datasets_comp_dimred): # Renamed variables\n",
    "    # Standardize data before PCA/t-SNE for comparability\n",
    "    X_data_cr_scaled = StandardScaler().fit_transform(X_data_cr)\n",
    "\n",
    "    # PCA\n",
    "    pca_cr = PCA(n_components=2, random_state=42)\n",
    "    X_pca_cr = pca_cr.fit_transform(X_data_cr_scaled)\n",
    "\n",
    "    # t-SNE\n",
    "    # Pre-reduce with PCA if high-dimensional for t-SNE speed and stability\n",
    "    if X_data_cr_scaled.shape[1] > 50:\n",
    "        n_pca_tsne_cr = 50\n",
    "        print(f\"For {name_cr}: Pre-reducing to {n_pca_tsne_cr} dims for t-SNE...\")\n",
    "        X_data_for_tsne = PCA(n_components=n_pca_tsne_cr, random_state=42).fit_transform(X_data_cr_scaled)\n",
    "    else:\n",
    "        X_data_for_tsne = X_data_cr_scaled # Use scaled data directly\n",
    "\n",
    "    print(f\"For {name_cr}: Running t-SNE...\")\n",
    "    tsne_cr = TSNE(n_components=2, perplexity=30, learning_rate='auto',\n",
    "                 init='pca', random_state=42, n_jobs=-1)\n",
    "    X_tsne_cr = tsne_cr.fit_transform(X_data_for_tsne)\n",
    "\n",
    "    # Determine colormap: 'viridis' for continuous (Swiss Roll color), 'tab10' for categorical\n",
    "    cmap_cr = 'viridis' if name_cr == \"Swiss Roll (Nonlinear)\" else 'tab10'\n",
    "\n",
    "    # 绘制PCA\n",
    "    ax_pca = axes[i, 0]\n",
    "    ax_pca.scatter(X_pca_cr[:, 0], X_pca_cr[:, 1], c=y_data_cr,\n",
    "                        cmap=cmap_cr, alpha=0.7, s=20, edgecolor='none')\n",
    "    ax_pca.set_title(f'{name_cr} - PCA')\n",
    "    ax_pca.set_xlabel('PC 1')\n",
    "    ax_pca.set_ylabel('PC 2')\n",
    "    ax_pca.set_xticks([])\n",
    "    ax_pca.set_yticks([])\n",
    "    ax_pca.grid(True, alpha=0.2, linestyle='--')\n",
    "\n",
    "    # 绘制t-SNE\n",
    "    ax_tsne = axes[i, 1]\n",
    "    ax_tsne.scatter(X_tsne_cr[:, 0], X_tsne_cr[:, 1], c=y_data_cr,\n",
    "                        cmap=cmap_cr, alpha=0.7, s=20, edgecolor='none')\n",
    "    ax_tsne.set_title(f'{name_cr} - t-SNE (perplexity=30)')\n",
    "    ax_tsne.set_xlabel('t-SNE 1')\n",
    "    ax_tsne.set_ylabel('t-SNE 2')\n",
    "    ax_tsne.set_xticks([])\n",
    "    ax_tsne.set_yticks([])\n",
    "    ax_tsne.grid(True, alpha=0.2, linestyle='--')\n",
    "\n",
    "\n",
    "fig.suptitle(\"PCA vs. t-SNE on Various Datasets\", fontsize=16, fontweight='bold')\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nPCA vs t-SNE Summary:\")\n",
    "print(\"- PCA: Linear, deterministic, fast. Preserves global variance. Good for dimensionality reduction before other algorithms.\")\n",
    "print(\"- t-SNE: Nonlinear, stochastic (results can vary slightly between runs unless random_state is fixed). Slower. Excellent for visualizing local structure and well-separated clusters.\")\n",
    "print(\"  t-SNE aims to preserve local similarities; distances between clusters in t-SNE map are not always meaningful.\")\n",
    "print(\"- For t-SNE, often beneficial to apply PCA first if original dimension is very high.\")\n",
    "```\n",
    "\n",
    "---\n",
    "11.4 特征重要性分析\n",
    "```python\n",
    "# 使用随机森林进行特征重要性分析\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# 使用乳腺癌数据集\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "cancer = load_breast_cancer()\n",
    "X_cancer = cancer.data\n",
    "y_cancer = cancer.target\n",
    "feature_names_cancer = cancer.feature_names\n",
    "\n",
    "# Scale data for potentially better RF performance, though tree models are less sensitive to feature scaling\n",
    "scaler_cancer = StandardScaler()\n",
    "X_cancer_scaled = scaler_cancer.fit_transform(X_cancer)\n",
    "\n",
    "# 训练随机森林\n",
    "# n_estimators: number of trees in the forest\n",
    "rf_feat_imp = RandomForestClassifier(n_estimators=100, random_state=42, oob_score=True) # Added oob_score\n",
    "rf_feat_imp.fit(X_cancer_scaled, y_cancer)\n",
    "print(f\"Random Forest OOB score: {rf_feat_imp.oob_score_:.4f}\")\n",
    "\n",
    "\n",
    "# 获取特征重要性 (Mean Decrease in Impurity - MDI)\n",
    "importances_mdi = rf_feat_imp.feature_importances_\n",
    "indices_mdi = np.argsort(importances_mdi)[::-1] # Sort in descending order\n",
    "\n",
    "# 可视化特征重要性\n",
    "num_features_to_plot = 20 # Plot top N features\n",
    "plt.figure(figsize=(12, 8), facecolor='white')\n",
    "plt.bar(range(min(num_features_to_plot, X_cancer_scaled.shape[1])),\n",
    "        importances_mdi[indices_mdi][:num_features_to_plot], # Select top N\n",
    "        color='skyblue', align='center')\n",
    "plt.xticks(range(min(num_features_to_plot, X_cancer_scaled.shape[1])),\n",
    "           feature_names_cancer[indices_mdi][:num_features_to_plot], # Select top N names\n",
    "           rotation=90, ha='right')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Feature Importance (MDI)')\n",
    "plt.title(f'Random Forest Feature Importance (Top {num_features_to_plot} Features - Breast Cancer Dataset)')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 基于重要性选择特征\n",
    "# SelectFromModel can use a threshold for importance or max_features\n",
    "# Default threshold is \"mean\". We can also use a percentile or a specific value.\n",
    "selector_sfm = SelectFromModel(rf_feat_imp, threshold='median', prefit=True) # Using median as threshold\n",
    "X_cancer_selected = selector_sfm.transform(X_cancer_scaled)\n",
    "selected_feature_indices = selector_sfm.get_support(indices=True)\n",
    "selected_feature_names = feature_names_cancer[selected_feature_indices]\n",
    "\n",
    "\n",
    "print(f\"\\nOriginal number of features: {X_cancer_scaled.shape[1]}\")\n",
    "print(f\"Number of features selected by SelectFromModel (threshold='median'): {X_cancer_selected.shape[1]}\")\n",
    "print(f\"Selected feature names: {selected_feature_names}\")\n",
    "\n",
    "print(\"\\nTop 10 most important features (MDI):\")\n",
    "for i in range(min(10, len(indices_mdi))):\n",
    "    print(f\"{i+1}. {feature_names_cancer[indices_mdi[i]]}: {importances_mdi[indices_mdi[i]]:.4f}\")\n",
    "\n",
    "\n",
    "# Alternative: Permutation Importance (more robust, but computationally more expensive)\n",
    "try:\n",
    "    from sklearn.inspection import permutation_importance\n",
    "    print(\"\\nCalculating Permutation Importance (can be slow)...\")\n",
    "    perm_importance = permutation_importance(rf_feat_imp, X_cancer_scaled, y_cancer,\n",
    "                                             n_repeats=10, random_state=42, n_jobs=-1)\n",
    "    importances_perm = perm_importance.importances_mean\n",
    "    indices_perm = np.argsort(importances_perm)[::-1]\n",
    "\n",
    "    plt.figure(figsize=(12, 8), facecolor='white')\n",
    "    plt.bar(range(min(num_features_to_plot, X_cancer_scaled.shape[1])),\n",
    "            importances_perm[indices_perm][:num_features_to_plot],\n",
    "            color='lightcoral', align='center', yerr=perm_importance.importances_std[indices_perm][:num_features_to_plot]) # Add error bars\n",
    "    plt.xticks(range(min(num_features_to_plot, X_cancer_scaled.shape[1])),\n",
    "               feature_names_cancer[indices_perm][:num_features_to_plot],\n",
    "               rotation=90, ha='right')\n",
    "    plt.xlabel('Features')\n",
    "    plt.ylabel('Feature Importance (Permutation)')\n",
    "    plt.title(f'Permutation Feature Importance (Top {num_features_to_plot} Features - Breast Cancer Dataset)')\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\nTop 10 most important features (Permutation):\")\n",
    "    for i in range(min(10, len(indices_perm))):\n",
    "        print(f\"{i+1}. {feature_names_cancer[indices_perm[i]]}: {importances_perm[indices_perm[i]]:.4f} +/- {perm_importance.importances_std[indices_perm[i]]:.4f}\")\n",
    "\n",
    "except ImportError:\n",
    "    print(\"\\nPermutation importance requires scikit-learn >= 0.22. Skipping.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nError calculating permutation importance: {e}. Skipping.\")\n",
    "```\n",
    "\n",
    "---\n",
    "11.5 降维方法综合比较\n",
    "```python\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from time import time\n",
    "\n",
    "# 准备数据 (using X_digits, y_digits from 11.2.1)\n",
    "n_samples_comp_all = 1000 # Renamed\n",
    "if X_digits.shape[0] > n_samples_comp_all:\n",
    "    indices_comp_all = np.random.choice(X_digits.shape[0], n_samples_comp_all, replace=False)\n",
    "    X_comparison_all = X_digits[indices_comp_all] # Renamed\n",
    "    y_comparison_all = y_digits[indices_comp_all] # Renamed\n",
    "else:\n",
    "    X_comparison_all = X_digits\n",
    "    y_comparison_all = y_digits\n",
    "\n",
    "# Scale data\n",
    "X_comparison_scaled = StandardScaler().fit_transform(X_comparison_all)\n",
    "\n",
    "# 定义降维方法\n",
    "# For LDA, n_components cannot be more than min(n_classes - 1, n_features)\n",
    "n_classes_comp = len(np.unique(y_comparison_all))\n",
    "lda_n_components = min(2, n_classes_comp - 1) # Target 2 components, but respect LDA limit\n",
    "\n",
    "methods_comp_all = { # Renamed\n",
    "    'PCA': PCA(n_components=2, random_state=42),\n",
    "    # LDA requires y for fit, so it's supervised\n",
    "    'LDA': LinearDiscriminantAnalysis(n_components=lda_n_components if lda_n_components > 0 else None),\n",
    "    't-SNE': TSNE(n_components=2, perplexity=30, learning_rate='auto', init='pca', random_state=42, n_jobs=-1)\n",
    "}\n",
    "if lda_n_components <=0: # If only 1 class, LDA is not applicable for reduction in this way\n",
    "    print(\"LDA not applicable due to insufficient classes for 2D projection. Removing from comparison.\")\n",
    "    del methods_comp_all['LDA']\n",
    "\n",
    "\n",
    "# 比较不同方法\n",
    "num_methods = len(methods_comp_all)\n",
    "fig, axes = plt.subplots(1, num_methods, figsize=(num_methods * 5.5, 5), facecolor='white', sharex=True, sharey=True) # Adjusted fig size\n",
    "if num_methods == 1: axes = [axes] # Ensure axes is iterable if only one method\n",
    "\n",
    "results_list_comp = [] # Renamed\n",
    "\n",
    "for i, (name_meth, method_obj) in enumerate(methods_comp_all.items()): # Renamed vars\n",
    "    print(f\"Running {name_meth}...\")\n",
    "    start_time = time()\n",
    "\n",
    "    X_reduced_meth = None # Initialize\n",
    "    # 降维\n",
    "    if name_meth == 't-SNE':\n",
    "        # t-SNE often benefits from PCA pre-reduction if original dim is high\n",
    "        n_pca_tsne_comp = min(50, X_comparison_scaled.shape[0]-1, X_comparison_scaled.shape[1])\n",
    "        if n_pca_tsne_comp > 1 and X_comparison_scaled.shape[1] > n_pca_tsne_comp :\n",
    "            X_preprocessed_tsne = PCA(n_components=n_pca_tsne_comp, random_state=42).fit_transform(X_comparison_scaled)\n",
    "        else:\n",
    "            X_preprocessed_tsne = X_comparison_scaled\n",
    "        X_reduced_meth = method_obj.fit_transform(X_preprocessed_tsne)\n",
    "    elif name_meth == 'LDA':\n",
    "        if method_obj.n_components is not None: # Only if LDA is applicable\n",
    "            X_reduced_meth = method_obj.fit_transform(X_comparison_scaled, y_comparison_all)\n",
    "        else: # LDA was deemed not applicable earlier\n",
    "            print(f\"Skipping {name_meth} as it was previously determined inapplicable.\")\n",
    "            # Fill with NaNs or skip plotting for this method\n",
    "            ax = axes[i]\n",
    "            ax.text(0.5, 0.5, f\"{name_meth}\\nNot Applicable\", ha='center', va='center', transform=ax.transAxes, fontsize=10, color='gray')\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            ax.set_title(f'{name_meth}', fontsize=12)\n",
    "\n",
    "    else: # PCA\n",
    "        X_reduced_meth = method_obj.fit_transform(X_comparison_scaled)\n",
    "\n",
    "    end_time = time()\n",
    "    time_taken = end_time - start_time\n",
    "\n",
    "    # 绘图\n",
    "    if X_reduced_meth is not None:\n",
    "        ax = axes[i]\n",
    "        scatter = ax.scatter(X_reduced_meth[:, 0], X_reduced_meth[:, 1],\n",
    "                            c=y_comparison_all, cmap='tab10',\n",
    "                            alpha=0.7, s=20, edgecolor='none')\n",
    "        ax.set_title(f'{name_meth}\\n({time_taken:.2f}s)', fontsize=12)\n",
    "        ax.set_xlabel('Component 1')\n",
    "        if i == 0: # Only set y-label for the first plot\n",
    "            ax.set_ylabel('Component 2')\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.grid(True, alpha=0.2, linestyle='--')\n",
    "        ax.set_aspect('equal', adjustable='box') # Make plots comparable\n",
    "\n",
    "\n",
    "    # 记录结果\n",
    "    results_list_comp.append({\n",
    "        'Method': name_meth,\n",
    "        'Time (s)': time_taken if X_reduced_meth is not None else np.nan,\n",
    "        'Supervised': 'Yes' if name_meth == 'LDA' else 'No',\n",
    "        'Output Dim': X_reduced_meth.shape[1] if X_reduced_meth is not None else 'N/A'\n",
    "    })\n",
    "\n",
    "fig.suptitle('Dimensionality Reduction Methods Comparison (Digits Subset)', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()\n",
    "\n",
    "# 显示比较结果\n",
    "results_df_comp = pd.DataFrame(results_list_comp) # Renamed\n",
    "print(\"\\nDimensionality Reduction Comparison Results:\")\n",
    "print(results_df_comp.round({'Time (s)': 3})) # Round time\n",
    "\n",
    "print(\"\\nKey Differences Summary:\")\n",
    "print(\"- PCA: Unsupervised, linear. Maximizes variance preservation. Fast. Components are orthogonal.\")\n",
    "print(\"- LDA: Supervised, linear. Maximizes class separability. Requires labels. Output dimension limited by n_classes-1.\")\n",
    "print(\"- t-SNE: Unsupervised, nonlinear. Preserves local structure/similarity. Excellent for visualization. Computationally more intensive. Distances in t-SNE map are not directly interpretable as global distances.\")\n",
    "```\n",
    "\n",
    "---\n",
    "聚类和降维实践总结\n",
    "最佳实践建议\n",
    "```python\n",
    "# 创建一个完整的无监督学习流程\n",
    "# (Helper functions find_optimal_k_elbow and find_optimal_eps_k_distance need to be defined or imported)\n",
    "\n",
    "# Simplified Elbow method for K-Means (from original, can be improved)\n",
    "def find_optimal_k_elbow(X_data, k_range=range(2, 11), random_state=42):\n",
    "    \"\"\"使用肘部法则找最佳k值 (simple version)\"\"\"\n",
    "    if X_data.shape[0] < max(k_range): # Ensure enough samples for max k\n",
    "        k_range = range(2, X_data.shape[0]) if X_data.shape[0] > 2 else [2]\n",
    "        if not k_range: return 2 # Default if not enough samples\n",
    "\n",
    "    inertias_k = []\n",
    "    for k_val in k_range:\n",
    "        kmeans = KMeans(n_clusters=k_val, random_state=random_state, n_init='auto')\n",
    "        kmeans.fit(X_data)\n",
    "        inertias_k.append(kmeans.inertia_)\n",
    "\n",
    "    if len(inertias_k) < 3: # Need at least 3 points for second derivative\n",
    "        # Fallback: choose k that gives biggest drop if only 2 points, or default if 1.\n",
    "        return k_range[np.argmin(inertias_k)] if inertias_k else 2\n",
    "\n",
    "    # Simple elbow detection (derivative-based, can be brittle)\n",
    "    # Using kneed library is more robust if available\n",
    "    try:\n",
    "        from kneed import KneeLocator\n",
    "        kn = KneeLocator(list(k_range), inertias_k, curve='convex', direction='decreasing')\n",
    "        return kn.elbow if kn.elbow else k_range[len(k_range)//2] # Fallback to middle k if no elbow\n",
    "    except ImportError:\n",
    "        print(\"Kneed library not found, using simple diff for elbow. `pip install kneed` for better results.\")\n",
    "        diffs1 = np.diff(inertias_k)\n",
    "        diffs2 = np.diff(diffs1)\n",
    "        if len(diffs2) > 0:\n",
    "            elbow_idx = np.argmax(diffs2) + 1 # Index in diffs1, corresponds to k_range[idx+1]\n",
    "            return k_range[elbow_idx +1] # +1 for k_range index, +1 because diffs2 starts from 3rd point\n",
    "        return k_range[len(k_range)//2] # Fallback\n",
    "\n",
    "# Simplified k-distance plot for DBSCAN eps (from original, can be improved)\n",
    "def find_optimal_eps_k_distance(X_data, k_neighbors=5):\n",
    "    \"\"\"使用k距离图找最佳eps (simple heuristic version)\"\"\"\n",
    "    if X_data.shape[0] <= k_neighbors:\n",
    "        print(\"Not enough samples for k-distance plot, returning default eps=0.5\")\n",
    "        return 0.5\n",
    "    nbrs = NearestNeighbors(n_neighbors=k_neighbors).fit(X_data)\n",
    "    distances, _ = nbrs.kneighbors(X_data)\n",
    "    k_distances_eps = np.sort(distances[:, k_neighbors-1])\n",
    "\n",
    "    # Simple elbow detection\n",
    "    try:\n",
    "        from kneed import KneeLocator\n",
    "        kn = KneeLocator(range(len(k_distances_eps)), k_distances_eps, curve='convex', direction='increasing')\n",
    "        return kn.elbow_y if kn.elbow_y else np.median(k_distances_eps) # Fallback to median\n",
    "    except ImportError:\n",
    "        print(\"Kneed library not found for eps estimation. Using simple diff. `pip install kneed`\")\n",
    "        if len(k_distances_eps) > 2:\n",
    "            diffs = np.diff(k_distances_eps)\n",
    "            if len(diffs) > 0:\n",
    "                 elbow_idx = np.argmax(diffs) # Point before the sharpest rise\n",
    "                 return k_distances_eps[elbow_idx] if elbow_idx < len(k_distances_eps) else np.median(k_distances_eps)\n",
    "        return np.median(k_distances_eps) # Fallback\n",
    "\n",
    "\n",
    "def unsupervised_learning_pipeline(X_pipe, algorithm='kmeans', n_clusters_pipe=None, dbscan_eps=None, dbscan_min_samples=5):\n",
    "    \"\"\"\n",
    "    完整的无监督学习管道 (简化版)\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Running Unsupervised Learning Pipeline with {algorithm.upper()} ---\")\n",
    "    # 1. 数据预处理\n",
    "    print(\"Step 1: Scaling data...\")\n",
    "    scaler_pipe = StandardScaler()\n",
    "    X_scaled_pipe = scaler_pipe.fit_transform(X_pipe)\n",
    "\n",
    "    # 2. 降维（可选，这里简化为PCA if features > 50)\n",
    "    X_processed_pipe = X_scaled_pipe\n",
    "    if X_scaled_pipe.shape[1] > 50:\n",
    "        print(\"Step 2: High dimensional data detected. Applying PCA (retaining 95% variance)...\")\n",
    "        pca_pipe = PCA(n_components=0.95, random_state=42)\n",
    "        X_processed_pipe = pca_pipe.fit_transform(X_scaled_pipe)\n",
    "        print(f\"Reduced dimensions from {X_scaled_pipe.shape[1]} to {X_processed_pipe.shape[1]}\")\n",
    "    else:\n",
    "        print(\"Step 2: Dimensionality reduction (PCA) not applied (features <= 50).\")\n",
    "\n",
    "\n",
    "    # 3. 聚类\n",
    "    print(f\"Step 3: Applying {algorithm.upper()} clustering...\")\n",
    "    clustering_model = None\n",
    "    labels_pipe = None\n",
    "\n",
    "    if algorithm == 'kmeans':\n",
    "        if n_clusters_pipe is None:\n",
    "            print(\"   Finding optimal K for K-Means using elbow method...\")\n",
    "            n_clusters_pipe = find_optimal_k_elbow(X_processed_pipe, k_range=range(2, min(11, X_processed_pipe.shape[0])))\n",
    "            print(f\"   Optimal K found: {n_clusters_pipe}\")\n",
    "        clustering_model = KMeans(n_clusters=n_clusters_pipe, random_state=42, n_init='auto')\n",
    "    elif algorithm == 'dbscan':\n",
    "        if dbscan_eps is None:\n",
    "            print(f\"   Finding optimal eps for DBSCAN using {dbscan_min_samples}-distance plot...\")\n",
    "            dbscan_eps = find_optimal_eps_k_distance(X_processed_pipe, k_neighbors=dbscan_min_samples)\n",
    "            print(f\"   Optimal eps found: {dbscan_eps:.3f}\")\n",
    "        clustering_model = DBSCAN(eps=dbscan_eps, min_samples=dbscan_min_samples)\n",
    "    else:\n",
    "        print(f\"Algorithm {algorithm} not supported in this simplified pipeline.\")\n",
    "        return None, None\n",
    "\n",
    "    if clustering_model:\n",
    "        labels_pipe = clustering_model.fit_predict(X_processed_pipe)\n",
    "\n",
    "\n",
    "    # 4. 评估 (Silhouette Score)\n",
    "    if labels_pipe is not None:\n",
    "        num_unique_labels = len(set(labels_pipe))\n",
    "        # Adjust for DBSCAN where -1 is noise and not a cluster\n",
    "        num_clusters_for_eval = num_unique_labels - (1 if -1 in labels_pipe and algorithm == 'dbscan' else 0)\n",
    "\n",
    "        if num_clusters_for_eval > 1 and (len(labels_pipe) - list(labels_pipe).count(-1)) > 1 : # Ensure enough clusters and non-noise points\n",
    "            # Filter noise for silhouette if DBSCAN\n",
    "            eval_mask = labels_pipe != -1 if algorithm == 'dbscan' else np.ones_like(labels_pipe, dtype=bool)\n",
    "            if np.sum(eval_mask) > 1 and len(set(labels_pipe[eval_mask])) > 1:\n",
    "                 silhouette_val = silhouette_score(X_processed_pipe[eval_mask], labels_pipe[eval_mask])\n",
    "                 print(f\"\\nStep 4: Evaluation - Silhouette Score: {silhouette_val:.3f}\")\n",
    "                 print(f\"   (Number of clusters found by {algorithm.upper()}: {num_clusters_for_eval}, Noise points: {list(labels_pipe).count(-1) if algorithm == 'dbscan' else 0})\")\n",
    "\n",
    "            else:\n",
    "                print(\"\\nStep 4: Evaluation - Not enough clusters/points after noise removal for Silhouette Score.\")\n",
    "        else:\n",
    "            print(f\"\\nStep 4: Evaluation - Silhouette Score not applicable (found {num_clusters_for_eval} cluster(s)).\")\n",
    "\n",
    "\n",
    "    # 5. 可视化 (2D using t-SNE if needed)\n",
    "    print(\"Step 5: Visualizing clustering results...\")\n",
    "    X_vis_pipe = X_processed_pipe\n",
    "    if X_processed_pipe.shape[1] > 2:\n",
    "        print(\"   Data has >2 dimensions, reducing to 2D using t-SNE for visualization (on subset if large)...\")\n",
    "        # Use a subset for t-SNE if data is very large\n",
    "        n_vis_samples = min(1000, X_processed_pipe.shape[0])\n",
    "        vis_indices = np.random.choice(X_processed_pipe.shape[0], n_vis_samples, replace=False)\n",
    "        tsne_vis = TSNE(n_components=2, perplexity=min(30, n_vis_samples-1), learning_rate='auto', init='pca', random_state=42, n_jobs=-1)\n",
    "        X_vis_pipe = tsne_vis.fit_transform(X_processed_pipe[vis_indices])\n",
    "        labels_vis_pipe = labels_pipe[vis_indices] if labels_pipe is not None else None\n",
    "    else: # Already 2D or 1D\n",
    "        X_vis_pipe = X_processed_pipe\n",
    "        labels_vis_pipe = labels_pipe\n",
    "\n",
    "    if labels_vis_pipe is not None and X_vis_pipe.shape[1] == 2:\n",
    "        plt.figure(figsize=(10, 8), facecolor='white')\n",
    "        # Handle DBSCAN noise color\n",
    "        if algorithm == 'dbscan':\n",
    "            unique_vis_labels = np.unique(labels_vis_pipe)\n",
    "            colors_vis = {lbl: plt.cm.viridis( (i) / (len(unique_vis_labels) -1) if lbl != -1 else 0) if lbl != -1 else 'red' for i, lbl in enumerate(unique_vis_labels) if lbl !=-1}\n",
    "            if -1 in unique_vis_labels: colors_vis[-1] = 'grey' # Noise as grey\n",
    "            \n",
    "            for k_vis in unique_vis_labels:\n",
    "                class_member_mask_vis = (labels_vis_pipe == k_vis)\n",
    "                plt.scatter(X_vis_pipe[class_member_mask_vis, 0], X_vis_pipe[class_member_mask_vis, 1],\n",
    "                            c=[colors_vis[k_vis]], label=f'Cluster {k_vis}' if k_vis !=-1 else 'Noise',\n",
    "                            alpha=0.7, s=50, edgecolor='none' if k_vis !=-1 else 'black')\n",
    "            plt.legend(title=\"Clusters\", loc=\"best\")\n",
    "\n",
    "        else: # K-Means like\n",
    "             scatter = plt.scatter(X_vis_pipe[:, 0], X_vis_pipe[:, 1], c=labels_vis_pipe,\n",
    "                                 cmap='viridis', alpha=0.7, s=50, edgecolor='none')\n",
    "             plt.colorbar(scatter, label='Cluster ID')\n",
    "\n",
    "        title_str = f'{algorithm.upper()} Clustering Results'\n",
    "        if X_processed_pipe.shape[1] > 2: title_str += ' (t-SNE projection)'\n",
    "        plt.title(title_str)\n",
    "        plt.xlabel('Component 1')\n",
    "        plt.ylabel('Component 2')\n",
    "        plt.grid(True, alpha=0.3, linestyle='--')\n",
    "        plt.show()\n",
    "    elif X_vis_pipe.shape[1] == 1:\n",
    "        print(\"   Data is 1D, plotting histogram of clusters.\")\n",
    "        pd.Series(labels_vis_pipe).value_counts().sort_index().plot(kind='bar', figsize=(8,5), colormap='viridis')\n",
    "        plt.title(f'{algorithm.upper()} Cluster Distribution (1D)')\n",
    "        plt.xlabel('Cluster ID')\n",
    "        plt.ylabel('Number of Samples')\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"   Visualization not performed (data not 2D or labels not available).\")\n",
    "\n",
    "    print(f\"--- Pipeline for {algorithm.upper()} complete. ---\")\n",
    "    return labels_pipe, clustering_model\n",
    "\n",
    "# 示例使用\n",
    "print(\"\\n=== Unsupervised Learning Pipeline Example ===\")\n",
    "X_example_pipe, y_example_pipe_true = make_blobs(n_samples=300, n_features=10, centers=4, random_state=42, cluster_std=1.5)\n",
    "\n",
    "# K-Means example\n",
    "labels_kmeans_pipe, model_kmeans_pipe = unsupervised_learning_pipeline(X_example_pipe, algorithm='kmeans')\n",
    "# You can also specify n_clusters:\n",
    "# labels_kmeans_pipe, model_kmeans_pipe = unsupervised_learning_pipeline(X_example_pipe, algorithm='kmeans', n_clusters_pipe=4)\n",
    "\n",
    "# DBSCAN example\n",
    "labels_dbscan_pipe, model_dbscan_pipe = unsupervised_learning_pipeline(X_example_pipe, algorithm='dbscan', dbscan_min_samples=7)\n",
    "# You can also specify eps:\n",
    "# labels_dbscan_pipe, model_dbscan_pipe = unsupervised_learning_pipeline(X_example_pipe, algorithm='dbscan', dbscan_eps=0.7, dbscan_min_samples=7)\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "方法选择指南\n",
    "```python\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CLUSTERING METHOD SELECTION GUIDE (Summary)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "guide_clustering = \"\"\"\n",
    "1. K-Means:\n",
    "   - Strengths: Simple, fast, scalable to large datasets. Good for general-purpose clustering.\n",
    "   - Weaknesses: Assumes spherical, equally sized clusters. Sensitive to initial centroid placement (use k-means++ and n_init). Must specify K (number of clusters). Can be affected by outliers.\n",
    "   - Use when: You have an idea of K, clusters are roughly spherical, and efficiency is important.\n",
    "\n",
    "2. Hierarchical Clustering (Agglomerative):\n",
    "   - Strengths: Does not require K to be pre-specified (can choose K from dendrogram). Produces a hierarchy of clusters (dendrogram) which can be insightful. Can work with various linkage methods to define cluster distance (ward, complete, average, single).\n",
    "   - Weaknesses: Computationally expensive for large datasets (O(n^2 log n) or O(n^3) depending on linkage). Decisions made early (merges/splits) are permanent (greedy). Can be sensitive to noise and outliers (especially 'single' linkage).\n",
    "   - Use when: You want to explore cluster hierarchy, dataset size is manageable, or you don't know K.\n",
    "\n",
    "3. DBSCAN (Density-Based Spatial Clustering of Applications with Noise):\n",
    "   - Strengths: Can find arbitrarily shaped clusters. Robust to outliers (identifies them as noise). Does not require specifying K.\n",
    "   - Weaknesses: Sensitive to parameters 'eps' (neighborhood radius) and 'min_samples'. Struggles with clusters of varying densities. Can be slow on very high-dimensional data (curse of dimensionality affects distance measures).\n",
    "   - Use when: Clusters have complex shapes, there might be noise/outliers, and you don't want to specify K (but are willing to tune eps and min_samples).\n",
    "\n",
    "Other notable methods:\n",
    "- Gaussian Mixture Models (GMM): Probabilistic, assumes clusters are Gaussian distributions. Soft clustering (assigns probabilities).\n",
    "- Affinity Propagation: Does not require K, selects \"exemplars\".\n",
    "- Mean Shift: Finds modes in density.\n",
    "\"\"\"\n",
    "print(guide_clustering)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DIMENSIONALITY REDUCTION METHOD SELECTION GUIDE (Summary)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "guide_dim_red = \"\"\"\n",
    "1. PCA (Principal Component Analysis):\n",
    "   - Type: Linear, Unsupervised.\n",
    "   - Goal: Maximize variance explained by new components. Components are orthogonal.\n",
    "   - Strengths: Fast, deterministic, interpretable (components show directions of max variance). Good for data compression, noise reduction, and pre-processing.\n",
    "   - Weaknesses: Only captures linear relationships. Can be affected by feature scaling if variances are very different. May not preserve cluster structure well if separation is not along high-variance directions.\n",
    "   - Use when: You need a simple, fast linear reduction, want to preserve global variance, or need interpretable components.\n",
    "\n",
    "2. t-SNE (t-distributed Stochastic Neighbor Embedding):\n",
    "   - Type: Nonlinear, Unsupervised.\n",
    "   - Goal: Preserve local similarities/structure from high-D to low-D (typically 2D or 3D for visualization).\n",
    "   - Strengths: Excellent for visualizing well-separated clusters and complex manifolds. Captures nonlinear structures.\n",
    "   - Weaknesses: Computationally intensive (especially for large N). Stochastic (results can vary slightly). Output is a visualization, not usually for transforming new data. Distances between clusters in t-SNE map are not always meaningful globally. Sensitive to perplexity parameter.\n",
    "   - Use when: Primarily for visualization of high-dimensional data to understand its structure.\n",
    "\n",
    "3. LDA (Linear Discriminant Analysis):\n",
    "   - Type: Linear, Supervised (requires class labels).\n",
    "   - Goal: Maximize separability between known classes.\n",
    "   - Strengths: Finds dimensions that best separate classes. Can be used for classification too.\n",
    "   - Weaknesses: Requires labels. Assumes data is normally distributed and classes have identical covariance matrices (though can work reasonably if violated). Number of components limited by (n_classes - 1) or n_features.\n",
    "   - Use when: You have labeled data and the goal is to find a lower-dimensional space that maximizes class discrimination (e.g., for subsequent classification).\n",
    "\n",
    "4. Feature Selection (e.g., using RandomForest importance, L1 regularization):\n",
    "   - Type: Can be Unsupervised (variance threshold) or Supervised (model-based).\n",
    "   - Goal: Select a subset of the original features.\n",
    "   - Strengths: Maintains original feature interpretability. Can reduce model complexity and overfitting.\n",
    "   - Weaknesses: May discard features that are useful in combination with others (ignores feature interactions if univariate). Model-based selection depends on the chosen model.\n",
    "   - Use when: Interpretability of original features is crucial, or you want to reduce the number of features without transforming them.\n",
    "\n",
    "Other notable methods:\n",
    "- UMAP (Uniform Manifold Approximation and Projection): Similar to t-SNE (nonlinear, for visualization and general dim-red), often faster and better at preserving global structure.\n",
    "- Autoencoders: Neural network-based, can learn complex nonlinear mappings. Can be unsupervised or supervised.\n",
    "\"\"\"\n",
    "print(guide_dim_red)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6914f26b-64ed-4e59-ba4e-ff15add6bb78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs, load_iris, load_digits\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "\n",
    "# 创建示例数据\n",
    "X, y_true = make_blobs(n_samples=300, centers=4, n_features=2,\n",
    "                       cluster_std=0.6, random_state=42)\n",
    "\n",
    "# 可视化原始数据\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X[:, 0], X[:, 1], s=50, alpha=0.8)\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Original Data')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# 应用K-Means\n",
    "kmeans = KMeans(n_clusters=4, random_state=42, n_init='auto') # Added n_init for future versions\n",
    "y_kmeans = kmeans.fit_predict(X)\n",
    "\n",
    "# 可视化聚类结果\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='viridis', alpha=0.8)\n",
    "centers = kmeans.cluster_centers_\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='red', s=200, alpha=0.8,\n",
    "            marker='*', edgecolor='black', linewidth=2)\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('K-Means Clustering Results')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Cluster centers:\\n{centers}\")\n",
    "print(f\"Inertia (within-cluster sum of squares): {kmeans.inertia_:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52eeba8-b392-4bcf-9629-817daaccc56f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
