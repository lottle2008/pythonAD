{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "720dfcfb-a5ed-4f30-bc8d-e7761b11f670",
   "metadata": {},
   "source": [
    "# Python与Hugging Face模型互动教程 —— 玩转AI模型\n",
    "\n",
    "## 引言：为什么要学习Hugging Face？\n",
    "\n",
    "在AI时代，Hugging Face已经成为了AI模型的\"GitHub\"。它提供了：\n",
    "- 数万个预训练模型\n",
    "- 简单易用的API\n",
    "- 强大的模型库（Transformers）\n",
    "- 活跃的社区\n",
    "\n",
    "今天，我们将学习如何使用Python与Hugging Face上的模型进行互动，让你能够快速使用各种AI模型。\n",
    "\n",
    "## 一、Hugging Face是什么？\n",
    "\n",
    "### 1.1 Hugging Face简介\n",
    "\n",
    "Hugging Face是一个AI社区和平台，它提供：\n",
    "\n",
    "1. **模型仓库（Model Hub）**\n",
    "   - 超过10万个预训练模型\n",
    "   - 涵盖NLP、计算机视觉、音频等领域\n",
    "   - 支持多种框架（PyTorch、TensorFlow等）\n",
    "\n",
    "2. **Transformers库**\n",
    "   - 统一的API接口\n",
    "   - 支持各种任务（文本生成、分类、翻译等）\n",
    "   - 简化模型使用流程\n",
    "\n",
    "3. **数据集仓库（Dataset Hub）**\n",
    "   - 数千个数据集\n",
    "   - 统一的数据加载接口\n",
    "\n",
    "4. **Spaces**\n",
    "   - 托管AI应用\n",
    "   - 展示模型Demo\n",
    "\n",
    "### 1.2 为什么选择Hugging Face？\n",
    "\n",
    "```python\n",
    "# 传统方式：复杂的模型加载和使用\n",
    "import torch\n",
    "model = torch.load('model.pt')\n",
    "# 需要了解模型结构、预处理方式等...\n",
    "\n",
    "# Hugging Face方式：简单直接\n",
    "from transformers import pipeline\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "result = classifier(\"I love this movie!\")\n",
    "# 就这么简单！\n",
    "```\n",
    "\n",
    "## 二、环境准备\n",
    "\n",
    "### 2.1 安装必要的库\n",
    "\n",
    "```bash\n",
    "# 创建虚拟环境\n",
    "python -m venv hf_env\n",
    "source hf_env/bin/activate  # Windows: hf_env\\Scripts\\activate\n",
    "\n",
    "# 安装基础库\n",
    "pip install transformers\n",
    "pip install torch  # 或 tensorflow\n",
    "pip install datasets\n",
    "pip install huggingface-hub\n",
    "\n",
    "# 安装额外功能（可选）\n",
    "pip install accelerate  # 加速训练\n",
    "pip install sentencepiece  # 某些模型需要\n",
    "pip install protobuf  # 某些模型需要\n",
    "```\n",
    "\n",
    "### 2.2 配置Hugging Face账户（可选）\n",
    "\n",
    "如果你想使用私有模型或上传模型，需要配置token：\n",
    "\n",
    "```python\n",
    "# 方法1：使用命令行\n",
    "# huggingface-cli login\n",
    "\n",
    "# 方法2：在代码中设置\n",
    "from huggingface_hub import login\n",
    "login(token=\"your_token_here\")\n",
    "\n",
    "# 方法3：使用环境变量\n",
    "# export HUGGING_FACE_HUB_TOKEN=\"your_token_here\"\n",
    "```\n",
    "\n",
    "## 三、快速开始：Pipeline API\n",
    "\n",
    "### 3.1 什么是Pipeline？\n",
    "\n",
    "Pipeline是Hugging Face提供的高级API，让你能够快速使用模型，无需了解复杂的细节。\n",
    "\n",
    "```python\n",
    "from transformers import pipeline\n",
    "\n",
    "# 创建一个pipeline就像创建一个函数\n",
    "# 这个函数可以处理特定的任务\n",
    "sentiment_analyzer = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "# 使用就像调用函数一样简单\n",
    "result = sentiment_analyzer(\"I am very happy today!\")\n",
    "print(result)\n",
    "# 输出: [{'label': 'POSITIVE', 'score': 0.9998}]\n",
    "```\n",
    "\n",
    "### 3.2 常用Pipeline示例\n",
    "\n",
    "```python\n",
    "from transformers import pipeline\n",
    "\n",
    "# 1. 情感分析\n",
    "print(\"=== 情感分析 ===\")\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
    "texts = [\n",
    "    \"I love this product!\",\n",
    "    \"This is terrible.\",\n",
    "    \"It's okay, not great but not bad either.\"\n",
    "]\n",
    "for text in texts:\n",
    "    result = sentiment_pipeline(text)\n",
    "    print(f\"文本: {text}\")\n",
    "    print(f\"结果: {result[0]['label']}, 置信度: {result[0]['score']:.4f}\\n\")\n",
    "\n",
    "# 2. 文本生成\n",
    "print(\"=== 文本生成 ===\")\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "prompt = \"Once upon a time\"\n",
    "result = generator(prompt, max_length=50, num_return_sequences=2)\n",
    "for i, text in enumerate(result):\n",
    "    print(f\"生成 {i+1}: {text['generated_text']}\\n\")\n",
    "\n",
    "# 3. 问答系统\n",
    "print(\"=== 问答系统 ===\")\n",
    "qa_pipeline = pipeline(\"question-answering\")\n",
    "context = \"\"\"\n",
    "Hugging Face is a company that develops tools for building applications using machine learning.\n",
    "It is most notable for its Transformers library built for NLP applications.\n",
    "The company was founded in 2016 by French entrepreneurs.\n",
    "\"\"\"\n",
    "question = \"When was Hugging Face founded?\"\n",
    "answer = qa_pipeline(question=question, context=context)\n",
    "print(f\"问题: {question}\")\n",
    "print(f\"答案: {answer['answer']}, 置信度: {answer['score']:.4f}\\n\")\n",
    "\n",
    "# 4. 命名实体识别\n",
    "print(\"=== 命名实体识别 ===\")\n",
    "ner_pipeline = pipeline(\"ner\", aggregation_strategy=\"simple\")\n",
    "text = \"Apple Inc. was founded by Steve Jobs in Cupertino, California.\"\n",
    "entities = ner_pipeline(text)\n",
    "for entity in entities:\n",
    "    print(f\"实体: {entity['word']}, 类型: {entity['entity_group']}, 分数: {entity['score']:.4f}\")\n",
    "\n",
    "# 5. 文本摘要\n",
    "print(\"\\n=== 文本摘要 ===\")\n",
    "summarizer = pipeline(\"summarization\")\n",
    "article = \"\"\"\n",
    "The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, \n",
    "and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. \n",
    "During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest \n",
    "man-made structure in the world, a title it held for 41 years until the Chrysler Building in \n",
    "New York City was finished in 1930.\n",
    "\"\"\"\n",
    "summary = summarizer(article, max_length=50, min_length=25, do_sample=False)\n",
    "print(f\"原文长度: {len(article.split())} 词\")\n",
    "print(f\"摘要: {summary[0]['summary_text']}\")\n",
    "\n",
    "# 6. 翻译\n",
    "print(\"\\n=== 翻译 ===\")\n",
    "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-zh\")\n",
    "text = \"Hello, how are you today?\"\n",
    "translation = translator(text)\n",
    "print(f\"英文: {text}\")\n",
    "print(f\"中文: {translation[0]['translation_text']}\")\n",
    "\n",
    "# 7. 零样本分类\n",
    "print(\"\\n=== 零样本分类 ===\")\n",
    "classifier = pipeline(\"zero-shot-classification\")\n",
    "text = \"This is a tutorial about using Hugging Face with Python\"\n",
    "candidate_labels = [\"education\", \"politics\", \"technology\", \"sports\"]\n",
    "result = classifier(text, candidate_labels)\n",
    "print(f\"文本: {text}\")\n",
    "print(\"分类结果:\")\n",
    "for label, score in zip(result['labels'], result['scores']):\n",
    "    print(f\"  {label}: {score:.4f}\")\n",
    "```\n",
    "\n",
    "### 3.3 指定特定模型\n",
    "\n",
    "```python\n",
    "# 使用特定模型而不是默认模型\n",
    "from transformers import pipeline\n",
    "\n",
    "# 1. 使用中文情感分析模型\n",
    "chinese_sentiment = pipeline(\n",
    "    \"sentiment-analysis\", \n",
    "    model=\"uer/roberta-base-finetuned-dianping-chinese\"\n",
    ")\n",
    "result = chinese_sentiment(\"这个产品真的很棒！\")\n",
    "print(f\"中文情感分析: {result}\")\n",
    "\n",
    "# 2. 使用特定的文本生成模型\n",
    "gpt2_medium = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"gpt2-medium\"\n",
    ")\n",
    "\n",
    "# 3. 使用本地模型\n",
    "# 首先下载模型到本地\n",
    "local_model_path = \"./my_local_model\"\n",
    "local_pipeline = pipeline(\"text-classification\", model=local_model_path)\n",
    "```\n",
    "\n",
    "## 四、深入使用：AutoModel和AutoTokenizer\n",
    "\n",
    "### 4.1 理解Tokenizer和Model\n",
    "\n",
    "当你需要更多控制时，可以直接使用AutoModel和AutoTokenizer：\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# 1. 加载tokenizer和model\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# 2. 准备输入\n",
    "text = \"I love using Hugging Face!\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "# 3. 推理\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    \n",
    "# 4. 处理输出\n",
    "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "print(f\"预测结果: {predictions}\")\n",
    "```\n",
    "\n",
    "### 4.2 批量处理\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# 批量处理多个文本\n",
    "texts = [\n",
    "    \"I love this movie!\",\n",
    "    \"This is terrible.\",\n",
    "    \"Not bad, quite enjoyable.\",\n",
    "    \"Absolutely fantastic!\"\n",
    "]\n",
    "\n",
    "# Tokenize所有文本\n",
    "inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "# 批量推理\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "\n",
    "# 解析结果\n",
    "for i, text in enumerate(texts):\n",
    "    neg_score = predictions[i][0].item()\n",
    "    pos_score = predictions[i][1].item()\n",
    "    sentiment = \"POSITIVE\" if pos_score > neg_score else \"NEGATIVE\"\n",
    "    confidence = max(pos_score, neg_score)\n",
    "    print(f\"文本: {text}\")\n",
    "    print(f\"情感: {sentiment}, 置信度: {confidence:.4f}\\n\")\n",
    "```\n",
    "\n",
    "### 4.3 使用不同类型的模型\n",
    "\n",
    "```python\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,  # 分类\n",
    "    AutoModelForTokenClassification,     # 词元分类（如NER）\n",
    "    AutoModelForQuestionAnswering,       # 问答\n",
    "    AutoModelForCausalLM,               # 文本生成（GPT类）\n",
    "    AutoModelForSeq2SeqLM,              # 序列到序列（T5类）\n",
    "    AutoModelForMaskedLM,               # 掩码语言模型（BERT类）\n",
    "    AutoTokenizer\n",
    ")\n",
    "\n",
    "# 1. 文本生成（GPT-2）\n",
    "print(\"=== 文本生成示例 ===\")\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# 设置pad_token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "input_text = \"The future of AI is\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# 生成文本\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_length=50,\n",
    "    num_return_sequences=1,\n",
    "    temperature=0.8,\n",
    "    do_sample=True,\n",
    "    top_p=0.9\n",
    ")\n",
    "\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(f\"输入: {input_text}\")\n",
    "print(f\"生成: {generated_text}\\n\")\n",
    "\n",
    "# 2. 掩码语言模型（BERT）\n",
    "print(\"=== 掩码预测示例 ===\")\n",
    "from transformers import pipeline\n",
    "\n",
    "unmasker = pipeline('fill-mask', model='bert-base-uncased')\n",
    "text = \"The capital of France is [MASK].\"\n",
    "predictions = unmasker(text)\n",
    "\n",
    "print(f\"输入: {text}\")\n",
    "print(\"预测结果:\")\n",
    "for pred in predictions[:3]:\n",
    "    print(f\"  {pred['token_str']}: {pred['score']:.4f}\")\n",
    "```\n",
    "\n",
    "## 五、处理中文模型\n",
    "\n",
    "### 5.1 中文文本分类\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# 使用中文BERT模型\n",
    "model_name = \"bert-base-chinese\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# 中文文本示例\n",
    "texts = [\n",
    "    \"这部电影真的太精彩了！\",\n",
    "    \"服务态度很差，不推荐。\",\n",
    "    \"还可以，一般般吧。\"\n",
    "]\n",
    "\n",
    "for text in texts:\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    \n",
    "    print(f\"文本: {text}\")\n",
    "    print(f\"预测分数: {predictions}\\n\")\n",
    "```\n",
    "\n",
    "### 5.2 中文问答系统\n",
    "\n",
    "```python\n",
    "from transformers import pipeline\n",
    "\n",
    "# 使用中文问答模型\n",
    "qa_pipeline = pipeline(\n",
    "    \"question-answering\",\n",
    "    model=\"uer/roberta-base-chinese-extractive-qa\"\n",
    ")\n",
    "\n",
    "context = \"\"\"\n",
    "杭州是浙江省的省会城市，位于中国东南沿海。\n",
    "杭州以其美丽的西湖风景而闻名，被誉为\"人间天堂\"。\n",
    "这座城市有着悠久的历史，可以追溯到2200多年前。\n",
    "杭州也是中国重要的经济中心，阿里巴巴总部就位于此地。\n",
    "\"\"\"\n",
    "\n",
    "questions = [\n",
    "    \"杭州是哪个省的省会？\",\n",
    "    \"杭州有什么著名景点？\",\n",
    "    \"哪家知名公司的总部在杭州？\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    answer = qa_pipeline(question=question, context=context)\n",
    "    print(f\"问题: {question}\")\n",
    "    print(f\"答案: {answer['answer']}, 置信度: {answer['score']:.4f}\\n\")\n",
    "```\n",
    "\n",
    "### 5.3 中文文本生成\n",
    "\n",
    "```python\n",
    "from transformers import pipeline\n",
    "\n",
    "# 使用中文GPT模型\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"uer/gpt2-chinese-cluecorpussmall\",\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "prompts = [\n",
    "    \"今天天气\",\n",
    "    \"人工智能的发展\",\n",
    "    \"我最喜欢的\"\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    result = generator(\n",
    "        prompt,\n",
    "        max_length=50,\n",
    "        num_return_sequences=1,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    print(f\"提示: {prompt}\")\n",
    "    print(f\"生成: {result[0]['generated_text']}\\n\")\n",
    "```\n",
    "\n",
    "## 六、图像处理模型\n",
    "\n",
    "### 6.1 图像分类\n",
    "\n",
    "```python\n",
    "from transformers import pipeline\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "# 创建图像分类pipeline\n",
    "classifier = pipeline(\"image-classification\", model=\"google/vit-base-patch16-224\")\n",
    "\n",
    "# 方法1：从URL加载图像\n",
    "url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "# 分类\n",
    "results = classifier(image)\n",
    "print(\"图像分类结果:\")\n",
    "for result in results[:3]:\n",
    "    print(f\"  {result['label']}: {result['score']:.4f}\")\n",
    "\n",
    "# 方法2：从本地文件加载\n",
    "# image = Image.open(\"path/to/your/image.jpg\")\n",
    "# results = classifier(image)\n",
    "```\n",
    "\n",
    "### 6.2 目标检测\n",
    "\n",
    "```python\n",
    "from transformers import pipeline\n",
    "from PIL import Image, ImageDraw\n",
    "import requests\n",
    "\n",
    "# 创建目标检测pipeline\n",
    "detector = pipeline(\"object-detection\", model=\"facebook/detr-resnet-50\")\n",
    "\n",
    "# 加载图像\n",
    "url = \"https://images.unsplash.com/photo-1518991669955-9c7e78ec80ca\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "# 检测对象\n",
    "results = detector(image)\n",
    "\n",
    "# 可视化结果\n",
    "draw = ImageDraw.Draw(image)\n",
    "for result in results:\n",
    "    box = result['box']\n",
    "    label = result['label']\n",
    "    score = result['score']\n",
    "    \n",
    "    if score > 0.9:  # 只显示高置信度的检测\n",
    "        # 画边界框\n",
    "        draw.rectangle(\n",
    "            [box['xmin'], box['ymin'], box['xmax'], box['ymax']],\n",
    "            outline=\"red\",\n",
    "            width=3\n",
    "        )\n",
    "        # 添加标签\n",
    "        draw.text(\n",
    "            (box['xmin'], box['ymin']),\n",
    "            f\"{label}: {score:.2f}\",\n",
    "            fill=\"red\"\n",
    "        )\n",
    "\n",
    "# 保存结果\n",
    "image.save(\"detection_result.jpg\")\n",
    "print(\"检测结果已保存到 detection_result.jpg\")\n",
    "```\n",
    "\n",
    "### 6.3 图像描述生成\n",
    "\n",
    "```python\n",
    "from transformers import pipeline\n",
    "from PIL import Image\n",
    "\n",
    "# 创建图像到文本pipeline\n",
    "image_to_text = pipeline(\n",
    "    \"image-to-text\",\n",
    "    model=\"Salesforce/blip-image-captioning-base\"\n",
    ")\n",
    "\n",
    "# 加载图像\n",
    "image = Image.open(\"your_image.jpg\")\n",
    "\n",
    "# 生成描述\n",
    "result = image_to_text(image)\n",
    "print(f\"图像描述: {result[0]['generated_text']}\")\n",
    "```\n",
    "\n",
    "## 七、音频处理模型\n",
    "\n",
    "### 7.1 语音识别\n",
    "\n",
    "```python\n",
    "from transformers import pipeline\n",
    "import librosa\n",
    "\n",
    "# 创建语音识别pipeline\n",
    "transcriber = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=\"openai/whisper-small\"\n",
    ")\n",
    "\n",
    "# 加载音频文件\n",
    "# 注意：需要安装 librosa: pip install librosa\n",
    "audio_file = \"path/to/audio.wav\"\n",
    "\n",
    "# 转录\n",
    "result = transcriber(audio_file)\n",
    "print(f\"转录结果: {result['text']}\")\n",
    "\n",
    "# 处理长音频\n",
    "result = transcriber(\n",
    "    audio_file,\n",
    "    chunk_length_s=30,  # 每30秒一个块\n",
    "    stride_length_s=5   # 5秒重叠\n",
    ")\n",
    "print(f\"长音频转录: {result['text']}\")\n",
    "```\n",
    "\n",
    "### 7.2 音频分类\n",
    "\n",
    "```python\n",
    "from transformers import pipeline\n",
    "\n",
    "# 创建音频分类pipeline\n",
    "audio_classifier = pipeline(\n",
    "    \"audio-classification\",\n",
    "    model=\"superb/hubert-base-superb-er\"\n",
    ")\n",
    "\n",
    "# 分类音频\n",
    "audio_file = \"path/to/audio.wav\"\n",
    "results = audio_classifier(audio_file)\n",
    "\n",
    "print(\"音频分类结果:\")\n",
    "for result in results[:3]:\n",
    "    print(f\"  {result['label']}: {result['score']:.4f}\")\n",
    "```\n",
    "\n",
    "## 八、多模态模型\n",
    "\n",
    "### 8.1 视觉问答（VQA）\n",
    "\n",
    "```python\n",
    "from transformers import pipeline\n",
    "from PIL import Image\n",
    "\n",
    "# 创建VQA pipeline\n",
    "vqa = pipeline(\"visual-question-answering\")\n",
    "\n",
    "# 加载图像\n",
    "image = Image.open(\"path/to/image.jpg\")\n",
    "\n",
    "# 提问\n",
    "questions = [\n",
    "    \"What color is the sky?\",\n",
    "    \"How many people are in the image?\",\n",
    "    \"What is the main object in the picture?\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    result = vqa(image=image, question=question)\n",
    "    print(f\"问题: {question}\")\n",
    "    print(f\"答案: {result[0]['answer']}, 置信度: {result[0]['score']:.4f}\\n\")\n",
    "```\n",
    "\n",
    "### 8.2 CLIP模型（图文匹配）\n",
    "\n",
    "```python\n",
    "from transformers import pipeline\n",
    "\n",
    "# 创建零样本图像分类pipeline（使用CLIP）\n",
    "classifier = pipeline(\n",
    "    \"zero-shot-image-classification\",\n",
    "    model=\"openai/clip-vit-base-patch32\"\n",
    ")\n",
    "\n",
    "# 加载图像\n",
    "from PIL import Image\n",
    "image = Image.open(\"path/to/image.jpg\")\n",
    "\n",
    "# 定义候选标签\n",
    "candidate_labels = [\"cat\", \"dog\", \"bird\", \"car\", \"tree\", \"building\"]\n",
    "\n",
    "# 分类\n",
    "results = classifier(\n",
    "    images=image,\n",
    "    candidate_labels=candidate_labels\n",
    ")\n",
    "\n",
    "print(\"图像分类结果:\")\n",
    "for result in results:\n",
    "    print(f\"  {result['label']}: {result['score']:.4f}\")\n",
    "```\n",
    "\n",
    "## 九、模型微调基础\n",
    "\n",
    "### 9.1 准备数据集\n",
    "\n",
    "```python\n",
    "from datasets import Dataset, DatasetDict\n",
    "import pandas as pd\n",
    "\n",
    "# 创建示例数据\n",
    "data = {\n",
    "    'text': [\n",
    "        \"I love this product!\",\n",
    "        \"This is terrible.\",\n",
    "        \"Great quality, highly recommend.\",\n",
    "        \"Waste of money.\",\n",
    "        \"Not bad, decent value.\"\n",
    "    ],\n",
    "    'label': [1, 0, 1, 0, 1]  # 1: positive, 0: negative\n",
    "}\n",
    "\n",
    "# 创建Dataset\n",
    "df = pd.DataFrame(data)\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# 分割训练集和测试集\n",
    "train_test = dataset.train_test_split(test_size=0.2)\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': train_test['train'],\n",
    "    'test': train_test['test']\n",
    "})\n",
    "\n",
    "print(f\"训练集大小: {len(dataset_dict['train'])}\")\n",
    "print(f\"测试集大小: {len(dataset_dict['test'])}\")\n",
    "```\n",
    "\n",
    "### 9.2 简单的模型微调\n",
    "\n",
    "```python\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "\n",
    "# 加载模型和tokenizer\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# 数据预处理函数\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, padding=True)\n",
    "\n",
    "# 应用预处理\n",
    "tokenized_datasets = dataset_dict.map(preprocess_function, batched=True)\n",
    "\n",
    "# 设置训练参数\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "# 定义评估函数\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    accuracy = (predictions == labels).mean()\n",
    "    return {\"accuracy\": accuracy}\n",
    "\n",
    "# 创建Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# 训练模型\n",
    "# trainer.train()\n",
    "\n",
    "# 保存模型\n",
    "# trainer.save_model(\"./my-finetuned-model\")\n",
    "```\n",
    "\n",
    "## 十、性能优化\n",
    "\n",
    "### 10.1 使用GPU加速\n",
    "\n",
    "```python\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "# 检查GPU是否可用\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "print(f\"使用设备: {'GPU' if device == 0 else 'CPU'}\")\n",
    "\n",
    "# 在GPU上运行pipeline\n",
    "classifier = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# 批量处理以提高效率\n",
    "texts = [\"Text 1\", \"Text 2\", \"Text 3\"] * 100\n",
    "results = classifier(texts, batch_size=32)\n",
    "```\n",
    "\n",
    "### 10.2 模型量化\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# 加载模型\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# 动态量化\n",
    "quantized_model = torch.quantization.quantize_dynamic(\n",
    "    model,\n",
    "    {torch.nn.Linear},\n",
    "    dtype=torch.qint8\n",
    ")\n",
    "\n",
    "# 比较模型大小\n",
    "import os\n",
    "\n",
    "def get_model_size(model):\n",
    "    torch.save(model.state_dict(), \"temp.p\")\n",
    "    size = os.path.getsize(\"temp.p\") / 1e6\n",
    "    os.remove(\"temp.p\")\n",
    "    return size\n",
    "\n",
    "print(f\"原始模型大小: {get_model_size(model):.2f} MB\")\n",
    "print(f\"量化模型大小: {get_model_size(quantized_model):.2f} MB\")\n",
    "```\n",
    "\n",
    "### 10.3 使用ONNX加速\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "# 导出为ONNX格式\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# 准备虚拟输入\n",
    "dummy_input = tokenizer(\n",
    "    \"Hello, world!\",\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "# 导出ONNX\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    tuple(dummy_input.values()),\n",
    "    \"model.onnx\",\n",
    "    input_names=['input_ids', 'attention_mask'],\n",
    "    output_names=['output'],\n",
    "    dynamic_axes={\n",
    "        'input_ids': {0: 'batch_size', 1: 'sequence'},\n",
    "        'attention_mask': {0: 'batch_size', 1: 'sequence'},\n",
    "        'output': {0: 'batch_size'}\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"模型已导出为ONNX格式\")\n",
    "```\n",
    "\n",
    "## 十一、实战项目：构建智能问答系统\n",
    "\n",
    "### 11.1 项目结构\n",
    "\n",
    "```python\n",
    "# qa_system.py\n",
    "from transformers import pipeline\n",
    "from typing import List, Dict\n",
    "import json\n",
    "\n",
    "class QASystem:\n",
    "    def __init__(self, model_name=\"deepset/roberta-base-squad2\"):\n",
    "        \"\"\"初始化问答系统\"\"\"\n",
    "        self.qa_pipeline = pipeline(\"question-answering\", model=model_name)\n",
    "        self.knowledge_base = {}\n",
    "        \n",
    "    def add_document(self, doc_id: str, title: str, content: str):\n",
    "        \"\"\"添加文档到知识库\"\"\"\n",
    "        self.knowledge_base[doc_id] = {\n",
    "            \"title\": title,\n",
    "            \"content\": content\n",
    "        }\n",
    "        \n",
    "    def load_knowledge_base(self, file_path: str):\n",
    "        \"\"\"从文件加载知识库\"\"\"\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            self.knowledge_base = json.load(f)\n",
    "            \n",
    "    def search_documents(self, query: str, top_k: int = 3) -> List[str]:\n",
    "        \"\"\"简单的文档搜索（基于关键词）\"\"\"\n",
    "        query_words = set(query.lower().split())\n",
    "        scores = {}\n",
    "        \n",
    "        for doc_id, doc in self.knowledge_base.items():\n",
    "            content_words = set(doc['content'].lower().split())\n",
    "            score = len(query_words.intersection(content_words))\n",
    "            scores[doc_id] = score\n",
    "            \n",
    "        # 返回得分最高的文档\n",
    "        sorted_docs = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        return [doc_id for doc_id, _ in sorted_docs[:top_k]]\n",
    "    \n",
    "    def answer_question(self, question: str, context: str = None) -> Dict:\n",
    "        \"\"\"回答问题\"\"\"\n",
    "        if context is None:\n",
    "            # 从知识库中搜索相关文档\n",
    "            relevant_docs = self.search_documents(question)\n",
    "            if not relevant_docs:\n",
    "                return {\n",
    "                    \"answer\": \"抱歉，我在知识库中找不到相关信息。\",\n",
    "                    \"confidence\": 0.0,\n",
    "                    \"source\": None\n",
    "                }\n",
    "            \n",
    "            # 合并相关文档作为上下文\n",
    "            contexts = []\n",
    "            for doc_id in relevant_docs:\n",
    "                doc = self.knowledge_base[doc_id]\n",
    "                contexts.append(f\"{doc['title']}: {doc['content']}\")\n",
    "            context = \" \".join(contexts)\n",
    "            source = relevant_docs[0]\n",
    "        else:\n",
    "            source = \"provided_context\"\n",
    "        \n",
    "        # 使用问答模型\n",
    "        try:\n",
    "            result = self.qa_pipeline(\n",
    "                question=question,\n",
    "                context=context,\n",
    "                max_answer_len=100\n",
    "            )\n",
    "            \n",
    "            return {\n",
    "                \"answer\": result['answer'],\n",
    "                \"confidence\": result['score'],\n",
    "                \"source\": source,\n",
    "                \"context_used\": context[:200] + \"...\" if len(context) > 200 else context\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"answer\": f\"处理问题时出错: {str(e)}\",\n",
    "                \"confidence\": 0.0,\n",
    "                \"source\": None\n",
    "            }\n",
    "    \n",
    "    def interactive_qa(self):\n",
    "        \"\"\"交互式问答\"\"\"\n",
    "        print(\"智能问答系统已启动！输入 'quit' 退出。\")\n",
    "        print(\"输入 'add' 添加新文档到知识库。\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        while True:\n",
    "            user_input = input(\"\\n请输入您的问题: \").strip()\n",
    "            \n",
    "            if user_input.lower() == 'quit':\n",
    "                print(\"感谢使用，再见！\")\n",
    "                break\n",
    "                \n",
    "            elif user_input.lower() == 'add':\n",
    "                title = input(\"文档标题: \")\n",
    "                content = input(\"文档内容: \")\n",
    "                doc_id = f\"doc_{len(self.knowledge_base) + 1}\"\n",
    "                self.add_document(doc_id, title, content)\n",
    "                print(f\"文档已添加，ID: {doc_id}\")\n",
    "                \n",
    "            else:\n",
    "                result = self.answer_question(user_input)\n",
    "                print(f\"\\n答案: {result['answer']}\")\n",
    "                print(f\"置信度: {result['confidence']:.2%}\")\n",
    "                if result['source']:\n",
    "                    print(f\"来源: {result['source']}\")\n",
    "\n",
    "# 使用示例\n",
    "if __name__ == \"__main__\":\n",
    "    # 创建问答系统\n",
    "    qa = QASystem()\n",
    "    \n",
    "    # 添加一些示例文档\n",
    "    qa.add_document(\n",
    "        \"doc_1\",\n",
    "        \"Python简介\",\n",
    "        \"Python是一种高级编程语言，由Guido van Rossum在1991年创建。它以简洁易读的语法著称，广泛应用于数据科学、人工智能、Web开发等领域。\"\n",
    "    )\n",
    "    \n",
    "    qa.add_document(\n",
    "        \"doc_2\",\n",
    "        \"Hugging Face介绍\",\n",
    "        \"Hugging Face是一家专注于自然语言处理的公司，提供了Transformers库，使得使用预训练模型变得非常简单。它的模型中心托管了数万个模型。\"\n",
    "    )\n",
    "    \n",
    "    qa.add_document(\n",
    "        \"doc_3\",\n",
    "        \"机器学习基础\",\n",
    "        \"机器学习是人工智能的一个分支，让计算机能够从数据中学习。主要分为监督学习、无监督学习和强化学习三大类。\"\n",
    "    )\n",
    "    \n",
    "    # 测试问答\n",
    "    questions = [\n",
    "        \"Python是什么时候创建的？\",\n",
    "        \"Hugging Face提供了什么库？\",\n",
    "        \"机器学习有哪些类型？\",\n",
    "        \"谁创建了Python？\"\n",
    "    ]\n",
    "    \n",
    "    print(\"=== 测试问答系统 ===\\n\")\n",
    "    for q in questions:\n",
    "        print(f\"问题: {q}\")\n",
    "        result = qa.answer_question(q)\n",
    "        print(f\"答案: {result['answer']}\")\n",
    "        print(f\"置信度: {result['confidence']:.2%}\")\n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    # 启动交互模式\n",
    "    # qa.interactive_qa()\n",
    "```\n",
    "\n",
    "### 11.2 增强版：结合向量数据库\n",
    "\n",
    "```python\n",
    "# enhanced_qa_system.py\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "import faiss\n",
    "\n",
    "class EnhancedQASystem:\n",
    "    def __init__(self):\n",
    "        # 初始化句子编码器\n",
    "        self.encoder = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "        \n",
    "        # 初始化问答模型\n",
    "        from transformers import pipeline\n",
    "        self.qa_pipeline = pipeline(\"question-answering\")\n",
    "        \n",
    "        # 初始化向量索引\n",
    "        self.dimension = 384  # MiniLM输出维度\n",
    "        self.index = faiss.IndexFlatL2(self.dimension)\n",
    "        \n",
    "        # 文档存储\n",
    "        self.documents = []\n",
    "        \n",
    "    def add_document(self, text: str, metadata: dict = None):\n",
    "        \"\"\"添加文档并建立向量索引\"\"\"\n",
    "        # 编码文档\n",
    "        embedding = self.encoder.encode([text])\n",
    "        \n",
    "        # 添加到索引\n",
    "        self.index.add(embedding)\n",
    "        \n",
    "        # 存储文档\n",
    "        self.documents.append({\n",
    "            'text': text,\n",
    "            'metadata': metadata or {},\n",
    "            'embedding': embedding[0]\n",
    "        })\n",
    "        \n",
    "    def search_similar(self, query: str, k: int = 3) -> List[Tuple[int, float]]:\n",
    "        \"\"\"搜索相似文档\"\"\"\n",
    "        # 编码查询\n",
    "        query_embedding = self.encoder.encode([query])\n",
    "        \n",
    "        # 搜索\n",
    "        distances, indices = self.index.search(query_embedding, k)\n",
    "        \n",
    "        # 返回结果\n",
    "        results = []\n",
    "        for i, (idx, dist) in enumerate(zip(indices[0], distances[0])):\n",
    "            if idx < len(self.documents):\n",
    "                results.append((idx, float(dist)))\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def answer_question_with_context(self, question: str) -> dict:\n",
    "        \"\"\"基于相似文档回答问题\"\"\"\n",
    "        # 搜索相关文档\n",
    "        similar_docs = self.search_similar(question, k=3)\n",
    "        \n",
    "        if not similar_docs:\n",
    "            return {\n",
    "                'answer': '找不到相关信息',\n",
    "                'confidence': 0.0,\n",
    "                'sources': []\n",
    "            }\n",
    "        \n",
    "        # 构建上下文\n",
    "        contexts = []\n",
    "        sources = []\n",
    "        for idx, distance in similar_docs:\n",
    "            doc = self.documents[idx]\n",
    "            contexts.append(doc['text'])\n",
    "            sources.append({\n",
    "                'text': doc['text'][:100] + '...',\n",
    "                'similarity': 1 / (1 + distance)  # 转换为相似度分数\n",
    "            })\n",
    "        \n",
    "        combined_context = ' '.join(contexts)\n",
    "        \n",
    "        # 问答\n",
    "        result = self.qa_pipeline(\n",
    "            question=question,\n",
    "            context=combined_context\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'answer': result['answer'],\n",
    "            'confidence': result['score'],\n",
    "            'sources': sources\n",
    "        }\n",
    "\n",
    "# 使用示例\n",
    "if __name__ == \"__main__\":\n",
    "    qa = EnhancedQASystem()\n",
    "    \n",
    "    # 添加文档\n",
    "    documents = [\n",
    "        \"深度学习是机器学习的一个子集，使用多层神经网络来学习数据的表示。\",\n",
    "        \"卷积神经网络（CNN）特别适合处理图像数据，在计算机视觉任务中表现出色。\",\n",
    "        \"循环神经网络（RNN）擅长处理序列数据，如文本和时间序列。\",\n",
    "        \"Transformer架构革命性地改变了NLP领域，BERT和GPT都基于这种架构。\"\n",
    "    ]\n",
    "    \n",
    "    for doc in documents:\n",
    "        qa.add_document(doc)\n",
    "    \n",
    "    # 测试问答\n",
    "    question = \"什么神经网络适合处理图像？\"\n",
    "    result = qa.answer_question_with_context(question)\n",
    "    \n",
    "    print(f\"问题: {question}\")\n",
    "    print(f\"答案: {result['answer']}\")\n",
    "    print(f\"置信度: {result['confidence']:.2%}\")\n",
    "    print(\"\\n相关文档:\")\n",
    "    for i, source in enumerate(result['sources']):\n",
    "        print(f\"{i+1}. {source['text']} (相似度: {source['similarity']:.2%})\")\n",
    "```\n",
    "\n",
    "## 十二、最佳实践和注意事项\n",
    "\n",
    "### 12.1 模型选择指南\n",
    "\n",
    "```python\n",
    "# model_selection_guide.py\n",
    "\n",
    "def recommend_model(task: str, language: str = \"en\", size_constraint: str = \"medium\"):\n",
    "    \"\"\"根据任务推荐合适的模型\"\"\"\n",
    "    \n",
    "    recommendations = {\n",
    "        \"sentiment-analysis\": {\n",
    "            \"en\": {\n",
    "                \"small\": \"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "                \"medium\": \"roberta-base-sentiment\",\n",
    "                \"large\": \"roberta-large-mnli\"\n",
    "            },\n",
    "            \"zh\": {\n",
    "                \"small\": \"uer/roberta-base-finetuned-dianping-chinese\",\n",
    "                \"medium\": \"bert-base-chinese\",\n",
    "                \"large\": \"hfl/chinese-roberta-wwm-ext-large\"\n",
    "            }\n",
    "        },\n",
    "        \"text-generation\": {\n",
    "            \"en\": {\n",
    "                \"small\": \"gpt2\",\n",
    "                \"medium\": \"gpt2-medium\",\n",
    "                \"large\": \"gpt2-large\"\n",
    "            },\n",
    "            \"zh\": {\n",
    "                \"small\": \"uer/gpt2-chinese-cluecorpussmall\",\n",
    "                \"medium\": \"IDEA-CCNL/Wenzhong-GPT2-110M\",\n",
    "                \"large\": \"IDEA-CCNL/Wenzhong2.0-GPT2-3.5B-chinese\"\n",
    "            }\n",
    "        },\n",
    "        \"question-answering\": {\n",
    "            \"en\": {\n",
    "                \"small\": \"distilbert-base-cased-distilled-squad\",\n",
    "                \"medium\": \"deepset/roberta-base-squad2\",\n",
    "                \"large\": \"deepset/roberta-large-squad2\"\n",
    "            },\n",
    "            \"zh\": {\n",
    "                \"small\": \"uer/roberta-base-chinese-extractive-qa\",\n",
    "                \"medium\": \"luhua/chinese_pretrain_mrc_roberta_wwm_ext_base\",\n",
    "                \"large\": \"luhua/chinese_pretrain_mrc_macbert_large\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    if task in recommendations and language in recommendations[task]:\n",
    "        return recommendations[task][language].get(size_constraint, \"未找到合适的模型\")\n",
    "    else:\n",
    "        return \"不支持的任务或语言\"\n",
    "\n",
    "# 使用示例\n",
    "print(\"情感分析（中文，小模型）:\", recommend_model(\"sentiment-analysis\", \"zh\", \"small\"))\n",
    "print(\"文本生成（英文，中等）:\", recommend_model(\"text-generation\", \"en\", \"medium\"))\n",
    "print(\"问答（中文，大模型）:\", recommend_model(\"question-answering\", \"zh\", \"large\"))\n",
    "```\n",
    "\n",
    "### 12.2 错误处理和日志\n",
    "\n",
    "```python\n",
    "import logging\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "# 配置日志\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class SafeModelWrapper:\n",
    "    def __init__(self, task: str, model: str = None):\n",
    "        self.task = task\n",
    "        self.model_name = model\n",
    "        self.pipeline = None\n",
    "        self._initialize()\n",
    "    \n",
    "    def _initialize(self):\n",
    "        \"\"\"安全初始化模型\"\"\"\n",
    "        try:\n",
    "            logger.info(f\"正在加载模型: {self.model_name or 'default'} for {self.task}\")\n",
    "            self.pipeline = pipeline(self.task, model=self.model_name)\n",
    "            logger.info(\"模型加载成功\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"模型加载失败: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def predict(self, *args, **kwargs):\n",
    "        \"\"\"安全预测\"\"\"\n",
    "        if self.pipeline is None:\n",
    "            logger.error(\"模型未初始化\")\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            result = self.pipeline(*args, **kwargs)\n",
    "            logger.info(\"预测成功\")\n",
    "            return result\n",
    "        except torch.cuda.OutOfMemoryError:\n",
    "            logger.error(\"GPU内存不足\")\n",
    "            # 尝试清理内存并使用CPU\n",
    "            torch.cuda.empty_cache()\n",
    "            self.pipeline.device = -1\n",
    "            logger.info(\"切换到CPU模式重试\")\n",
    "            return self.predict(*args, **kwargs)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"预测失败: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "# 使用示例\n",
    "safe_classifier = SafeModelWrapper(\"sentiment-analysis\")\n",
    "result = safe_classifier.predict(\"This is a great product!\")\n",
    "if result:\n",
    "    print(f\"预测结果: {result}\")\n",
    "```\n",
    "\n",
    "### 12.3 资源管理\n",
    "\n",
    "```python\n",
    "import gc\n",
    "import torch\n",
    "from contextlib import contextmanager\n",
    "\n",
    "@contextmanager\n",
    "def model_context(pipeline_func, *args, **kwargs):\n",
    "    \"\"\"上下文管理器，自动清理模型资源\"\"\"\n",
    "    model = None\n",
    "    try:\n",
    "        model = pipeline_func(*args, **kwargs)\n",
    "        yield model\n",
    "    finally:\n",
    "        # 清理资源\n",
    "        if model is not None:\n",
    "            del model\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "# 使用示例\n",
    "with model_context(pipeline, \"sentiment-analysis\") as classifier:\n",
    "    result = classifier(\"I love this!\")\n",
    "    print(result)\n",
    "# 模型会在with块结束后自动清理\n",
    "```\n",
    "\n",
    "## 十三、总结\n",
    "\n",
    "### 13.1 我们学到了什么\n",
    "\n",
    "1. **Hugging Face基础**\n",
    "   - Model Hub的使用\n",
    "   - Transformers库的核心概念\n",
    "   - Pipeline API的便捷性\n",
    "\n",
    "2. **各种任务的实现**\n",
    "   - 文本分类、生成、问答\n",
    "   - 图像分类、目标检测\n",
    "   - 语音识别、音频分类\n",
    "   - 多模态任务\n",
    "\n",
    "3. **进阶技巧**\n",
    "   - 模型微调\n",
    "   - 性能优化\n",
    "   - 错误处理\n",
    "   - 资源管理\n",
    "\n",
    "4. **实战应用**\n",
    "   - 构建问答系统\n",
    "   - 整合向量数据库\n",
    "   - 生产环境考虑\n",
    "\n",
    "### 13.2 下一步学习建议\n",
    "\n",
    "1. **深入特定领域**\n",
    "   - 选择一个感兴趣的方向深入研究\n",
    "   - 尝试更多专业模型\n",
    "   - 参与社区讨论\n",
    "\n",
    "2. **模型训练和微调**\n",
    "   - 学习如何训练自己的模型\n",
    "   - 掌握数据集准备技巧\n",
    "   - 了解分布式训练\n",
    "\n",
    "3. **部署和优化**\n",
    "   - 学习模型部署技术\n",
    "   - 掌握模型压缩和加速\n",
    "   - 了解边缘计算部署\n",
    "\n",
    "4. **参与开源**\n",
    "   - 贡献代码到Transformers库\n",
    "   - 分享自己训练的模型\n",
    "   - 编写教程帮助他人\n",
    "\n",
    "### 13.3 有用的资源\n",
    "\n",
    "1. **官方资源**\n",
    "   - Hugging Face官网: https://huggingface.co/\n",
    "   - Transformers文档: https://huggingface.co/docs/transformers/\n",
    "   - 模型中心: https://huggingface.co/models\n",
    "\n",
    "2. **学习资源**\n",
    "   - Hugging Face课程: https://huggingface.co/course/\n",
    "   - 论文阅读: https://papers.huggingface.co/\n",
    "   - 社区论坛: https://discuss.huggingface.co/\n",
    "\n",
    "3. **相关工具**\n",
    "   - Gradio: 快速创建模型Demo\n",
    "   - Streamlit: 构建数据应用\n",
    "   - FastAPI: 构建API服务\n",
    "\n",
    "### 13.4 结语\n",
    "\n",
    "Hugging Face让AI模型的使用变得前所未有的简单。通过本教程，你已经掌握了使用各种AI模型的基本技能。记住：\n",
    "\n",
    "- **从简单开始**：先用Pipeline API快速实现功能\n",
    "- **逐步深入**：需要更多控制时再使用底层API\n",
    "- **注重实践**：多做项目，在实践中学习\n",
    "- **保持学习**：AI领域发展迅速，持续关注新技术\n",
    "\n",
    "现在，你已经拥有了强大的AI工具箱，去创造令人惊叹的应用吧！\n",
    "\n",
    "**Happy Modeling! 🤗**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65dd1d04-0cd5-48b0-a441-45d67dc3a53a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
