{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "720dfcfb-a5ed-4f30-bc8d-e7761b11f670",
   "metadata": {},
   "source": [
    "# Pythonä¸Hugging Faceæ¨¡å‹äº’åŠ¨æ•™ç¨‹ â€”â€” ç©è½¬AIæ¨¡å‹\n",
    "\n",
    "## å¼•è¨€ï¼šä¸ºä»€ä¹ˆè¦å­¦ä¹ Hugging Faceï¼Ÿ\n",
    "\n",
    "åœ¨AIæ—¶ä»£ï¼ŒHugging Faceå·²ç»æˆä¸ºäº†AIæ¨¡å‹çš„\"GitHub\"ã€‚å®ƒæä¾›äº†ï¼š\n",
    "- æ•°ä¸‡ä¸ªé¢„è®­ç»ƒæ¨¡å‹\n",
    "- ç®€å•æ˜“ç”¨çš„API\n",
    "- å¼ºå¤§çš„æ¨¡å‹åº“ï¼ˆTransformersï¼‰\n",
    "- æ´»è·ƒçš„ç¤¾åŒº\n",
    "\n",
    "ä»Šå¤©ï¼Œæˆ‘ä»¬å°†å­¦ä¹ å¦‚ä½•ä½¿ç”¨Pythonä¸Hugging Faceä¸Šçš„æ¨¡å‹è¿›è¡Œäº’åŠ¨ï¼Œè®©ä½ èƒ½å¤Ÿå¿«é€Ÿä½¿ç”¨å„ç§AIæ¨¡å‹ã€‚\n",
    "\n",
    "## ä¸€ã€Hugging Faceæ˜¯ä»€ä¹ˆï¼Ÿ\n",
    "\n",
    "### 1.1 Hugging Faceç®€ä»‹\n",
    "\n",
    "Hugging Faceæ˜¯ä¸€ä¸ªAIç¤¾åŒºå’Œå¹³å°ï¼Œå®ƒæä¾›ï¼š\n",
    "\n",
    "1. **æ¨¡å‹ä»“åº“ï¼ˆModel Hubï¼‰**\n",
    "   - è¶…è¿‡10ä¸‡ä¸ªé¢„è®­ç»ƒæ¨¡å‹\n",
    "   - æ¶µç›–NLPã€è®¡ç®—æœºè§†è§‰ã€éŸ³é¢‘ç­‰é¢†åŸŸ\n",
    "   - æ”¯æŒå¤šç§æ¡†æ¶ï¼ˆPyTorchã€TensorFlowç­‰ï¼‰\n",
    "\n",
    "2. **Transformersåº“**\n",
    "   - ç»Ÿä¸€çš„APIæ¥å£\n",
    "   - æ”¯æŒå„ç§ä»»åŠ¡ï¼ˆæ–‡æœ¬ç”Ÿæˆã€åˆ†ç±»ã€ç¿»è¯‘ç­‰ï¼‰\n",
    "   - ç®€åŒ–æ¨¡å‹ä½¿ç”¨æµç¨‹\n",
    "\n",
    "3. **æ•°æ®é›†ä»“åº“ï¼ˆDataset Hubï¼‰**\n",
    "   - æ•°åƒä¸ªæ•°æ®é›†\n",
    "   - ç»Ÿä¸€çš„æ•°æ®åŠ è½½æ¥å£\n",
    "\n",
    "4. **Spaces**\n",
    "   - æ‰˜ç®¡AIåº”ç”¨\n",
    "   - å±•ç¤ºæ¨¡å‹Demo\n",
    "\n",
    "### 1.2 ä¸ºä»€ä¹ˆé€‰æ‹©Hugging Faceï¼Ÿ\n",
    "\n",
    "```python\n",
    "# ä¼ ç»Ÿæ–¹å¼ï¼šå¤æ‚çš„æ¨¡å‹åŠ è½½å’Œä½¿ç”¨\n",
    "import torch\n",
    "model = torch.load('model.pt')\n",
    "# éœ€è¦äº†è§£æ¨¡å‹ç»“æ„ã€é¢„å¤„ç†æ–¹å¼ç­‰...\n",
    "\n",
    "# Hugging Faceæ–¹å¼ï¼šç®€å•ç›´æ¥\n",
    "from transformers import pipeline\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "result = classifier(\"I love this movie!\")\n",
    "# å°±è¿™ä¹ˆç®€å•ï¼\n",
    "```\n",
    "\n",
    "## äºŒã€ç¯å¢ƒå‡†å¤‡\n",
    "\n",
    "### 2.1 å®‰è£…å¿…è¦çš„åº“\n",
    "\n",
    "```bash\n",
    "# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ\n",
    "python -m venv hf_env\n",
    "source hf_env/bin/activate  # Windows: hf_env\\Scripts\\activate\n",
    "\n",
    "# å®‰è£…åŸºç¡€åº“\n",
    "pip install transformers\n",
    "pip install torch  # æˆ– tensorflow\n",
    "pip install datasets\n",
    "pip install huggingface-hub\n",
    "\n",
    "# å®‰è£…é¢å¤–åŠŸèƒ½ï¼ˆå¯é€‰ï¼‰\n",
    "pip install accelerate  # åŠ é€Ÿè®­ç»ƒ\n",
    "pip install sentencepiece  # æŸäº›æ¨¡å‹éœ€è¦\n",
    "pip install protobuf  # æŸäº›æ¨¡å‹éœ€è¦\n",
    "```\n",
    "\n",
    "### 2.2 é…ç½®Hugging Faceè´¦æˆ·ï¼ˆå¯é€‰ï¼‰\n",
    "\n",
    "å¦‚æœä½ æƒ³ä½¿ç”¨ç§æœ‰æ¨¡å‹æˆ–ä¸Šä¼ æ¨¡å‹ï¼Œéœ€è¦é…ç½®tokenï¼š\n",
    "\n",
    "```python\n",
    "# æ–¹æ³•1ï¼šä½¿ç”¨å‘½ä»¤è¡Œ\n",
    "# huggingface-cli login\n",
    "\n",
    "# æ–¹æ³•2ï¼šåœ¨ä»£ç ä¸­è®¾ç½®\n",
    "from huggingface_hub import login\n",
    "login(token=\"your_token_here\")\n",
    "\n",
    "# æ–¹æ³•3ï¼šä½¿ç”¨ç¯å¢ƒå˜é‡\n",
    "# export HUGGING_FACE_HUB_TOKEN=\"your_token_here\"\n",
    "```\n",
    "\n",
    "## ä¸‰ã€å¿«é€Ÿå¼€å§‹ï¼šPipeline API\n",
    "\n",
    "### 3.1 ä»€ä¹ˆæ˜¯Pipelineï¼Ÿ\n",
    "\n",
    "Pipelineæ˜¯Hugging Faceæä¾›çš„é«˜çº§APIï¼Œè®©ä½ èƒ½å¤Ÿå¿«é€Ÿä½¿ç”¨æ¨¡å‹ï¼Œæ— éœ€äº†è§£å¤æ‚çš„ç»†èŠ‚ã€‚\n",
    "\n",
    "```python\n",
    "from transformers import pipeline\n",
    "\n",
    "# åˆ›å»ºä¸€ä¸ªpipelineå°±åƒåˆ›å»ºä¸€ä¸ªå‡½æ•°\n",
    "# è¿™ä¸ªå‡½æ•°å¯ä»¥å¤„ç†ç‰¹å®šçš„ä»»åŠ¡\n",
    "sentiment_analyzer = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "# ä½¿ç”¨å°±åƒè°ƒç”¨å‡½æ•°ä¸€æ ·ç®€å•\n",
    "result = sentiment_analyzer(\"I am very happy today!\")\n",
    "print(result)\n",
    "# è¾“å‡º: [{'label': 'POSITIVE', 'score': 0.9998}]\n",
    "```\n",
    "\n",
    "### 3.2 å¸¸ç”¨Pipelineç¤ºä¾‹\n",
    "\n",
    "```python\n",
    "from transformers import pipeline\n",
    "\n",
    "# 1. æƒ…æ„Ÿåˆ†æ\n",
    "print(\"=== æƒ…æ„Ÿåˆ†æ ===\")\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
    "texts = [\n",
    "    \"I love this product!\",\n",
    "    \"This is terrible.\",\n",
    "    \"It's okay, not great but not bad either.\"\n",
    "]\n",
    "for text in texts:\n",
    "    result = sentiment_pipeline(text)\n",
    "    print(f\"æ–‡æœ¬: {text}\")\n",
    "    print(f\"ç»“æœ: {result[0]['label']}, ç½®ä¿¡åº¦: {result[0]['score']:.4f}\\n\")\n",
    "\n",
    "# 2. æ–‡æœ¬ç”Ÿæˆ\n",
    "print(\"=== æ–‡æœ¬ç”Ÿæˆ ===\")\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "prompt = \"Once upon a time\"\n",
    "result = generator(prompt, max_length=50, num_return_sequences=2)\n",
    "for i, text in enumerate(result):\n",
    "    print(f\"ç”Ÿæˆ {i+1}: {text['generated_text']}\\n\")\n",
    "\n",
    "# 3. é—®ç­”ç³»ç»Ÿ\n",
    "print(\"=== é—®ç­”ç³»ç»Ÿ ===\")\n",
    "qa_pipeline = pipeline(\"question-answering\")\n",
    "context = \"\"\"\n",
    "Hugging Face is a company that develops tools for building applications using machine learning.\n",
    "It is most notable for its Transformers library built for NLP applications.\n",
    "The company was founded in 2016 by French entrepreneurs.\n",
    "\"\"\"\n",
    "question = \"When was Hugging Face founded?\"\n",
    "answer = qa_pipeline(question=question, context=context)\n",
    "print(f\"é—®é¢˜: {question}\")\n",
    "print(f\"ç­”æ¡ˆ: {answer['answer']}, ç½®ä¿¡åº¦: {answer['score']:.4f}\\n\")\n",
    "\n",
    "# 4. å‘½åå®ä½“è¯†åˆ«\n",
    "print(\"=== å‘½åå®ä½“è¯†åˆ« ===\")\n",
    "ner_pipeline = pipeline(\"ner\", aggregation_strategy=\"simple\")\n",
    "text = \"Apple Inc. was founded by Steve Jobs in Cupertino, California.\"\n",
    "entities = ner_pipeline(text)\n",
    "for entity in entities:\n",
    "    print(f\"å®ä½“: {entity['word']}, ç±»å‹: {entity['entity_group']}, åˆ†æ•°: {entity['score']:.4f}\")\n",
    "\n",
    "# 5. æ–‡æœ¬æ‘˜è¦\n",
    "print(\"\\n=== æ–‡æœ¬æ‘˜è¦ ===\")\n",
    "summarizer = pipeline(\"summarization\")\n",
    "article = \"\"\"\n",
    "The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, \n",
    "and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. \n",
    "During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest \n",
    "man-made structure in the world, a title it held for 41 years until the Chrysler Building in \n",
    "New York City was finished in 1930.\n",
    "\"\"\"\n",
    "summary = summarizer(article, max_length=50, min_length=25, do_sample=False)\n",
    "print(f\"åŸæ–‡é•¿åº¦: {len(article.split())} è¯\")\n",
    "print(f\"æ‘˜è¦: {summary[0]['summary_text']}\")\n",
    "\n",
    "# 6. ç¿»è¯‘\n",
    "print(\"\\n=== ç¿»è¯‘ ===\")\n",
    "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-zh\")\n",
    "text = \"Hello, how are you today?\"\n",
    "translation = translator(text)\n",
    "print(f\"è‹±æ–‡: {text}\")\n",
    "print(f\"ä¸­æ–‡: {translation[0]['translation_text']}\")\n",
    "\n",
    "# 7. é›¶æ ·æœ¬åˆ†ç±»\n",
    "print(\"\\n=== é›¶æ ·æœ¬åˆ†ç±» ===\")\n",
    "classifier = pipeline(\"zero-shot-classification\")\n",
    "text = \"This is a tutorial about using Hugging Face with Python\"\n",
    "candidate_labels = [\"education\", \"politics\", \"technology\", \"sports\"]\n",
    "result = classifier(text, candidate_labels)\n",
    "print(f\"æ–‡æœ¬: {text}\")\n",
    "print(\"åˆ†ç±»ç»“æœ:\")\n",
    "for label, score in zip(result['labels'], result['scores']):\n",
    "    print(f\"  {label}: {score:.4f}\")\n",
    "```\n",
    "\n",
    "### 3.3 æŒ‡å®šç‰¹å®šæ¨¡å‹\n",
    "\n",
    "```python\n",
    "# ä½¿ç”¨ç‰¹å®šæ¨¡å‹è€Œä¸æ˜¯é»˜è®¤æ¨¡å‹\n",
    "from transformers import pipeline\n",
    "\n",
    "# 1. ä½¿ç”¨ä¸­æ–‡æƒ…æ„Ÿåˆ†ææ¨¡å‹\n",
    "chinese_sentiment = pipeline(\n",
    "    \"sentiment-analysis\", \n",
    "    model=\"uer/roberta-base-finetuned-dianping-chinese\"\n",
    ")\n",
    "result = chinese_sentiment(\"è¿™ä¸ªäº§å“çœŸçš„å¾ˆæ£’ï¼\")\n",
    "print(f\"ä¸­æ–‡æƒ…æ„Ÿåˆ†æ: {result}\")\n",
    "\n",
    "# 2. ä½¿ç”¨ç‰¹å®šçš„æ–‡æœ¬ç”Ÿæˆæ¨¡å‹\n",
    "gpt2_medium = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"gpt2-medium\"\n",
    ")\n",
    "\n",
    "# 3. ä½¿ç”¨æœ¬åœ°æ¨¡å‹\n",
    "# é¦–å…ˆä¸‹è½½æ¨¡å‹åˆ°æœ¬åœ°\n",
    "local_model_path = \"./my_local_model\"\n",
    "local_pipeline = pipeline(\"text-classification\", model=local_model_path)\n",
    "```\n",
    "\n",
    "## å››ã€æ·±å…¥ä½¿ç”¨ï¼šAutoModelå’ŒAutoTokenizer\n",
    "\n",
    "### 4.1 ç†è§£Tokenizerå’ŒModel\n",
    "\n",
    "å½“ä½ éœ€è¦æ›´å¤šæ§åˆ¶æ—¶ï¼Œå¯ä»¥ç›´æ¥ä½¿ç”¨AutoModelå’ŒAutoTokenizerï¼š\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# 1. åŠ è½½tokenizerå’Œmodel\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# 2. å‡†å¤‡è¾“å…¥\n",
    "text = \"I love using Hugging Face!\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "# 3. æ¨ç†\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    \n",
    "# 4. å¤„ç†è¾“å‡º\n",
    "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "print(f\"é¢„æµ‹ç»“æœ: {predictions}\")\n",
    "```\n",
    "\n",
    "### 4.2 æ‰¹é‡å¤„ç†\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# æ‰¹é‡å¤„ç†å¤šä¸ªæ–‡æœ¬\n",
    "texts = [\n",
    "    \"I love this movie!\",\n",
    "    \"This is terrible.\",\n",
    "    \"Not bad, quite enjoyable.\",\n",
    "    \"Absolutely fantastic!\"\n",
    "]\n",
    "\n",
    "# Tokenizeæ‰€æœ‰æ–‡æœ¬\n",
    "inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "# æ‰¹é‡æ¨ç†\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "\n",
    "# è§£æç»“æœ\n",
    "for i, text in enumerate(texts):\n",
    "    neg_score = predictions[i][0].item()\n",
    "    pos_score = predictions[i][1].item()\n",
    "    sentiment = \"POSITIVE\" if pos_score > neg_score else \"NEGATIVE\"\n",
    "    confidence = max(pos_score, neg_score)\n",
    "    print(f\"æ–‡æœ¬: {text}\")\n",
    "    print(f\"æƒ…æ„Ÿ: {sentiment}, ç½®ä¿¡åº¦: {confidence:.4f}\\n\")\n",
    "```\n",
    "\n",
    "### 4.3 ä½¿ç”¨ä¸åŒç±»å‹çš„æ¨¡å‹\n",
    "\n",
    "```python\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,  # åˆ†ç±»\n",
    "    AutoModelForTokenClassification,     # è¯å…ƒåˆ†ç±»ï¼ˆå¦‚NERï¼‰\n",
    "    AutoModelForQuestionAnswering,       # é—®ç­”\n",
    "    AutoModelForCausalLM,               # æ–‡æœ¬ç”Ÿæˆï¼ˆGPTç±»ï¼‰\n",
    "    AutoModelForSeq2SeqLM,              # åºåˆ—åˆ°åºåˆ—ï¼ˆT5ç±»ï¼‰\n",
    "    AutoModelForMaskedLM,               # æ©ç è¯­è¨€æ¨¡å‹ï¼ˆBERTç±»ï¼‰\n",
    "    AutoTokenizer\n",
    ")\n",
    "\n",
    "# 1. æ–‡æœ¬ç”Ÿæˆï¼ˆGPT-2ï¼‰\n",
    "print(\"=== æ–‡æœ¬ç”Ÿæˆç¤ºä¾‹ ===\")\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# è®¾ç½®pad_token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "input_text = \"The future of AI is\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# ç”Ÿæˆæ–‡æœ¬\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_length=50,\n",
    "    num_return_sequences=1,\n",
    "    temperature=0.8,\n",
    "    do_sample=True,\n",
    "    top_p=0.9\n",
    ")\n",
    "\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(f\"è¾“å…¥: {input_text}\")\n",
    "print(f\"ç”Ÿæˆ: {generated_text}\\n\")\n",
    "\n",
    "# 2. æ©ç è¯­è¨€æ¨¡å‹ï¼ˆBERTï¼‰\n",
    "print(\"=== æ©ç é¢„æµ‹ç¤ºä¾‹ ===\")\n",
    "from transformers import pipeline\n",
    "\n",
    "unmasker = pipeline('fill-mask', model='bert-base-uncased')\n",
    "text = \"The capital of France is [MASK].\"\n",
    "predictions = unmasker(text)\n",
    "\n",
    "print(f\"è¾“å…¥: {text}\")\n",
    "print(\"é¢„æµ‹ç»“æœ:\")\n",
    "for pred in predictions[:3]:\n",
    "    print(f\"  {pred['token_str']}: {pred['score']:.4f}\")\n",
    "```\n",
    "\n",
    "## äº”ã€å¤„ç†ä¸­æ–‡æ¨¡å‹\n",
    "\n",
    "### 5.1 ä¸­æ–‡æ–‡æœ¬åˆ†ç±»\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# ä½¿ç”¨ä¸­æ–‡BERTæ¨¡å‹\n",
    "model_name = \"bert-base-chinese\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# ä¸­æ–‡æ–‡æœ¬ç¤ºä¾‹\n",
    "texts = [\n",
    "    \"è¿™éƒ¨ç”µå½±çœŸçš„å¤ªç²¾å½©äº†ï¼\",\n",
    "    \"æœåŠ¡æ€åº¦å¾ˆå·®ï¼Œä¸æ¨èã€‚\",\n",
    "    \"è¿˜å¯ä»¥ï¼Œä¸€èˆ¬èˆ¬å§ã€‚\"\n",
    "]\n",
    "\n",
    "for text in texts:\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    \n",
    "    print(f\"æ–‡æœ¬: {text}\")\n",
    "    print(f\"é¢„æµ‹åˆ†æ•°: {predictions}\\n\")\n",
    "```\n",
    "\n",
    "### 5.2 ä¸­æ–‡é—®ç­”ç³»ç»Ÿ\n",
    "\n",
    "```python\n",
    "from transformers import pipeline\n",
    "\n",
    "# ä½¿ç”¨ä¸­æ–‡é—®ç­”æ¨¡å‹\n",
    "qa_pipeline = pipeline(\n",
    "    \"question-answering\",\n",
    "    model=\"uer/roberta-base-chinese-extractive-qa\"\n",
    ")\n",
    "\n",
    "context = \"\"\"\n",
    "æ­å·æ˜¯æµ™æ±Ÿçœçš„çœä¼šåŸå¸‚ï¼Œä½äºä¸­å›½ä¸œå—æ²¿æµ·ã€‚\n",
    "æ­å·ä»¥å…¶ç¾ä¸½çš„è¥¿æ¹–é£æ™¯è€Œé—»åï¼Œè¢«èª‰ä¸º\"äººé—´å¤©å ‚\"ã€‚\n",
    "è¿™åº§åŸå¸‚æœ‰ç€æ‚ ä¹…çš„å†å²ï¼Œå¯ä»¥è¿½æº¯åˆ°2200å¤šå¹´å‰ã€‚\n",
    "æ­å·ä¹Ÿæ˜¯ä¸­å›½é‡è¦çš„ç»æµä¸­å¿ƒï¼Œé˜¿é‡Œå·´å·´æ€»éƒ¨å°±ä½äºæ­¤åœ°ã€‚\n",
    "\"\"\"\n",
    "\n",
    "questions = [\n",
    "    \"æ­å·æ˜¯å“ªä¸ªçœçš„çœä¼šï¼Ÿ\",\n",
    "    \"æ­å·æœ‰ä»€ä¹ˆè‘—åæ™¯ç‚¹ï¼Ÿ\",\n",
    "    \"å“ªå®¶çŸ¥åå…¬å¸çš„æ€»éƒ¨åœ¨æ­å·ï¼Ÿ\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    answer = qa_pipeline(question=question, context=context)\n",
    "    print(f\"é—®é¢˜: {question}\")\n",
    "    print(f\"ç­”æ¡ˆ: {answer['answer']}, ç½®ä¿¡åº¦: {answer['score']:.4f}\\n\")\n",
    "```\n",
    "\n",
    "### 5.3 ä¸­æ–‡æ–‡æœ¬ç”Ÿæˆ\n",
    "\n",
    "```python\n",
    "from transformers import pipeline\n",
    "\n",
    "# ä½¿ç”¨ä¸­æ–‡GPTæ¨¡å‹\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"uer/gpt2-chinese-cluecorpussmall\",\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "prompts = [\n",
    "    \"ä»Šå¤©å¤©æ°”\",\n",
    "    \"äººå·¥æ™ºèƒ½çš„å‘å±•\",\n",
    "    \"æˆ‘æœ€å–œæ¬¢çš„\"\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    result = generator(\n",
    "        prompt,\n",
    "        max_length=50,\n",
    "        num_return_sequences=1,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    print(f\"æç¤º: {prompt}\")\n",
    "    print(f\"ç”Ÿæˆ: {result[0]['generated_text']}\\n\")\n",
    "```\n",
    "\n",
    "## å…­ã€å›¾åƒå¤„ç†æ¨¡å‹\n",
    "\n",
    "### 6.1 å›¾åƒåˆ†ç±»\n",
    "\n",
    "```python\n",
    "from transformers import pipeline\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "# åˆ›å»ºå›¾åƒåˆ†ç±»pipeline\n",
    "classifier = pipeline(\"image-classification\", model=\"google/vit-base-patch16-224\")\n",
    "\n",
    "# æ–¹æ³•1ï¼šä»URLåŠ è½½å›¾åƒ\n",
    "url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "# åˆ†ç±»\n",
    "results = classifier(image)\n",
    "print(\"å›¾åƒåˆ†ç±»ç»“æœ:\")\n",
    "for result in results[:3]:\n",
    "    print(f\"  {result['label']}: {result['score']:.4f}\")\n",
    "\n",
    "# æ–¹æ³•2ï¼šä»æœ¬åœ°æ–‡ä»¶åŠ è½½\n",
    "# image = Image.open(\"path/to/your/image.jpg\")\n",
    "# results = classifier(image)\n",
    "```\n",
    "\n",
    "### 6.2 ç›®æ ‡æ£€æµ‹\n",
    "\n",
    "```python\n",
    "from transformers import pipeline\n",
    "from PIL import Image, ImageDraw\n",
    "import requests\n",
    "\n",
    "# åˆ›å»ºç›®æ ‡æ£€æµ‹pipeline\n",
    "detector = pipeline(\"object-detection\", model=\"facebook/detr-resnet-50\")\n",
    "\n",
    "# åŠ è½½å›¾åƒ\n",
    "url = \"https://images.unsplash.com/photo-1518991669955-9c7e78ec80ca\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "# æ£€æµ‹å¯¹è±¡\n",
    "results = detector(image)\n",
    "\n",
    "# å¯è§†åŒ–ç»“æœ\n",
    "draw = ImageDraw.Draw(image)\n",
    "for result in results:\n",
    "    box = result['box']\n",
    "    label = result['label']\n",
    "    score = result['score']\n",
    "    \n",
    "    if score > 0.9:  # åªæ˜¾ç¤ºé«˜ç½®ä¿¡åº¦çš„æ£€æµ‹\n",
    "        # ç”»è¾¹ç•Œæ¡†\n",
    "        draw.rectangle(\n",
    "            [box['xmin'], box['ymin'], box['xmax'], box['ymax']],\n",
    "            outline=\"red\",\n",
    "            width=3\n",
    "        )\n",
    "        # æ·»åŠ æ ‡ç­¾\n",
    "        draw.text(\n",
    "            (box['xmin'], box['ymin']),\n",
    "            f\"{label}: {score:.2f}\",\n",
    "            fill=\"red\"\n",
    "        )\n",
    "\n",
    "# ä¿å­˜ç»“æœ\n",
    "image.save(\"detection_result.jpg\")\n",
    "print(\"æ£€æµ‹ç»“æœå·²ä¿å­˜åˆ° detection_result.jpg\")\n",
    "```\n",
    "\n",
    "### 6.3 å›¾åƒæè¿°ç”Ÿæˆ\n",
    "\n",
    "```python\n",
    "from transformers import pipeline\n",
    "from PIL import Image\n",
    "\n",
    "# åˆ›å»ºå›¾åƒåˆ°æ–‡æœ¬pipeline\n",
    "image_to_text = pipeline(\n",
    "    \"image-to-text\",\n",
    "    model=\"Salesforce/blip-image-captioning-base\"\n",
    ")\n",
    "\n",
    "# åŠ è½½å›¾åƒ\n",
    "image = Image.open(\"your_image.jpg\")\n",
    "\n",
    "# ç”Ÿæˆæè¿°\n",
    "result = image_to_text(image)\n",
    "print(f\"å›¾åƒæè¿°: {result[0]['generated_text']}\")\n",
    "```\n",
    "\n",
    "## ä¸ƒã€éŸ³é¢‘å¤„ç†æ¨¡å‹\n",
    "\n",
    "### 7.1 è¯­éŸ³è¯†åˆ«\n",
    "\n",
    "```python\n",
    "from transformers import pipeline\n",
    "import librosa\n",
    "\n",
    "# åˆ›å»ºè¯­éŸ³è¯†åˆ«pipeline\n",
    "transcriber = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=\"openai/whisper-small\"\n",
    ")\n",
    "\n",
    "# åŠ è½½éŸ³é¢‘æ–‡ä»¶\n",
    "# æ³¨æ„ï¼šéœ€è¦å®‰è£… librosa: pip install librosa\n",
    "audio_file = \"path/to/audio.wav\"\n",
    "\n",
    "# è½¬å½•\n",
    "result = transcriber(audio_file)\n",
    "print(f\"è½¬å½•ç»“æœ: {result['text']}\")\n",
    "\n",
    "# å¤„ç†é•¿éŸ³é¢‘\n",
    "result = transcriber(\n",
    "    audio_file,\n",
    "    chunk_length_s=30,  # æ¯30ç§’ä¸€ä¸ªå—\n",
    "    stride_length_s=5   # 5ç§’é‡å \n",
    ")\n",
    "print(f\"é•¿éŸ³é¢‘è½¬å½•: {result['text']}\")\n",
    "```\n",
    "\n",
    "### 7.2 éŸ³é¢‘åˆ†ç±»\n",
    "\n",
    "```python\n",
    "from transformers import pipeline\n",
    "\n",
    "# åˆ›å»ºéŸ³é¢‘åˆ†ç±»pipeline\n",
    "audio_classifier = pipeline(\n",
    "    \"audio-classification\",\n",
    "    model=\"superb/hubert-base-superb-er\"\n",
    ")\n",
    "\n",
    "# åˆ†ç±»éŸ³é¢‘\n",
    "audio_file = \"path/to/audio.wav\"\n",
    "results = audio_classifier(audio_file)\n",
    "\n",
    "print(\"éŸ³é¢‘åˆ†ç±»ç»“æœ:\")\n",
    "for result in results[:3]:\n",
    "    print(f\"  {result['label']}: {result['score']:.4f}\")\n",
    "```\n",
    "\n",
    "## å…«ã€å¤šæ¨¡æ€æ¨¡å‹\n",
    "\n",
    "### 8.1 è§†è§‰é—®ç­”ï¼ˆVQAï¼‰\n",
    "\n",
    "```python\n",
    "from transformers import pipeline\n",
    "from PIL import Image\n",
    "\n",
    "# åˆ›å»ºVQA pipeline\n",
    "vqa = pipeline(\"visual-question-answering\")\n",
    "\n",
    "# åŠ è½½å›¾åƒ\n",
    "image = Image.open(\"path/to/image.jpg\")\n",
    "\n",
    "# æé—®\n",
    "questions = [\n",
    "    \"What color is the sky?\",\n",
    "    \"How many people are in the image?\",\n",
    "    \"What is the main object in the picture?\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    result = vqa(image=image, question=question)\n",
    "    print(f\"é—®é¢˜: {question}\")\n",
    "    print(f\"ç­”æ¡ˆ: {result[0]['answer']}, ç½®ä¿¡åº¦: {result[0]['score']:.4f}\\n\")\n",
    "```\n",
    "\n",
    "### 8.2 CLIPæ¨¡å‹ï¼ˆå›¾æ–‡åŒ¹é…ï¼‰\n",
    "\n",
    "```python\n",
    "from transformers import pipeline\n",
    "\n",
    "# åˆ›å»ºé›¶æ ·æœ¬å›¾åƒåˆ†ç±»pipelineï¼ˆä½¿ç”¨CLIPï¼‰\n",
    "classifier = pipeline(\n",
    "    \"zero-shot-image-classification\",\n",
    "    model=\"openai/clip-vit-base-patch32\"\n",
    ")\n",
    "\n",
    "# åŠ è½½å›¾åƒ\n",
    "from PIL import Image\n",
    "image = Image.open(\"path/to/image.jpg\")\n",
    "\n",
    "# å®šä¹‰å€™é€‰æ ‡ç­¾\n",
    "candidate_labels = [\"cat\", \"dog\", \"bird\", \"car\", \"tree\", \"building\"]\n",
    "\n",
    "# åˆ†ç±»\n",
    "results = classifier(\n",
    "    images=image,\n",
    "    candidate_labels=candidate_labels\n",
    ")\n",
    "\n",
    "print(\"å›¾åƒåˆ†ç±»ç»“æœ:\")\n",
    "for result in results:\n",
    "    print(f\"  {result['label']}: {result['score']:.4f}\")\n",
    "```\n",
    "\n",
    "## ä¹ã€æ¨¡å‹å¾®è°ƒåŸºç¡€\n",
    "\n",
    "### 9.1 å‡†å¤‡æ•°æ®é›†\n",
    "\n",
    "```python\n",
    "from datasets import Dataset, DatasetDict\n",
    "import pandas as pd\n",
    "\n",
    "# åˆ›å»ºç¤ºä¾‹æ•°æ®\n",
    "data = {\n",
    "    'text': [\n",
    "        \"I love this product!\",\n",
    "        \"This is terrible.\",\n",
    "        \"Great quality, highly recommend.\",\n",
    "        \"Waste of money.\",\n",
    "        \"Not bad, decent value.\"\n",
    "    ],\n",
    "    'label': [1, 0, 1, 0, 1]  # 1: positive, 0: negative\n",
    "}\n",
    "\n",
    "# åˆ›å»ºDataset\n",
    "df = pd.DataFrame(data)\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# åˆ†å‰²è®­ç»ƒé›†å’Œæµ‹è¯•é›†\n",
    "train_test = dataset.train_test_split(test_size=0.2)\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': train_test['train'],\n",
    "    'test': train_test['test']\n",
    "})\n",
    "\n",
    "print(f\"è®­ç»ƒé›†å¤§å°: {len(dataset_dict['train'])}\")\n",
    "print(f\"æµ‹è¯•é›†å¤§å°: {len(dataset_dict['test'])}\")\n",
    "```\n",
    "\n",
    "### 9.2 ç®€å•çš„æ¨¡å‹å¾®è°ƒ\n",
    "\n",
    "```python\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "\n",
    "# åŠ è½½æ¨¡å‹å’Œtokenizer\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# æ•°æ®é¢„å¤„ç†å‡½æ•°\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, padding=True)\n",
    "\n",
    "# åº”ç”¨é¢„å¤„ç†\n",
    "tokenized_datasets = dataset_dict.map(preprocess_function, batched=True)\n",
    "\n",
    "# è®¾ç½®è®­ç»ƒå‚æ•°\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "# å®šä¹‰è¯„ä¼°å‡½æ•°\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    accuracy = (predictions == labels).mean()\n",
    "    return {\"accuracy\": accuracy}\n",
    "\n",
    "# åˆ›å»ºTrainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# è®­ç»ƒæ¨¡å‹\n",
    "# trainer.train()\n",
    "\n",
    "# ä¿å­˜æ¨¡å‹\n",
    "# trainer.save_model(\"./my-finetuned-model\")\n",
    "```\n",
    "\n",
    "## åã€æ€§èƒ½ä¼˜åŒ–\n",
    "\n",
    "### 10.1 ä½¿ç”¨GPUåŠ é€Ÿ\n",
    "\n",
    "```python\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "# æ£€æŸ¥GPUæ˜¯å¦å¯ç”¨\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "print(f\"ä½¿ç”¨è®¾å¤‡: {'GPU' if device == 0 else 'CPU'}\")\n",
    "\n",
    "# åœ¨GPUä¸Šè¿è¡Œpipeline\n",
    "classifier = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# æ‰¹é‡å¤„ç†ä»¥æé«˜æ•ˆç‡\n",
    "texts = [\"Text 1\", \"Text 2\", \"Text 3\"] * 100\n",
    "results = classifier(texts, batch_size=32)\n",
    "```\n",
    "\n",
    "### 10.2 æ¨¡å‹é‡åŒ–\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# åŠ è½½æ¨¡å‹\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# åŠ¨æ€é‡åŒ–\n",
    "quantized_model = torch.quantization.quantize_dynamic(\n",
    "    model,\n",
    "    {torch.nn.Linear},\n",
    "    dtype=torch.qint8\n",
    ")\n",
    "\n",
    "# æ¯”è¾ƒæ¨¡å‹å¤§å°\n",
    "import os\n",
    "\n",
    "def get_model_size(model):\n",
    "    torch.save(model.state_dict(), \"temp.p\")\n",
    "    size = os.path.getsize(\"temp.p\") / 1e6\n",
    "    os.remove(\"temp.p\")\n",
    "    return size\n",
    "\n",
    "print(f\"åŸå§‹æ¨¡å‹å¤§å°: {get_model_size(model):.2f} MB\")\n",
    "print(f\"é‡åŒ–æ¨¡å‹å¤§å°: {get_model_size(quantized_model):.2f} MB\")\n",
    "```\n",
    "\n",
    "### 10.3 ä½¿ç”¨ONNXåŠ é€Ÿ\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "# å¯¼å‡ºä¸ºONNXæ ¼å¼\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# å‡†å¤‡è™šæ‹Ÿè¾“å…¥\n",
    "dummy_input = tokenizer(\n",
    "    \"Hello, world!\",\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "# å¯¼å‡ºONNX\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    tuple(dummy_input.values()),\n",
    "    \"model.onnx\",\n",
    "    input_names=['input_ids', 'attention_mask'],\n",
    "    output_names=['output'],\n",
    "    dynamic_axes={\n",
    "        'input_ids': {0: 'batch_size', 1: 'sequence'},\n",
    "        'attention_mask': {0: 'batch_size', 1: 'sequence'},\n",
    "        'output': {0: 'batch_size'}\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"æ¨¡å‹å·²å¯¼å‡ºä¸ºONNXæ ¼å¼\")\n",
    "```\n",
    "\n",
    "## åä¸€ã€å®æˆ˜é¡¹ç›®ï¼šæ„å»ºæ™ºèƒ½é—®ç­”ç³»ç»Ÿ\n",
    "\n",
    "### 11.1 é¡¹ç›®ç»“æ„\n",
    "\n",
    "```python\n",
    "# qa_system.py\n",
    "from transformers import pipeline\n",
    "from typing import List, Dict\n",
    "import json\n",
    "\n",
    "class QASystem:\n",
    "    def __init__(self, model_name=\"deepset/roberta-base-squad2\"):\n",
    "        \"\"\"åˆå§‹åŒ–é—®ç­”ç³»ç»Ÿ\"\"\"\n",
    "        self.qa_pipeline = pipeline(\"question-answering\", model=model_name)\n",
    "        self.knowledge_base = {}\n",
    "        \n",
    "    def add_document(self, doc_id: str, title: str, content: str):\n",
    "        \"\"\"æ·»åŠ æ–‡æ¡£åˆ°çŸ¥è¯†åº“\"\"\"\n",
    "        self.knowledge_base[doc_id] = {\n",
    "            \"title\": title,\n",
    "            \"content\": content\n",
    "        }\n",
    "        \n",
    "    def load_knowledge_base(self, file_path: str):\n",
    "        \"\"\"ä»æ–‡ä»¶åŠ è½½çŸ¥è¯†åº“\"\"\"\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            self.knowledge_base = json.load(f)\n",
    "            \n",
    "    def search_documents(self, query: str, top_k: int = 3) -> List[str]:\n",
    "        \"\"\"ç®€å•çš„æ–‡æ¡£æœç´¢ï¼ˆåŸºäºå…³é”®è¯ï¼‰\"\"\"\n",
    "        query_words = set(query.lower().split())\n",
    "        scores = {}\n",
    "        \n",
    "        for doc_id, doc in self.knowledge_base.items():\n",
    "            content_words = set(doc['content'].lower().split())\n",
    "            score = len(query_words.intersection(content_words))\n",
    "            scores[doc_id] = score\n",
    "            \n",
    "        # è¿”å›å¾—åˆ†æœ€é«˜çš„æ–‡æ¡£\n",
    "        sorted_docs = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        return [doc_id for doc_id, _ in sorted_docs[:top_k]]\n",
    "    \n",
    "    def answer_question(self, question: str, context: str = None) -> Dict:\n",
    "        \"\"\"å›ç­”é—®é¢˜\"\"\"\n",
    "        if context is None:\n",
    "            # ä»çŸ¥è¯†åº“ä¸­æœç´¢ç›¸å…³æ–‡æ¡£\n",
    "            relevant_docs = self.search_documents(question)\n",
    "            if not relevant_docs:\n",
    "                return {\n",
    "                    \"answer\": \"æŠ±æ­‰ï¼Œæˆ‘åœ¨çŸ¥è¯†åº“ä¸­æ‰¾ä¸åˆ°ç›¸å…³ä¿¡æ¯ã€‚\",\n",
    "                    \"confidence\": 0.0,\n",
    "                    \"source\": None\n",
    "                }\n",
    "            \n",
    "            # åˆå¹¶ç›¸å…³æ–‡æ¡£ä½œä¸ºä¸Šä¸‹æ–‡\n",
    "            contexts = []\n",
    "            for doc_id in relevant_docs:\n",
    "                doc = self.knowledge_base[doc_id]\n",
    "                contexts.append(f\"{doc['title']}: {doc['content']}\")\n",
    "            context = \" \".join(contexts)\n",
    "            source = relevant_docs[0]\n",
    "        else:\n",
    "            source = \"provided_context\"\n",
    "        \n",
    "        # ä½¿ç”¨é—®ç­”æ¨¡å‹\n",
    "        try:\n",
    "            result = self.qa_pipeline(\n",
    "                question=question,\n",
    "                context=context,\n",
    "                max_answer_len=100\n",
    "            )\n",
    "            \n",
    "            return {\n",
    "                \"answer\": result['answer'],\n",
    "                \"confidence\": result['score'],\n",
    "                \"source\": source,\n",
    "                \"context_used\": context[:200] + \"...\" if len(context) > 200 else context\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"answer\": f\"å¤„ç†é—®é¢˜æ—¶å‡ºé”™: {str(e)}\",\n",
    "                \"confidence\": 0.0,\n",
    "                \"source\": None\n",
    "            }\n",
    "    \n",
    "    def interactive_qa(self):\n",
    "        \"\"\"äº¤äº’å¼é—®ç­”\"\"\"\n",
    "        print(\"æ™ºèƒ½é—®ç­”ç³»ç»Ÿå·²å¯åŠ¨ï¼è¾“å…¥ 'quit' é€€å‡ºã€‚\")\n",
    "        print(\"è¾“å…¥ 'add' æ·»åŠ æ–°æ–‡æ¡£åˆ°çŸ¥è¯†åº“ã€‚\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        while True:\n",
    "            user_input = input(\"\\nè¯·è¾“å…¥æ‚¨çš„é—®é¢˜: \").strip()\n",
    "            \n",
    "            if user_input.lower() == 'quit':\n",
    "                print(\"æ„Ÿè°¢ä½¿ç”¨ï¼Œå†è§ï¼\")\n",
    "                break\n",
    "                \n",
    "            elif user_input.lower() == 'add':\n",
    "                title = input(\"æ–‡æ¡£æ ‡é¢˜: \")\n",
    "                content = input(\"æ–‡æ¡£å†…å®¹: \")\n",
    "                doc_id = f\"doc_{len(self.knowledge_base) + 1}\"\n",
    "                self.add_document(doc_id, title, content)\n",
    "                print(f\"æ–‡æ¡£å·²æ·»åŠ ï¼ŒID: {doc_id}\")\n",
    "                \n",
    "            else:\n",
    "                result = self.answer_question(user_input)\n",
    "                print(f\"\\nç­”æ¡ˆ: {result['answer']}\")\n",
    "                print(f\"ç½®ä¿¡åº¦: {result['confidence']:.2%}\")\n",
    "                if result['source']:\n",
    "                    print(f\"æ¥æº: {result['source']}\")\n",
    "\n",
    "# ä½¿ç”¨ç¤ºä¾‹\n",
    "if __name__ == \"__main__\":\n",
    "    # åˆ›å»ºé—®ç­”ç³»ç»Ÿ\n",
    "    qa = QASystem()\n",
    "    \n",
    "    # æ·»åŠ ä¸€äº›ç¤ºä¾‹æ–‡æ¡£\n",
    "    qa.add_document(\n",
    "        \"doc_1\",\n",
    "        \"Pythonç®€ä»‹\",\n",
    "        \"Pythonæ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€ï¼Œç”±Guido van Rossumåœ¨1991å¹´åˆ›å»ºã€‚å®ƒä»¥ç®€æ´æ˜“è¯»çš„è¯­æ³•è‘—ç§°ï¼Œå¹¿æ³›åº”ç”¨äºæ•°æ®ç§‘å­¦ã€äººå·¥æ™ºèƒ½ã€Webå¼€å‘ç­‰é¢†åŸŸã€‚\"\n",
    "    )\n",
    "    \n",
    "    qa.add_document(\n",
    "        \"doc_2\",\n",
    "        \"Hugging Faceä»‹ç»\",\n",
    "        \"Hugging Faceæ˜¯ä¸€å®¶ä¸“æ³¨äºè‡ªç„¶è¯­è¨€å¤„ç†çš„å…¬å¸ï¼Œæä¾›äº†Transformersåº“ï¼Œä½¿å¾—ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹å˜å¾—éå¸¸ç®€å•ã€‚å®ƒçš„æ¨¡å‹ä¸­å¿ƒæ‰˜ç®¡äº†æ•°ä¸‡ä¸ªæ¨¡å‹ã€‚\"\n",
    "    )\n",
    "    \n",
    "    qa.add_document(\n",
    "        \"doc_3\",\n",
    "        \"æœºå™¨å­¦ä¹ åŸºç¡€\",\n",
    "        \"æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œè®©è®¡ç®—æœºèƒ½å¤Ÿä»æ•°æ®ä¸­å­¦ä¹ ã€‚ä¸»è¦åˆ†ä¸ºç›‘ç£å­¦ä¹ ã€æ— ç›‘ç£å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ ä¸‰å¤§ç±»ã€‚\"\n",
    "    )\n",
    "    \n",
    "    # æµ‹è¯•é—®ç­”\n",
    "    questions = [\n",
    "        \"Pythonæ˜¯ä»€ä¹ˆæ—¶å€™åˆ›å»ºçš„ï¼Ÿ\",\n",
    "        \"Hugging Faceæä¾›äº†ä»€ä¹ˆåº“ï¼Ÿ\",\n",
    "        \"æœºå™¨å­¦ä¹ æœ‰å“ªäº›ç±»å‹ï¼Ÿ\",\n",
    "        \"è°åˆ›å»ºäº†Pythonï¼Ÿ\"\n",
    "    ]\n",
    "    \n",
    "    print(\"=== æµ‹è¯•é—®ç­”ç³»ç»Ÿ ===\\n\")\n",
    "    for q in questions:\n",
    "        print(f\"é—®é¢˜: {q}\")\n",
    "        result = qa.answer_question(q)\n",
    "        print(f\"ç­”æ¡ˆ: {result['answer']}\")\n",
    "        print(f\"ç½®ä¿¡åº¦: {result['confidence']:.2%}\")\n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    # å¯åŠ¨äº¤äº’æ¨¡å¼\n",
    "    # qa.interactive_qa()\n",
    "```\n",
    "\n",
    "### 11.2 å¢å¼ºç‰ˆï¼šç»“åˆå‘é‡æ•°æ®åº“\n",
    "\n",
    "```python\n",
    "# enhanced_qa_system.py\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "import faiss\n",
    "\n",
    "class EnhancedQASystem:\n",
    "    def __init__(self):\n",
    "        # åˆå§‹åŒ–å¥å­ç¼–ç å™¨\n",
    "        self.encoder = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "        \n",
    "        # åˆå§‹åŒ–é—®ç­”æ¨¡å‹\n",
    "        from transformers import pipeline\n",
    "        self.qa_pipeline = pipeline(\"question-answering\")\n",
    "        \n",
    "        # åˆå§‹åŒ–å‘é‡ç´¢å¼•\n",
    "        self.dimension = 384  # MiniLMè¾“å‡ºç»´åº¦\n",
    "        self.index = faiss.IndexFlatL2(self.dimension)\n",
    "        \n",
    "        # æ–‡æ¡£å­˜å‚¨\n",
    "        self.documents = []\n",
    "        \n",
    "    def add_document(self, text: str, metadata: dict = None):\n",
    "        \"\"\"æ·»åŠ æ–‡æ¡£å¹¶å»ºç«‹å‘é‡ç´¢å¼•\"\"\"\n",
    "        # ç¼–ç æ–‡æ¡£\n",
    "        embedding = self.encoder.encode([text])\n",
    "        \n",
    "        # æ·»åŠ åˆ°ç´¢å¼•\n",
    "        self.index.add(embedding)\n",
    "        \n",
    "        # å­˜å‚¨æ–‡æ¡£\n",
    "        self.documents.append({\n",
    "            'text': text,\n",
    "            'metadata': metadata or {},\n",
    "            'embedding': embedding[0]\n",
    "        })\n",
    "        \n",
    "    def search_similar(self, query: str, k: int = 3) -> List[Tuple[int, float]]:\n",
    "        \"\"\"æœç´¢ç›¸ä¼¼æ–‡æ¡£\"\"\"\n",
    "        # ç¼–ç æŸ¥è¯¢\n",
    "        query_embedding = self.encoder.encode([query])\n",
    "        \n",
    "        # æœç´¢\n",
    "        distances, indices = self.index.search(query_embedding, k)\n",
    "        \n",
    "        # è¿”å›ç»“æœ\n",
    "        results = []\n",
    "        for i, (idx, dist) in enumerate(zip(indices[0], distances[0])):\n",
    "            if idx < len(self.documents):\n",
    "                results.append((idx, float(dist)))\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def answer_question_with_context(self, question: str) -> dict:\n",
    "        \"\"\"åŸºäºç›¸ä¼¼æ–‡æ¡£å›ç­”é—®é¢˜\"\"\"\n",
    "        # æœç´¢ç›¸å…³æ–‡æ¡£\n",
    "        similar_docs = self.search_similar(question, k=3)\n",
    "        \n",
    "        if not similar_docs:\n",
    "            return {\n",
    "                'answer': 'æ‰¾ä¸åˆ°ç›¸å…³ä¿¡æ¯',\n",
    "                'confidence': 0.0,\n",
    "                'sources': []\n",
    "            }\n",
    "        \n",
    "        # æ„å»ºä¸Šä¸‹æ–‡\n",
    "        contexts = []\n",
    "        sources = []\n",
    "        for idx, distance in similar_docs:\n",
    "            doc = self.documents[idx]\n",
    "            contexts.append(doc['text'])\n",
    "            sources.append({\n",
    "                'text': doc['text'][:100] + '...',\n",
    "                'similarity': 1 / (1 + distance)  # è½¬æ¢ä¸ºç›¸ä¼¼åº¦åˆ†æ•°\n",
    "            })\n",
    "        \n",
    "        combined_context = ' '.join(contexts)\n",
    "        \n",
    "        # é—®ç­”\n",
    "        result = self.qa_pipeline(\n",
    "            question=question,\n",
    "            context=combined_context\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'answer': result['answer'],\n",
    "            'confidence': result['score'],\n",
    "            'sources': sources\n",
    "        }\n",
    "\n",
    "# ä½¿ç”¨ç¤ºä¾‹\n",
    "if __name__ == \"__main__\":\n",
    "    qa = EnhancedQASystem()\n",
    "    \n",
    "    # æ·»åŠ æ–‡æ¡£\n",
    "    documents = [\n",
    "        \"æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªå­é›†ï¼Œä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œæ¥å­¦ä¹ æ•°æ®çš„è¡¨ç¤ºã€‚\",\n",
    "        \"å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ç‰¹åˆ«é€‚åˆå¤„ç†å›¾åƒæ•°æ®ï¼Œåœ¨è®¡ç®—æœºè§†è§‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚\",\n",
    "        \"å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰æ“…é•¿å¤„ç†åºåˆ—æ•°æ®ï¼Œå¦‚æ–‡æœ¬å’Œæ—¶é—´åºåˆ—ã€‚\",\n",
    "        \"Transformeræ¶æ„é©å‘½æ€§åœ°æ”¹å˜äº†NLPé¢†åŸŸï¼ŒBERTå’ŒGPTéƒ½åŸºäºè¿™ç§æ¶æ„ã€‚\"\n",
    "    ]\n",
    "    \n",
    "    for doc in documents:\n",
    "        qa.add_document(doc)\n",
    "    \n",
    "    # æµ‹è¯•é—®ç­”\n",
    "    question = \"ä»€ä¹ˆç¥ç»ç½‘ç»œé€‚åˆå¤„ç†å›¾åƒï¼Ÿ\"\n",
    "    result = qa.answer_question_with_context(question)\n",
    "    \n",
    "    print(f\"é—®é¢˜: {question}\")\n",
    "    print(f\"ç­”æ¡ˆ: {result['answer']}\")\n",
    "    print(f\"ç½®ä¿¡åº¦: {result['confidence']:.2%}\")\n",
    "    print(\"\\nç›¸å…³æ–‡æ¡£:\")\n",
    "    for i, source in enumerate(result['sources']):\n",
    "        print(f\"{i+1}. {source['text']} (ç›¸ä¼¼åº¦: {source['similarity']:.2%})\")\n",
    "```\n",
    "\n",
    "## åäºŒã€æœ€ä½³å®è·µå’Œæ³¨æ„äº‹é¡¹\n",
    "\n",
    "### 12.1 æ¨¡å‹é€‰æ‹©æŒ‡å—\n",
    "\n",
    "```python\n",
    "# model_selection_guide.py\n",
    "\n",
    "def recommend_model(task: str, language: str = \"en\", size_constraint: str = \"medium\"):\n",
    "    \"\"\"æ ¹æ®ä»»åŠ¡æ¨èåˆé€‚çš„æ¨¡å‹\"\"\"\n",
    "    \n",
    "    recommendations = {\n",
    "        \"sentiment-analysis\": {\n",
    "            \"en\": {\n",
    "                \"small\": \"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "                \"medium\": \"roberta-base-sentiment\",\n",
    "                \"large\": \"roberta-large-mnli\"\n",
    "            },\n",
    "            \"zh\": {\n",
    "                \"small\": \"uer/roberta-base-finetuned-dianping-chinese\",\n",
    "                \"medium\": \"bert-base-chinese\",\n",
    "                \"large\": \"hfl/chinese-roberta-wwm-ext-large\"\n",
    "            }\n",
    "        },\n",
    "        \"text-generation\": {\n",
    "            \"en\": {\n",
    "                \"small\": \"gpt2\",\n",
    "                \"medium\": \"gpt2-medium\",\n",
    "                \"large\": \"gpt2-large\"\n",
    "            },\n",
    "            \"zh\": {\n",
    "                \"small\": \"uer/gpt2-chinese-cluecorpussmall\",\n",
    "                \"medium\": \"IDEA-CCNL/Wenzhong-GPT2-110M\",\n",
    "                \"large\": \"IDEA-CCNL/Wenzhong2.0-GPT2-3.5B-chinese\"\n",
    "            }\n",
    "        },\n",
    "        \"question-answering\": {\n",
    "            \"en\": {\n",
    "                \"small\": \"distilbert-base-cased-distilled-squad\",\n",
    "                \"medium\": \"deepset/roberta-base-squad2\",\n",
    "                \"large\": \"deepset/roberta-large-squad2\"\n",
    "            },\n",
    "            \"zh\": {\n",
    "                \"small\": \"uer/roberta-base-chinese-extractive-qa\",\n",
    "                \"medium\": \"luhua/chinese_pretrain_mrc_roberta_wwm_ext_base\",\n",
    "                \"large\": \"luhua/chinese_pretrain_mrc_macbert_large\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    if task in recommendations and language in recommendations[task]:\n",
    "        return recommendations[task][language].get(size_constraint, \"æœªæ‰¾åˆ°åˆé€‚çš„æ¨¡å‹\")\n",
    "    else:\n",
    "        return \"ä¸æ”¯æŒçš„ä»»åŠ¡æˆ–è¯­è¨€\"\n",
    "\n",
    "# ä½¿ç”¨ç¤ºä¾‹\n",
    "print(\"æƒ…æ„Ÿåˆ†æï¼ˆä¸­æ–‡ï¼Œå°æ¨¡å‹ï¼‰:\", recommend_model(\"sentiment-analysis\", \"zh\", \"small\"))\n",
    "print(\"æ–‡æœ¬ç”Ÿæˆï¼ˆè‹±æ–‡ï¼Œä¸­ç­‰ï¼‰:\", recommend_model(\"text-generation\", \"en\", \"medium\"))\n",
    "print(\"é—®ç­”ï¼ˆä¸­æ–‡ï¼Œå¤§æ¨¡å‹ï¼‰:\", recommend_model(\"question-answering\", \"zh\", \"large\"))\n",
    "```\n",
    "\n",
    "### 12.2 é”™è¯¯å¤„ç†å’Œæ—¥å¿—\n",
    "\n",
    "```python\n",
    "import logging\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "# é…ç½®æ—¥å¿—\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class SafeModelWrapper:\n",
    "    def __init__(self, task: str, model: str = None):\n",
    "        self.task = task\n",
    "        self.model_name = model\n",
    "        self.pipeline = None\n",
    "        self._initialize()\n",
    "    \n",
    "    def _initialize(self):\n",
    "        \"\"\"å®‰å…¨åˆå§‹åŒ–æ¨¡å‹\"\"\"\n",
    "        try:\n",
    "            logger.info(f\"æ­£åœ¨åŠ è½½æ¨¡å‹: {self.model_name or 'default'} for {self.task}\")\n",
    "            self.pipeline = pipeline(self.task, model=self.model_name)\n",
    "            logger.info(\"æ¨¡å‹åŠ è½½æˆåŠŸ\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def predict(self, *args, **kwargs):\n",
    "        \"\"\"å®‰å…¨é¢„æµ‹\"\"\"\n",
    "        if self.pipeline is None:\n",
    "            logger.error(\"æ¨¡å‹æœªåˆå§‹åŒ–\")\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            result = self.pipeline(*args, **kwargs)\n",
    "            logger.info(\"é¢„æµ‹æˆåŠŸ\")\n",
    "            return result\n",
    "        except torch.cuda.OutOfMemoryError:\n",
    "            logger.error(\"GPUå†…å­˜ä¸è¶³\")\n",
    "            # å°è¯•æ¸…ç†å†…å­˜å¹¶ä½¿ç”¨CPU\n",
    "            torch.cuda.empty_cache()\n",
    "            self.pipeline.device = -1\n",
    "            logger.info(\"åˆ‡æ¢åˆ°CPUæ¨¡å¼é‡è¯•\")\n",
    "            return self.predict(*args, **kwargs)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"é¢„æµ‹å¤±è´¥: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "# ä½¿ç”¨ç¤ºä¾‹\n",
    "safe_classifier = SafeModelWrapper(\"sentiment-analysis\")\n",
    "result = safe_classifier.predict(\"This is a great product!\")\n",
    "if result:\n",
    "    print(f\"é¢„æµ‹ç»“æœ: {result}\")\n",
    "```\n",
    "\n",
    "### 12.3 èµ„æºç®¡ç†\n",
    "\n",
    "```python\n",
    "import gc\n",
    "import torch\n",
    "from contextlib import contextmanager\n",
    "\n",
    "@contextmanager\n",
    "def model_context(pipeline_func, *args, **kwargs):\n",
    "    \"\"\"ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼Œè‡ªåŠ¨æ¸…ç†æ¨¡å‹èµ„æº\"\"\"\n",
    "    model = None\n",
    "    try:\n",
    "        model = pipeline_func(*args, **kwargs)\n",
    "        yield model\n",
    "    finally:\n",
    "        # æ¸…ç†èµ„æº\n",
    "        if model is not None:\n",
    "            del model\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "# ä½¿ç”¨ç¤ºä¾‹\n",
    "with model_context(pipeline, \"sentiment-analysis\") as classifier:\n",
    "    result = classifier(\"I love this!\")\n",
    "    print(result)\n",
    "# æ¨¡å‹ä¼šåœ¨withå—ç»“æŸåè‡ªåŠ¨æ¸…ç†\n",
    "```\n",
    "\n",
    "## åä¸‰ã€æ€»ç»“\n",
    "\n",
    "### 13.1 æˆ‘ä»¬å­¦åˆ°äº†ä»€ä¹ˆ\n",
    "\n",
    "1. **Hugging FaceåŸºç¡€**\n",
    "   - Model Hubçš„ä½¿ç”¨\n",
    "   - Transformersåº“çš„æ ¸å¿ƒæ¦‚å¿µ\n",
    "   - Pipeline APIçš„ä¾¿æ·æ€§\n",
    "\n",
    "2. **å„ç§ä»»åŠ¡çš„å®ç°**\n",
    "   - æ–‡æœ¬åˆ†ç±»ã€ç”Ÿæˆã€é—®ç­”\n",
    "   - å›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹\n",
    "   - è¯­éŸ³è¯†åˆ«ã€éŸ³é¢‘åˆ†ç±»\n",
    "   - å¤šæ¨¡æ€ä»»åŠ¡\n",
    "\n",
    "3. **è¿›é˜¶æŠ€å·§**\n",
    "   - æ¨¡å‹å¾®è°ƒ\n",
    "   - æ€§èƒ½ä¼˜åŒ–\n",
    "   - é”™è¯¯å¤„ç†\n",
    "   - èµ„æºç®¡ç†\n",
    "\n",
    "4. **å®æˆ˜åº”ç”¨**\n",
    "   - æ„å»ºé—®ç­”ç³»ç»Ÿ\n",
    "   - æ•´åˆå‘é‡æ•°æ®åº“\n",
    "   - ç”Ÿäº§ç¯å¢ƒè€ƒè™‘\n",
    "\n",
    "### 13.2 ä¸‹ä¸€æ­¥å­¦ä¹ å»ºè®®\n",
    "\n",
    "1. **æ·±å…¥ç‰¹å®šé¢†åŸŸ**\n",
    "   - é€‰æ‹©ä¸€ä¸ªæ„Ÿå…´è¶£çš„æ–¹å‘æ·±å…¥ç ”ç©¶\n",
    "   - å°è¯•æ›´å¤šä¸“ä¸šæ¨¡å‹\n",
    "   - å‚ä¸ç¤¾åŒºè®¨è®º\n",
    "\n",
    "2. **æ¨¡å‹è®­ç»ƒå’Œå¾®è°ƒ**\n",
    "   - å­¦ä¹ å¦‚ä½•è®­ç»ƒè‡ªå·±çš„æ¨¡å‹\n",
    "   - æŒæ¡æ•°æ®é›†å‡†å¤‡æŠ€å·§\n",
    "   - äº†è§£åˆ†å¸ƒå¼è®­ç»ƒ\n",
    "\n",
    "3. **éƒ¨ç½²å’Œä¼˜åŒ–**\n",
    "   - å­¦ä¹ æ¨¡å‹éƒ¨ç½²æŠ€æœ¯\n",
    "   - æŒæ¡æ¨¡å‹å‹ç¼©å’ŒåŠ é€Ÿ\n",
    "   - äº†è§£è¾¹ç¼˜è®¡ç®—éƒ¨ç½²\n",
    "\n",
    "4. **å‚ä¸å¼€æº**\n",
    "   - è´¡çŒ®ä»£ç åˆ°Transformersåº“\n",
    "   - åˆ†äº«è‡ªå·±è®­ç»ƒçš„æ¨¡å‹\n",
    "   - ç¼–å†™æ•™ç¨‹å¸®åŠ©ä»–äºº\n",
    "\n",
    "### 13.3 æœ‰ç”¨çš„èµ„æº\n",
    "\n",
    "1. **å®˜æ–¹èµ„æº**\n",
    "   - Hugging Faceå®˜ç½‘: https://huggingface.co/\n",
    "   - Transformersæ–‡æ¡£: https://huggingface.co/docs/transformers/\n",
    "   - æ¨¡å‹ä¸­å¿ƒ: https://huggingface.co/models\n",
    "\n",
    "2. **å­¦ä¹ èµ„æº**\n",
    "   - Hugging Faceè¯¾ç¨‹: https://huggingface.co/course/\n",
    "   - è®ºæ–‡é˜…è¯»: https://papers.huggingface.co/\n",
    "   - ç¤¾åŒºè®ºå›: https://discuss.huggingface.co/\n",
    "\n",
    "3. **ç›¸å…³å·¥å…·**\n",
    "   - Gradio: å¿«é€Ÿåˆ›å»ºæ¨¡å‹Demo\n",
    "   - Streamlit: æ„å»ºæ•°æ®åº”ç”¨\n",
    "   - FastAPI: æ„å»ºAPIæœåŠ¡\n",
    "\n",
    "### 13.4 ç»“è¯­\n",
    "\n",
    "Hugging Faceè®©AIæ¨¡å‹çš„ä½¿ç”¨å˜å¾—å‰æ‰€æœªæœ‰çš„ç®€å•ã€‚é€šè¿‡æœ¬æ•™ç¨‹ï¼Œä½ å·²ç»æŒæ¡äº†ä½¿ç”¨å„ç§AIæ¨¡å‹çš„åŸºæœ¬æŠ€èƒ½ã€‚è®°ä½ï¼š\n",
    "\n",
    "- **ä»ç®€å•å¼€å§‹**ï¼šå…ˆç”¨Pipeline APIå¿«é€Ÿå®ç°åŠŸèƒ½\n",
    "- **é€æ­¥æ·±å…¥**ï¼šéœ€è¦æ›´å¤šæ§åˆ¶æ—¶å†ä½¿ç”¨åº•å±‚API\n",
    "- **æ³¨é‡å®è·µ**ï¼šå¤šåšé¡¹ç›®ï¼Œåœ¨å®è·µä¸­å­¦ä¹ \n",
    "- **ä¿æŒå­¦ä¹ **ï¼šAIé¢†åŸŸå‘å±•è¿…é€Ÿï¼ŒæŒç»­å…³æ³¨æ–°æŠ€æœ¯\n",
    "\n",
    "ç°åœ¨ï¼Œä½ å·²ç»æ‹¥æœ‰äº†å¼ºå¤§çš„AIå·¥å…·ç®±ï¼Œå»åˆ›é€ ä»¤äººæƒŠå¹çš„åº”ç”¨å§ï¼\n",
    "\n",
    "**Happy Modeling! ğŸ¤—**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65dd1d04-0cd5-48b0-a441-45d67dc3a53a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
