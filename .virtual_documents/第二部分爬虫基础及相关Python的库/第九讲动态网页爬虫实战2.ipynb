import requests

url = "https://item.jd.com/100095516761.html"

headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
}

response = requests.get(url, headers=headers)

if response.status_code == 200:
    print("请求成功!")
    
    # 从URL中提取文件名
    filename = url.split("/")[-1]
    
    # 确保文件有.html后缀
    if not filename.endswith('.html'):
        filename += '.html'
    
    # 保存响应内容到文件
    with open(filename, 'w', encoding='utf-8') as f:
        f.write(response.text)
    
    print(f"页面内容已保存到当前文件夹: {filename}")
else:
    print("请求失败,状态码:", response.status_code)


from selenium import webdriver
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.support.wait import WebDriverWait
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from time import sleep
from lxml import etree
import pandas as pd

# 初始化相关参数
url = 'https://www.jd.com/'
pages = 2  # 要抓取的产品页数
comment_pages = 5  # 要抓取的评论页数

# 启动 Chrome 浏览器
driver = webdriver.Chrome()

# 创建 WebDriverWait 对象
wait = WebDriverWait(driver, 10)

# 用于存储产品和评论的数据
data = pd.DataFrame()
comments = pd.DataFrame()

# 打开京东首页
driver.get(url)

# 等待并点击登录按钮
login_button = wait.until(EC.element_to_be_clickable((By.CLASS_NAME, 'link-login')))
login_button.click()

print("请使用京东 App 扫描二维码登录")
input("登录完成后按回车键继续...")

# 搜索产品
key = '华为p70 pro'
search_box = wait.until(EC.presence_of_element_located((By.ID, 'key')))
search_box.clear()
search_box.send_keys(key)  # 输入搜索关键词
search_box.send_keys(Keys.RETURN)  # 回车进行搜索
sleep(3)

# 定义函数以向下滚动页面
def scroll_down():
    for _ in range(2):
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        sleep(3)

# 定义函数以提取页面上的产品信息
def get_content():
    global data  # 使用全局变量来存储产品数据
    html = etree.HTML(driver.page_source)
    items = html.xpath('//div[@class="gl-i-wrap"]')
    for item in items:
        D = {}
        # 提取价格
        D['price'] = item.xpath('.//div[@class="p-price"]/strong/i/text()')[0]
        # 提取评论数
        D['comment'] = item.xpath('.//div[@class="p-commit"]/strong/a/text()')[0]
        # 提取店铺名称
        shopname = item.xpath('.//div[@class="p-shop"]/span/a/text()')
        D['shopname'] = shopname[0] if shopname else 'None'
        # 提取产品链接
        D['URL'] = 'https:' + item.xpath('.//div[@class="p-img"]/a/@href')[0]
        # 提取产品标题
        title = item.xpath('.//div[@class="p-name p-name-type-2"]/a/em')[0].xpath('string(.)').strip()
        D['title'] = title
        # 提取图片链接
        image_url = item.xpath('.//div[@class="p-img"]/a/img/@data-lazy-img')
        D['pnglink'] = 'https:' + image_url[0] if image_url and image_url[0] != 'done' else 'https:' + item.xpath('.//div[@class="p-img"]/a/img/@src')[0]
        
        # 将数据添加到 DataFrame 中
        data = pd.concat([data, pd.DataFrame([D])])

# 定义函数以点击下一页按钮
def next_page():
    try:
        next_button = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, '.pn-next:not(.disabled)')))
        driver.execute_script("arguments[0].click();", next_button)  # 点击下一页按钮
        sleep(4)
        return True
    except:
        print("No more pages or unable to click next page button")
        return False

# 抓取指定页数的产品信息
for _ in range(pages):
    scroll_down()  # 向下滚动页面
    get_content()  # 提取页面内容
    if not next_page():  # 如果无法点击下一页按钮，停止抓取
        break

# 获取产品评论（针对第一页的第一个产品）
if not data.empty:
    first_product_url = data.iloc[0]['URL']
    driver.get(first_product_url)  # 打开产品页面
    sleep(3)

    # 切换到评论标签
    comment_tab = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, 'li[data-anchor="#comment"]')))
    comment_tab.click()
    sleep(3)

    # 定义函数以提取评论内容
    def extract_comments():
        global comments  # 使用全局变量来存储评论数据
        comment_items = driver.find_elements(By.CSS_SELECTOR, 'div.comment-item')
        for comment in comment_items:
            C = {}
            C['content'] = comment.find_element(By.CSS_SELECTOR, 'p.comment-con').text  # 提取评论内容
            C['time'] = comment.find_element(By.CSS_SELECTOR, 'span.comment-time').text  # 提取评论时间
            C['user'] = comment.find_element(By.CSS_SELECTOR, 'div.user-info').text  # 提取用户名
            C['score'] = len(comment.find_elements(By.CSS_SELECTOR, 'span.star-red'))  # 提取评分
            comments = pd.concat([comments, pd.DataFrame([C])])

    # 定义函数以点击下一页的评论按钮
    def next_comment_page():
        try:
            next_button = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, 'a.ui-pager-next')))
            if 'disabled' not in next_button.get_attribute('class'):
                next_button.click()
                sleep(3)
                return True
            else:
                return False
        except:
            return False

    # 抓取指定页数的评论
    for _ in range(comment_pages):
        extract_comments()  # 提取评论
        if not next_comment_page():  # 如果无法点击下一页评论按钮，停止抓取
            break

# 关闭浏览器
driver.quit()

# 打印产品和评论数据
print("Products:")
print(data)
print("\nComments:")
print(comments)

# 保存产品和评论数据到 CSV 文件
data.to_csv('jd_products.csv', index=False, encoding='utf-8-sig')
comments.to_csv('jd_comments.csv', index=False, encoding='utf-8-sig')






from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium_stealth import stealth
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, WebDriverException, NoSuchElementException
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
import pandas as pd
import time
import random
import logging
import traceback

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def setup_driver():
    options = webdriver.ChromeOptions()
    options.add_argument("--disable-gpu")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-extensions")
    options.add_argument("--window-size=1920,1080")
    options.add_experimental_option("excludeSwitches", ["enable-automation"])
    options.add_experimental_option('useAutomationExtension', False)
    
    try:
        service = Service(ChromeDriverManager().install())
        driver = webdriver.Chrome(service=service, options=options)
        
        stealth(driver,
                languages=["zh-CN", "zh"],
                vendor="Google Inc.",
                platform="Win32",
                webgl_vendor="Intel Inc.",
                renderer="Intel Iris OpenGL Engine",
                fix_hairline=True,
        )
        
        logging.info("成功创建 Chrome 驱动")
        return driver
    except Exception as e:
        logging.error(f"创建驱动失败: {e}")
        logging.error(traceback.format_exc())
        raise

def open_product_page(driver, product_id):
    url = f"https://item.jd.com/{product_id}.html"
    try:
        driver.get(url)
        logging.info(f"已打开商品页面: {url}")
        time.sleep(random.uniform(3, 5))
    except Exception as e:
        logging.error(f"打开商品页面失败: {e}")
        raise

def wait_for_login(driver):
    input("请在浏览器中登录，然后按回车键继续...")
    logging.info("用户确认已登录")

def navigate_to_comments(driver, wait):
    try:
        # 尝试多个可能的选择器
        selectors = [
            'li[data-anchor="#comment"]',
            'a[href="#comment"]',
            'div[id="comment"]',
            'div.tab-main:nth-child(2)'
        ]
        
        for selector in selectors:
            try:
                comment_tab = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, selector)))
                comment_tab.click()
                logging.info(f"已点击评论标签，使用选择器: {selector}")
                break
            except:
                continue
        else:
            raise Exception("无法找到评论标签")
        
        time.sleep(5)
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(2)
        
    except Exception as e:
        logging.error(f"导航到评论区时出错: {e}")
        raise

def save_page_source(driver):
    try:
        with open("jd_comment_page.html", "w", encoding="utf-8") as f:
            f.write(driver.page_source)
        logging.info("已保存页面源代码到 jd_comment_page.html")
    except Exception as e:
        logging.error(f"保存页面源代码失败: {e}")

def extract_comments(driver, wait, comments):
    try:
        # 尝试多个可能的选择器
        selectors = [
            'div.comment-item',
            'div.comment-list div',
            'div[data-guid]',
            'div.comment'
        ]
        
        comment_items = []
        for selector in selectors:
            comment_items = driver.find_elements(By.CSS_SELECTOR, selector)
            if comment_items:
                logging.info(f"找到评论元素，使用选择器: {selector}")
                break
        
        if not comment_items:
            logging.warning("未找到评论元素，可能页面结构已改变")
            save_page_source(driver)
            return
        
        for item in comment_items:
            try:
                content = item.find_element(By.CSS_SELECTOR, 'p.comment-con, div.comment-content').text
                time_elem = item.find_element(By.CSS_SELECTOR, 'span.comment-time, span.time').text
                user = item.find_element(By.CSS_SELECTOR, 'div.user-info, span.user-name').text
                stars = len(item.find_elements(By.CSS_SELECTOR, 'span.star-red, i.star-yellow'))
                
                comment = {
                    'content': content.strip() if content else "N/A",
                    'time': time_elem.strip() if time_elem else "N/A",
                    'user': user.strip() if user else "N/A",
                    'score': stars if stars else 0
                }
                comments.append(comment)
            except Exception as e:
                logging.error(f"提取单条评论时出错: {e}")
        
        logging.info(f"已提取 {len(comment_items)} 条评论")
    except Exception as e:
        logging.error(f"提取评论过程中发生错误: {e}")
        raise

def next_page(driver, wait):
    try:
        # 尝试多个可能的选择器
        selectors = [
            'a.ui-pager-next',
            'a.next',
            'a[aria-label="Next"]'
        ]
        
        for selector in selectors:
            try:
                next_button = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, selector)))
                if 'disabled' not in next_button.get_attribute('class'):
                    next_button.click()
                    logging.info(f"已点击下一页，使用选择器: {selector}")
                    time.sleep(random.uniform(3, 5))
                    return True
                else:
                    logging.info("已到达最后一页")
                    return False
            except:
                continue
        
        logging.error("无法找到下一页按钮")
        return False
    except Exception as e:
        logging.error(f"翻页时发生错误: {e}")
        return False

def main():
    product_id = '100095516761'  # 更换为您想爬取的商品ID
    comment_pages = 5
    comments = []
    driver = None

    try:
        driver = setup_driver()
        wait = WebDriverWait(driver, 20)

        open_product_page(driver, product_id)
        wait_for_login(driver)  # 等待登录确认
        navigate_to_comments(driver, wait)
        
        for page in range(comment_pages):
            logging.info(f"正在爬取第 {page + 1} 页评论")
            extract_comments(driver, wait, comments)
            if not next_page(driver, wait):
                break

    except Exception as e:
        logging.error(f"爬取过程中发生错误: {e}")
        logging.error(traceback.format_exc())
    finally:
        if driver:
            driver.quit()
            logging.info("已关闭浏览器")

    if comments:
        comments_df = pd.DataFrame(comments)
        comments_df.to_csv('jd_comments.csv', index=False, encoding='utf-8-sig')
        logging.info(f"成功爬取 {len(comments_df)} 条评论，已保存到 jd_comments.csv 文件")
        logging.info("\n评论示例:")
        logging.info(comments_df.head().to_string())
    else:
        logging.warning("未成功爬取到评论")

if __name__ == "__main__":
    main()





!pip install webdriver_manager



